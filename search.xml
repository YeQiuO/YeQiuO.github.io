<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>设计模式</title>
    <url>/Design-Pattern/</url>
    <content><![CDATA[<h1 id="单例模式">单例模式</h1>
<p>双重校验锁</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">static</span> Singleton uniqueInstance;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (uniqueInstance == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="comment">//类对象加锁</span></span><br><span class="line">    <span class="keyword">synchronized</span> (Singleton.class) &#123;</span><br><span class="line">        <span class="keyword">if</span> (uniqueInstance == <span class="literal">null</span>) &#123;</span><br><span class="line">            uniqueInstance = <span class="keyword">new</span> <span class="title class_">Singleton</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="模板方法模式">模板方法模式</h1>
<blockquote>
<p>抽象出步骤的执行顺序作为抽象方法，具体的实现方法交给子类实现</p>
</blockquote>
<p>实现 AQS 抽象类的锁，需要重写 CLH 锁的
<code>tryAcquire-tryRelease</code>、<code>tryAcquireShared-tryReleaseShared</code>、<code>isHeldExclusively</code>
方法。</p>
<p>重写钩子方法，可以自定义 <code>state</code>
的含义：值为多少时代表加锁成功/失败 or
解锁成功/失败，也可以实现共享锁或独占锁。</p>
<h1 id="观察者模式">观察者模式</h1>
<p>具体查看 JUC_Lock 博客的 <code>CompletableFuture</code>
的底层实现</p>
<h1 id="适配器模式">适配器模式</h1>
<p>具体查看 SpringMVC 博客的 <code>HandlerAdapter</code> 的底层实现</p>
<p>开闭原则：新增代替修改</p>
]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title>模拟面试题</title>
    <url>/Interview-Questions/</url>
    <content><![CDATA[<h1 id="java-basic">Java-basic</h1>
<h1 id="jvm">JVM</h1>
<ol type="1">
<li>了解垃圾回收机制吗？新生代和老年代的垃圾回收是怎么样的</li>
</ol>
<h1 id="juc">JUC</h1>
<ol type="1">
<li>简要描述线程与进程的关系，区别及优缺点？</li>
<li>线程之间哪些资源是共享的，哪些资源是私有的，为什么？</li>
<li>说一下你对于 AQS 原理的理解。</li>
<li>如何实现线程安全？</li>
</ol>
<h1 id="中间件">中间件</h1>
<ol type="1">
<li>了解过消息中间件吗？说一下使用场景</li>
</ol>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>面试准备</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构和算法</title>
    <url>/Java-Algorithm/</url>
    <content><![CDATA[<h1 id="算法">算法</h1>
<h2 id="链表">链表</h2>
<h3 id="快慢指针">快慢指针</h3>
<p>链表倒数第n个数</p>
<h3 id="分治法">分治法</h3>
<p>排序</p>
<h2 id="二分查找">二分查找</h2>
<p>【1】目标不存在于数组中，找第一个大于它的下标</p>
<p>【2】有多个目标存在于数组中，找第一个等于它的下标</p>
<p>【思路】基于二分查找，优化寻找 mid 指针和收敛 left right
指针的步骤，具体问题具体分析</p>
<ul>
<li>mid=(left+right)&gt;&gt;2；if
(nums[left]<strong>&lt;</strong>nums[mid]) left=mid<strong>+1</strong>;
else right=mid;【找左边】</li>
<li>mid=(left+right<strong>+1</strong>)&gt;&gt;2；if
(nums[left]<strong>&lt;=</strong>nums[mid]) left=mid; else
right=mid<strong>-1</strong>;【找右边】</li>
</ul>
<h2 id="堆排序">堆排序</h2>
<p>数组实现大根堆的两个主要方法（注意动态扩容）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Heap</span> &#123;</span><br><span class="line">    <span class="type">int</span>[] data;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">downHeapify</span><span class="params">(<span class="type">int</span> i)</span> &#123;</span><br><span class="line">        <span class="comment">// 下标从0开始，子节点下标为2i+1和2i+2</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">l</span> <span class="operator">=</span> i*<span class="number">2</span> + <span class="number">1</span>, r = i*<span class="number">2</span> + <span class="number">2</span>, largest = i;</span><br><span class="line">        <span class="keyword">if</span> (l &lt; heapSize &amp;&amp; data[l] &gt; data[largest]) &#123;</span><br><span class="line">            largest = l;</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">if</span> (r &lt; heapSize &amp;&amp; data[r] &gt; data[largest]) &#123;</span><br><span class="line">            largest = r;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (largest != i) &#123;</span><br><span class="line">            swap(i, largest);</span><br><span class="line">            maxHeapify(largest);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">upHeapify</span><span class="params">(<span class="type">int</span> i)</span> &#123;</span><br><span class="line">        <span class="comment">// 父节点下标为(i-1)/2</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">parent</span> <span class="operator">=</span> (i-<span class="number">1</span>)/<span class="number">2</span>, min = i;</span><br><span class="line">        <span class="keyword">if</span> (parent&gt;=<span class="number">0</span> &amp;&amp; data[parent] &lt; data[i]) &#123;</span><br><span class="line">            min = parent;</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">if</span> (min != i) &#123;</span><br><span class="line">            swap(min, i);</span><br><span class="line">            upHeapify(min);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="动态规划">动态规划</h2>
<p>思路：动规的中心思想就是把一个复杂问题拆分成多个简单、可以逐步堆叠得到答案的子问题。</p>
<p>做题步骤</p>
<ol type="1">
<li>确定dp数组（dp table）以及下标的含义</li>
<li>确定递推公式</li>
<li>dp数组如何初始化</li>
<li>确定遍历顺序</li>
<li>举例推导dp数组</li>
</ol>
<h3 id="背包">0-1背包</h3>
<blockquote>
<p>每个物品只能用一次</p>
</blockquote>
<p>下标含义：dp[物品 i ][容量 j ] 。如果
物品价值==物品种类，dp数组可以直接用 boolean[][]，不相等用 int[][]</p>
<p>状态转移方程：<code>dp[i][j] = dp[i - 1][j] || dp[i - 1][j - nums[i]]</code></p>
<p>初始化：物品 0 和容量 0 的情况</p>
<p>遍历顺序：二维数组先遍历物品和容量都可以，一维数组只能先遍历容量</p>
<h3 id="树形-dp">树形 dp</h3>
<p>题型：<strong>树的直径</strong></p>
<p>思路：可以看作是再求树的深度，在求深度的同时“更新树的最长直径”。</p>
<p>代码模板</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LongestPath</span> &#123;</span><br><span class="line">    List&lt;Integer&gt;[] g;</span><br><span class="line">    <span class="type">int</span> <span class="variable">longestPath</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> <span class="title function_">dfs</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">        <span class="comment">// 两种思路：1.记录最大和次大长的路径；2.记录最大长 max，max+当前结果 和 result 比大小</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">max</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> child:g[x]) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">cur</span> <span class="operator">=</span> dfs(child) + <span class="number">1</span>;</span><br><span class="line">            longestPath = Math.max(longestPath, max + cur);</span><br><span class="line">            max = Math.max(max, cur);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">solve</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] relate = &#123;-<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;  <span class="comment">// 边的关系(题目给出)</span></span><br><span class="line">        <span class="comment">// 构建关系，List 存儿子节点，可以拓展到一般树的情况</span></span><br><span class="line">        g = <span class="keyword">new</span> <span class="title class_">ArrayList</span>[relate.length];</span><br><span class="line">        Arrays.setAll(g, e -&gt; <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;());</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i&lt;relate.length; i++) &#123;</span><br><span class="line">            g[relate[i]].add(i);</span><br><span class="line">        &#125;</span><br><span class="line">        dfs(<span class="number">0</span>);</span><br><span class="line">        System.out.println(longestPath);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>拓展：</p>
<ul>
<li>【124】求二叉树中最大的路径和。改左右子树的深度为节点值的和，若和为负数则返回
0</li>
<li>【2245】一般树的最长路径。遍历所有邻居（过滤掉父节点），取最长、次长两条路径相加，结果为最长路径。此外，根据题意还要去除父子结点相同的情况。</li>
</ul>
<p>题型：<strong>树上最大独立集</strong>（从图中选择尽量多的点，使得这些点互不相邻）</p>
<p>题型：<strong>树上最小支配集</strong>（从图中选择尽量少的点，使得这些点和其父子结点包括了整棵树）</p>
<h3 id="二维-dp">二维 dp</h3>
<p>题型：最长公共子序列</p>
<p>思路：定义 <code>dp[i][j]</code> 表示 <code>text1[0:i-1]</code> 和
<code>text2[0:j-1]</code> 的最长公共子序列</p>
<h2 id="前缀和">前缀和</h2>
<blockquote>
<p>应用题型：连续子数组</p>
</blockquote>
<h2 id="并查集">并查集</h2>
<blockquote>
<p>应用题型：检查树是否连通；构造最小生成树</p>
<p>扩展：带权并查集</p>
</blockquote>
<p>数据结构模板</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Node</span> &#123;</span><br><span class="line">    <span class="type">int</span>[] tab;</span><br><span class="line">    Node (<span class="type">int</span> size) &#123;</span><br><span class="line">        <span class="built_in">this</span>.tab = <span class="keyword">new</span> <span class="title class_">int</span>[size];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> <span class="title function_">find</span><span class="params">(<span class="type">int</span> i)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> i==tab[i] ? i : (tab[i] = find(tab[i]));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">union</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span> &#123;</span><br><span class="line">        tab[find(x)] = find(y);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="java-技巧">Java 技巧</h1>
<p>强制类型转换</p>
<ul>
<li>向零截断，直接保留整数位 <code>1/2 == -1/2 == 0</code></li>
</ul>
<p>基本数据类型的默认值</p>
<ul>
<li>int  double：0</li>
<li>boolean：false</li>
</ul>
<p>常量池</p>
<ul>
<li>包装类型常量池：Byte, Short, Integer [-128,127], Long, Character,
Boolean</li>
<li>字符串常量池</li>
</ul>
<p>初始化</p>
<ul>
<li>成员变量（定义在类里方法外的变量）一定要进行初始化的，如果不显式的进行初始化，那么虚拟机会进行默认的初始化
<ul>
<li>基本数据类型一般是给予默认值</li>
<li>引用类型初始值为 null</li>
</ul></li>
<li>局部变量不会自动初始化，所以在声明局部变量时要注意，可以不在声明时初始化，但在使用之前一定要进行初始化，否则会报编译错误</li>
</ul>
<h2 id="数组列表快速初始化">数组、列表快速初始化</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span>[] array = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line"><span class="type">int</span>[] array = <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;;</span><br><span class="line"><span class="type">int</span>[][] array = &#123;&#123;<span class="number">1</span>,<span class="number">2</span>&#125;,&#123;<span class="number">2</span>,<span class="number">3</span>&#125;&#125;;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">List&lt;&gt; list = Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>);  <span class="comment">// 返回的是Arrays的内部类【无法增删】</span></span><br><span class="line">List&lt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;()&#123;&#123;</span><br><span class="line">    add(<span class="number">1</span>);</span><br><span class="line">    add(<span class="number">2</span>);</span><br><span class="line">    add(<span class="number">3</span>);</span><br><span class="line">&#125;&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="初始化小根堆">初始化小根堆</h2>
<p>使用优先级队列 <code>PriorityQueue</code> 实现小根堆</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">PriorityQueue&lt;<span class="type">int</span>[]&gt; queue = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;&gt;((pair1,pair2) -&gt; pair1[<span class="number">1</span>]-pair2[<span class="number">1</span>]);  <span class="comment">// 数组</span></span><br><span class="line">PriorityQueue&lt;ListNode&gt; queue = <span class="keyword">new</span> <span class="title class_">PriorityQueue</span>&lt;&gt;((o1, o2) -&gt; o1.val-o2.val);  <span class="comment">// 节点</span></span><br><span class="line"></span><br><span class="line">queue.offer(data)  <span class="comment">// 节点入堆</span></span><br><span class="line">data = queue.poll()  <span class="comment">// 头节点出堆</span></span><br></pre></td></tr></table></figure>
<h2 id="list-转数组">List 转数组</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 基本数据类型需要手动拆箱</span></span><br><span class="line"><span class="type">int</span>[] array = list.stream().mapToInt(Integer::valueOf).toArray();</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 流式</span></span><br><span class="line">String[] array = list.stream().toArray(String[]::<span class="keyword">new</span>);</span><br><span class="line"><span class="comment">// 串行【规定好数组大小，zhuan&#x27;hua速度更快】</span></span><br><span class="line">String[] array = list.toArray(<span class="keyword">new</span> <span class="title class_">String</span>[list.size()]);</span><br><span class="line">String[] array = list.toArray(<span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">0</span>]);</span><br></pre></td></tr></table></figure>
<h2 id="数组转-list">数组转 List</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 基本数据类型需要手动装箱</span></span><br><span class="line"><span class="type">int</span>[] arr = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> &#125;;</span><br><span class="line">List&lt;Integer&gt; list = Arrays.stream(arr).boxed().collect(Collectors.toList());</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Integer[] integers = <span class="keyword">new</span> <span class="title class_">Integer</span>[]&#123;<span class="number">3</span>,<span class="number">8</span>,<span class="number">20</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">25</span>&#125;;</span><br><span class="line">List&lt;Integer&gt; list = Arrays.asList(integers); </span><br><span class="line"><span class="comment">// ArrayList&lt;Object&gt; list = new ArrayList&lt;&gt;();</span></span><br><span class="line"><span class="comment">// Collections.addAll(list, a);</span></span><br></pre></td></tr></table></figure>
<h2 id="位运算值交换">位运算值交换</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">ch[a] ^= ch[b];</span><br><span class="line">ch[b] ^= ch[a];</span><br><span class="line">ch[a] ^= ch[b];</span><br></pre></td></tr></table></figure>
<h2 id="泛型占位符">泛型&amp;占位符</h2>
<p>泛型：T E K V &gt;
泛型的初衷就是为了能在编译器检查出类型安全问题，并通过编译错误提示程序员</p>
<ul>
<li>泛型必须是引用类型，不能是基本类型</li>
<li>泛型通过类型擦除实现</li>
</ul>
<p>占位符：?</p>
<p>TODO lambda stream 流式计算 详解</p>
<h2 id="位运算判断奇偶">位运算判断奇偶</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">num &amp; <span class="number">1</span> == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h2 id="十进制数转-char">十进制数转 char</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">char</span> <span class="variable">t</span> <span class="operator">=</span> Character.forDigit(i, <span class="number">10</span>);</span><br></pre></td></tr></table></figure>
<h2 id="collection-转-list">Collection 转 List</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Collection&lt;T&gt; collection = map.values();</span><br><span class="line">List&lt;T&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;T&gt;(collection);</span><br></pre></td></tr></table></figure>
<h2 id="map-遍历方法">Map 遍历方法</h2>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (Map.Entry&lt;Key.class, Value.class&gt; entry : map.entrySet()) &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;key= &quot;</span> + entry.getKey() + <span class="string">&quot; and value= &quot;</span> + entry.getValue());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="在条件语句中赋值">在条件语句中赋值</h2>
<p>不能直接在 <code>if</code> 语句的条件部分进行声明和赋值，需要在
<code>if</code> 语句之前声明变量，并在条件语句中只进行赋值或比较操作</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> dis;  </span><br><span class="line"><span class="keyword">if</span> ((dis = i - dp[i - <span class="number">1</span>]) &gt; <span class="number">1</span> &amp;&amp; s.charAt(dis - <span class="number">2</span>) == <span class="string">&#x27;(&#x27;</span>) &#123;  </span><br><span class="line">    <span class="comment">// ... 使用 dis 进行操作 ...  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="stringbuilderstringbuffer-常用方法">StringBuilder/StringBuffer
常用方法</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s.append(char c) // 将 char 追加到此序列</span><br><span class="line">s.charAt(int index) // 返回指定索引处的此序列中的 char 值</span><br><span class="line">s.deleteCharAt(int index) // 按此顺序删除指定位置的 char</span><br><span class="line">s.setCharAt(int index, char ch) // 指定索引处的字符设置为 ch</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Java算法</tag>
      </tags>
  </entry>
  <entry>
    <title>如何工作</title>
    <url>/How-to-Work/</url>
    <content><![CDATA[<h1 id="方法论">方法论</h1>
<p>不卑不亢，谦虚有礼</p>
<p>可以竞争但要避免内斗</p>
<p>及时和领导同步进度</p>
<p>平和谦逊，思维敏捷，处理问题简明扼要</p>
]]></content>
      <categories>
        <category>工作</category>
        <category>思考</category>
      </categories>
      <tags>
        <tag>工作</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM 应用开发</title>
    <url>/LLM-Application/</url>
    <content><![CDATA[<p>实验室项目对话模块的主要工作和创新点，先大致记录一下思路，后续慢慢完善</p>
<h1 id="何为llm应用开发">何为LLM应用开发</h1>
<p>网上不少人认为，与其去做 Prompt 应用不如去提升 LLM 的智能，Prompt
应用对于大语言模型能力提升的作用有限。但我不以为然，获得 Prompt
的加持之后，LLM
可以<strong>应用现实生活的所有工具</strong>；可以<strong>按步骤正确处理一个复杂问题</strong>（因为
LLM
不同于人类对话，只有构造出非常详细且周密的过程约束，才能得到最正确的答案，不然会出现“幻觉”现象）；可以引导
<strong>LLM 自我规划</strong>（下文的长文本对话），目前 Prompt
最常应用在 <code>Dynamic Few-Shot Examples</code> 领域中，可以快速定位与
Query 最相关的小样本，不使不相关的内容分散 LLM
的注意力，而不只局限于“聊天机器人”和“搜索引擎”这类看似没啥么太大用处的名头。</p>
<p>一言以蔽之，应用开发就是将 LLM
聪明的大脑装上可随时替换的手脚、五官和前额叶、海马体，它可以变成任何人，使用任何工具。</p>
<h1 id="长文本对话">长文本对话</h1>
<h2 id="问题背景">问题背景</h2>
<p>虽然模型都支持32k的上下文，但是无法要求模型一次性输出超长文本。若给模型输入：“给我讲讲中国五千年历史，字数不得少于五千”，是无法得到想要的回答，模型最多生成一两千的字，而且由于所有输出都是一次返回的，上下文逻辑和内容都不尽人意。</p>
<h2 id="解决思路">解决思路</h2>
<p>先调用模型生成一次大纲，再拆分大纲分批次输入模型，每次模型的输出只需要关注一小块内容，因此可以获得更优结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">OutlinePrompt = ChatPromptTemplate.from_template(<span class="string">f&#x27;请根据<span class="subst">&#123;query&#125;</span>，按照以下格式对问题提炼目录。回答内容尽量简短\n \</span></span><br><span class="line"><span class="string">            开始生成大纲:\n \</span></span><br><span class="line"><span class="string">            1. XXX\n    1.1xxx 1.2xxx 1.3xxx ...\n  \</span></span><br><span class="line"><span class="string">            2. XXX\n    2.1xxx 2.2xxx 2.3xxx ...\n  \</span></span><br><span class="line"><span class="string">            .......\n&#x27;</span>)</span><br><span class="line">chain = LLMChain(prompt=OutlinePrompt, llm=model1, memory=<span class="literal">None</span>, verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>由于需要<strong>结构化解析</strong>输出结果并<strong>链式调用</strong>模型扩写子论点，采用结构化输出解析器
<code>from langchain.output_parsers import StructuredOutputParser, ResponseSchema</code>
和模型调用链
<code>from langchain.chains import SimpleSequentialChain, SequentialChain</code>
，该方法相较于直接 for
循环调用模型更加麻烦，但是便于整体项目的开发、调用和维护。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">chat_prompt = PromptTemplate.from_template(<span class="string">f&#x27;请以<span class="subst">&#123;title&#125;</span>的<span class="subst">&#123;contexts&#125;</span>为主要内容，帮我扩写到<span class="subst">&#123;num&#125;</span>字&#x27;</span>)</span><br><span class="line">combined_prompt = PromptTemplate.from_template(<span class="string">f&#x27;写一段过渡段，从<span class="subst">&#123;title&#125;</span>自然过渡到<span class="subst">&#123;next_title&#125;</span>，内容简短&#x27;</span>)</span><br><span class="line">chat_chain = LLMChain(llm=llm, prompt=chat_prompt, output_key=<span class="string">&quot;chat&quot;</span>)</span><br><span class="line">combined_chain = LLMChain(llm=llm, prompt=combined_prompt, output_key=<span class="string">&quot;combined&quot;</span>)</span><br><span class="line">single_chain = SequentialChain(</span><br><span class="line">    chains=[chat_chain, combined_chain], </span><br><span class="line">    input_variables=[<span class="string">&quot;title&quot;</span>, <span class="string">&quot;contexts&quot;</span>, <span class="string">&quot;num&quot;</span>, <span class="string">&quot;next_title&quot;</span>],</span><br><span class="line">    output_variables=[<span class="string">&quot;chat&quot;</span>, <span class="string">&quot;combined&quot;</span>],</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>目前还是通过 for 循环遍历大纲，后续考虑采用路由链
<code>LLMRouterChain</code>，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">router_prompt = PromptTemplate(</span><br><span class="line">    template=router_template,</span><br><span class="line">    input_variables=[<span class="string">&quot;input&quot;</span>],</span><br><span class="line">    output_parser=RouterOutputParser(),</span><br><span class="line">)</span><br><span class="line">chain = MultiPromptChain(</span><br><span class="line">    router_chain=router_chain,    <span class="comment"># 路由链路</span></span><br><span class="line">    destination_chains=destination_chains,   <span class="comment"># 目标链路（LLmChain 数组）</span></span><br><span class="line">    default_chain=default_chain,      <span class="comment"># 默认链路</span></span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="私有数据知识库">私有数据知识库</h1>
<h2 id="问题背景-1">问题背景</h2>
<p>离线私有数据不能直接作为语料库训练模型，LLM
需要具有基于私有数据返回的能力。</p>
<h2 id="解决思路-1">解决思路</h2>
<p>知识库系统要包括文档加载、切分、存储、检索和存储聊天记录模块，具体通过向量表征
<code>Embeddings</code> 和向量存储 <code>Vector Store</code> 实现</p>
<ul>
<li>文本表征是对文本语义的向量表征，相似内容的文本具有相似的表征向量。这使我们可以在向量空间中比较文本的相似性。</li>
<li>向量数据库<code>Vector Database</code>用来存储文档的文本块。对于给定的文档，我们首先将其分成较小的文本块<code>chunks</code>，然后获取每个小文本块的文本表征，并将这些表征储存在向量数据库中。这个流程正是创建索引<code>index</code>的过程。将文档分成小文本块的原因在于我们可能无法将整个文档传入语言模型进行处理。</li>
</ul>
<img data-src="/LLM-Application/image-20240424102449161.png" class="" title="image-20240424102449161">
<p>Langchain 中文本分割器 <code>langchain.text_splitter</code> 都根据
chunk_size(块大小) 和 chunk_overlap(块与块之间的重叠大小) 进行分割</p>
<ul>
<li>chunk_size 指每个块包含的字符或 Token（如单词、句子等）的数量</li>
<li>chunk_overlap
指两个块之间共享的字符数量，用于保持上下文的连贯性，避免分割丢失上下文信息</li>
</ul>
<style>.jjtacixvdbtg{zoom:50%;}</style>
<img data-src="/LLM-Application/image-20240424102824838.png" class="jjtacixvdbtg" alt="langchain.text_splitter">
<p><strong>Q1</strong>：如何加强搜索结果的多样性？</p>
<p><strong>A1</strong>：最大边际相关性
<code>Maximum marginal relevance</code>
，过滤搜索结果中相似度很高的文档，可以同时满足查询的相关性和结果的多样性</p>
<p><strong>Q2</strong>：如何将查询限定在某些文档中？如 LLM
在查询时可能同时查找
浙江省财政报告、江苏省财政报告，但问题只与浙江省相关</p>
<p><strong>A2</strong>：通过 <code>SelfQueryRetriever</code> 参数
<code>document_content_description</code>
指定元素的不同字段（source）以及它们对应的位置（page）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">metadata_field_info_chinese = [</span><br><span class="line">    AttributeInfo(</span><br><span class="line">        name=<span class="string">&quot;source&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;文章来源于 `index-浙江省财政报告`, `index-江苏省财政报告` 的其中之一&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">&quot;string&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">    AttributeInfo(</span><br><span class="line">        name=<span class="string">&quot;page&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;文章中的哪一页&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="string">&quot;integer&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p><strong>Q3</strong>：如何通过过获取到的文档得到 LLM 响应</p>
<p><strong>A3</strong>：采用 检索式问答链 <code>RetrievalQA</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">template = <span class="string">&quot;&quot;&quot;使用以下上下文片段来回答最后的问题。如果你不知道答案，只需说不知道，不要试图编造答案。答案最多使用三个句子。尽量简明扼要地回答。在回答的最后一定要说&quot;感谢您的提问！&quot;</span></span><br><span class="line"><span class="string">&#123;context&#125;</span></span><br><span class="line"><span class="string">问题：&#123;question&#125;</span></span><br><span class="line"><span class="string">有用的回答：&quot;&quot;&quot;</span></span><br><span class="line">QA_CHAIN_PROMPT = PromptTemplate.from_template(template)</span><br><span class="line"><span class="comment"># RetrievalQA 内部使用了 QA_CHAIN_PROMPT</span></span><br><span class="line">qa_chain = RetrievalQA.from_chain_type( </span><br><span class="line">    llm,</span><br><span class="line">    retriever=vectordb.as_retriever()</span><br><span class="line">)</span><br><span class="line">result = qa_chain(&#123;<span class="string">&quot;query&quot;</span>: question&#125;)</span><br></pre></td></tr></table></figure>
<p><strong>Q4</strong>：如果文档太多，无法将它们全部适配到上下文窗口中怎么办？</p>
<p><strong>A4</strong>：采用
<code>MapReduce</code>，首先将每个独立的文档单独发送到语言模型以获取原始答案。然后，将答案通过最终对语言模型的一次调用组合成最终的答案</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">qa_chain_mr = RetrievalQA.from_chain_type(</span><br><span class="line">    llm,</span><br><span class="line">    retriever=vectordb.as_retriever(),</span><br><span class="line">    chain_type=<span class="string">&quot;map_reduce&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>如果信息分布在两个文档之间，无法在同一上下文中获取到所有的信息，也就没法给出正确答案。为解决这个问题，可以采用
<code>Refine 检索式问答链</code> ，Refine 文档链类似于 MapReduce
链，对于每一个文档，会调用一次 LLM，但有所改进的是，我们每次发送给 LLM
的最终提示是一个序列，这个序列会将先前的响应与新数据结合在一起，并请求得到改进后的响应。这种方法类似于
RNN，我们增强了上下文，从而解决信息分布在不同文档的问题。</p>
<p><a href="https://github.com/chatchat-space/Langchain-Chatchat">Github
参考项目链接</a></p>
<h1 id="多场景知识图谱">多场景知识图谱</h1>
<h2 id="问题背景-2">问题背景</h2>
<p>参考 Langchain 外部知识库开源项目的 <a
href="https://github.com/chatchat-space/Langchain-Chatchat/issues/1583">Issue</a>，可以发现，由于单一的文档切分方法和已训练好的分词器切分方法（项目中用的是
bge-large-zh-v1.5
模型）并不能理解场景相关的概念，也就无法构建有针对性的对话知识库，因此简单的导入外部数据无法实现多场景需求。</p>
<h2 id="解决思路-2">解决思路</h2>
<p>针对私有数据的多场景对话需求，可以构建并应用不同场景的知识图谱。</p>
<img data-src="/LLM-Application/image-20240424140419971.png" class="" title="image-20240424140419971">
<p>基于 用户输入Query 和 图数据库 Schema 构建 Prompt，通过 LLM
获取简短、精要的实体和关系信息，后续再基于此生成对话响应。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> GraphCypherQAChain</span><br><span class="line"><span class="keyword">from</span> langchain.graphs <span class="keyword">import</span> Neo4jGraph</span><br><span class="line"></span><br><span class="line">graph = Neo4jGraph(</span><br><span class="line">    url=<span class="string">&quot;bolt://localhost:7687&quot;</span>, </span><br><span class="line">    username=<span class="string">&quot;neo4j&quot;</span>, </span><br><span class="line">    password=<span class="string">&quot;&quot;</span></span><br><span class="line">)</span><br><span class="line">chain = GraphCypherQAChain.from_llm(</span><br><span class="line">    model, graph=graph, verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Langchain 底层针对图数据库对话的 Prompt</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CYPHER_GENERATION_TEMPLATE = <span class="string">&quot;&quot;&quot;Task:Generate Cypher statement to query a graph database.</span></span><br><span class="line"><span class="string">Instructions:</span></span><br><span class="line"><span class="string">Use only the provided relationship types and properties in the schema.</span></span><br><span class="line"><span class="string">Do not use any other relationship types or properties that are not provided.</span></span><br><span class="line"><span class="string">Schema:</span></span><br><span class="line"><span class="string">&#123;schema&#125;</span></span><br><span class="line"><span class="string">Note: Do not include any explanations or apologies in your responses.</span></span><br><span class="line"><span class="string">Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.</span></span><br><span class="line"><span class="string">Do not include any text except the generated Cypher statement.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The question is:</span></span><br><span class="line"><span class="string">&#123;question&#125;&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>Langchain 底层针对上下文问答的 Prompt</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CYPHER_QA_TEMPLATE = <span class="string">&quot;&quot;&quot;You are an assistant that helps to form nice and human understandable answers.</span></span><br><span class="line"><span class="string">The information part contains the provided information that you must use to construct an answer.</span></span><br><span class="line"><span class="string">The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.</span></span><br><span class="line"><span class="string">Make the answer sound as a response to the question. Do not mention that you based the result on the given information.</span></span><br><span class="line"><span class="string">If the provided information is empty, say that you don&#x27;t know the answer.</span></span><br><span class="line"><span class="string">Information:</span></span><br><span class="line"><span class="string">&#123;context&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: &#123;question&#125;</span></span><br><span class="line"><span class="string">Helpful Answer:&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="多轮对话">多轮对话</h1>
<p>采用 qdrant 向量库保存对话历史</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomQdrantMemory</span>(<span class="title class_ inherited__">BaseChatMemory</span>):</span><br><span class="line">    <span class="comment"># 访问数据库抽取最相关联的 message_limit 条数据</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">buffer</span>(<span class="params">self, inputs: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]</span>) -&gt; <span class="type">List</span>[BaseMessage]:</span><br><span class="line">        chat_messages: <span class="type">List</span>[BaseMessage] = []</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> inputs:</span><br><span class="line">            <span class="keyword">return</span> chat_messages</span><br><span class="line">        hits = self.qdrant.search(</span><br><span class="line">            collection_name=self.conversation_id,</span><br><span class="line">            query_vector=self.encoder.encode(inputs[<span class="string">&#x27;input&#x27;</span>]).tolist(),</span><br><span class="line">            limit=self.message_limit,</span><br><span class="line">        )</span><br><span class="line">        hits = <span class="built_in">sorted</span>(hits, key=<span class="keyword">lambda</span> x:x.<span class="built_in">id</span>)</span><br><span class="line">        <span class="keyword">for</span> hit <span class="keyword">in</span> hits:</span><br><span class="line">            chat_messages.append(HumanMessage(content=hit.payload[<span class="string">&#x27;human&#x27;</span>]))</span><br><span class="line">            chat_messages.append(AIMessage(content=hit.payload[<span class="string">&#x27;assistant&#x27;</span>]))</span><br><span class="line">        curr_buffer_length = self.llm.get_num_tokens(get_buffer_string(chat_messages))</span><br><span class="line">        <span class="comment"># 如果超出模型最长上下文，则弹出最早的对话历史</span></span><br><span class="line">        <span class="keyword">if</span> curr_buffer_length &gt; self.max_token_limit:</span><br><span class="line">            pruned_memory = []</span><br><span class="line">            <span class="keyword">while</span> curr_buffer_length &gt; self.max_token_limit <span class="keyword">and</span> chat_messages:</span><br><span class="line">                pruned_memory.append(chat_messages.pop())</span><br><span class="line">                curr_buffer_length = self.llm.get_num_tokens(get_buffer_string(chat_messages))</span><br><span class="line">        <span class="keyword">return</span> chat_messages        </span><br></pre></td></tr></table></figure>
<h1 id="虚拟人设">虚拟人设</h1>
<p>流程</p>
<ul>
<li>音视频上传
<ul>
<li>语音转文字 FFmpeg</li>
<li>【初始化时】声纹构建</li>
<li>语音转文字 Whisper</li>
<li>特征语料库构建</li>
</ul></li>
<li>语料 + Chroma 向量库相似度 + KNN 距离匹配</li>
<li>组成输入，LLM 响应结果</li>
</ul>
<p><a href="https://github.com/LC1332/Chat-Haruhi-Suzumiya">Github
参考项目链接</a></p>
<h1 id="python-notes">Python Notes</h1>
<p>def _call() 方法把这个类型的对象当作函数来使用</p>
]]></content>
      <categories>
        <category>LLM应用开发</category>
      </categories>
      <tags>
        <tag>Langchain</tag>
        <tag>向量数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>大模型微调方法</title>
    <url>/LLM-PEFT/</url>
    <content><![CDATA[<blockquote>
<p>因此，近年来研究者们提出了各种各样的参数高效迁移学习方法，即<strong>固定住预训练模型的大部分参数，仅调整模型的一小部分参数来达到与全部参数的微调接近的效果</strong>。调整的可以是模型自有的参数，也可以是额外加入的一些参数。目前的微调方法分为加性方法、选择性方法、重参数化方法和混合方法。</p>
</blockquote>
<h1 id="qa">Q&amp;A</h1>
<p>[Q] 什么情况下模型显存占用大？</p>
<ul>
<li>batch size 大，并行计算；</li>
<li>模型参数量大，每个参数都需要存储空间（精度不同占用空间不同 fp32 fp16
bf16 int8 int4）；</li>
<li>计算过程产生大量的中间变量（如激活值、梯度等），这些中间变量也需要存储在显存中，以便进行后续的计算；</li>
<li>优化器所需的额外信息多（如动量、学习率等）；</li>
<li>模型复杂，反向传播的计算图占用更多显存；</li>
</ul>
<p>[Q] 为什么减少了可训练参数就会减少显存占用？</p>
<ul>
<li>不需要存储它们的梯度；</li>
<li>不需要为不可训练参数分配额外的内存来存储优化状态（如动量、学习率等）；</li>
<li>不需要存储中间激活值（激活值用于在后向传播过程中，计算梯度，进而更新模型的参数）；</li>
</ul>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">以 LP32 精度全量微调一个 1.3B 的模型需要占用多少显存？</span><br><span class="line"></span><br><span class="line">模型权重：1.2G * 4B = 4.8GB</span><br><span class="line">梯度：1.2G * 4B = 4.8GB</span><br><span class="line">优化器状态：1.2G * 4B * 2 = 9.6GB</span><br><span class="line">前向激活值：取决于序列长度、隐层维度、batch size</span><br><span class="line"></span><br><span class="line">实际加载后，占用大约 20GB 的显存</span><br></pre></td></tr></table></figure>
<p>[Q] 模型训练过程中有什么有效的减少显存占用的方法？</p>
<ul>
<li>梯度累计技术：将多个小批量训练数据的梯度进行累积，在达到指定累积次数后，使用累积梯度统一更新一次模型参数，从而达到与大批量数据训练相近的效果；
<ul>
<li>相当于降低了 batch size，因而减少了显存占用；</li>
<li>多个小批量的可训练参数的梯度是存在一起的，不额外占用显存；</li>
</ul></li>
<li>梯度检查点
gradient_checkpoint：在后向传播过程中重新计算前向传播中的某些中间激活值，从而减少显存占用；</li>
</ul>
<p>[Q] 为什么需要 PEFT？</p>
<ul>
<li><p><strong>全量微调大模型</strong>虽然效果还不错，但需要大量的计算资源和训练数据（训练数据中通用数据和领域数据都要有）；</p></li>
<li><p>为了降低计算资源，只<strong>微调模型的某几层参数</strong>时，无法得到较好的微调结果，会出现欠拟合或泛化能力降低的情况，模型无法有效学习领域数据的知识，且可能会破坏了预训练模型的通用语言语义能力；</p></li>
<li><p>当修改了输入 Embedding
或大模型每一层的某些部分时，可以在有限的计算资源上，得到与全量微调近似的微调效果，这便是
PEFT 存在的理由；</p></li>
<li><p>不同领域的全量微调模型需要分开部署，资源占用量大；由于只修改了部分参数，PEFT
中的方法，都具有“<strong>一次部署，多领域共用</strong>”的能力，只需要一次就能加载所有参数，针对不同领域的调用切换不同的
PEFT 参数，即采用该领域微调所增加的小部分参数；</p></li>
</ul>
<p>[Q] 有哪些显存优化策略？</p>
<ul>
<li>高效微调 PEFT；
<ul>
<li>Lora</li>
<li>Prefix-Tuning</li>
</ul></li>
<li>前向激活值；
<ul>
<li>Gradient Accumulation</li>
<li>Gradient Checkpoints</li>
<li>Data Length</li>
</ul></li>
<li>优化器状态；
<ul>
<li>Adafactor Optiomizer</li>
</ul></li>
<li>前向激活值 + 梯度；
<ul>
<li>Freeze Model</li>
</ul></li>
<li>参数量化；
<ul>
<li>QLora</li>
</ul></li>
</ul>
<h1 id="背景">背景</h1>
<p>2018年Bert问世后，NLP
研究的趋势转变为“预训练模型+微调”。虽然更大的模型带来了更好的性能，但以传统的方式对模型进行全量微调会消耗巨大的计算和存储资源，过高的硬件门槛不利于该领域的研究和发展。</p>
<p>在此背景下，参数高效微调和量化技术应运而生，参数高效微调方法仅对模型的一小部分参数（这一小部分可能是模型自身的，也可能是外部引入的）进行训练，便可以为模型带来显著的性能变化，一些场景下甚至不输于全量微调，颇有一种<strong>四两拨千斤</strong>的感觉。</p>
<p>由于训练一小部分参数，极大程度降低了训练大模型的算力需求，不需要多机多卡，单卡即可完成对一些大模型的训练。不仅如此，少量的训练参数对存储的要求同样降低了很多，大多数的参数高效微调方法只需要保存训练部分的参数，与动辄几十
GB 的预训练大模型相比，几乎可以忽略。</p>
<p>当下流行 PEFT 结构一览：</p>
<img data-src="/LLM-PEFT/peft%E6%96%B9%E6%B3%95.jpg" class="" title="peft方法">
<p>常用的参数高效微调方法：</p>
<style>.mqgaymmenrwv{zoom: 50%;}</style>
<img data-src="/LLM-PEFT/image-20240718093213465.png" class="mqgaymmenrwv" alt="image-20240718093213465">
<ul>
<li>Selective：选择模型中的一部分参数进行微调；</li>
<li>Additive：冻结模型原有参数，微调新增的参数和模块；
<ul>
<li>Adapters：</li>
<li>Soft Prompt：</li>
</ul></li>
<li>Reparameterzation：</li>
</ul>
<h1 id="实现方法">实现方法</h1>
<h2 id="开源工具">开源工具</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>开源项目</th>
<th>微调方法</th>
<th>实现方法</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://github.com/hiyouga/LLaMA-Factory">Llama
Factory</a></td>
<td>GaLore、BAdam、DoRA、LongLoRA、LLaMA
Pro、Mixture-of-Depths、LoRA+、LoftQ、PiSSA</td>
<td>peft 包、编写 LongLoRA、LlamaPro
流程【重点是对微调流程的整体架构，包括量化、数据预处理、加速训练、评估和模型存储等，用户可以开箱即用】</td>
</tr>
<tr class="even">
<td><a href="https://github.com/modelscope/swift">Swift</a></td>
<td>LoRA、LoRA+、LLaMA
Pro、GaLore、LISA、UnSloth、SCEdit、NEFTune、LongLoRA、Adapter、IA3、AdaLoRA</td>
<td>peft 包、编写 LongLoRA、LoRA、LlamaPro、Adapter 流程</td>
</tr>
<tr class="odd">
<td><a
href="https://github.com/meta-llama/llama-recipes">llama-recipes</a></td>
<td>LoRA、QLoRA、IA3、AdaLoRA</td>
<td>peft 包</td>
</tr>
<tr class="even">
<td><a href="https://github.com/yangjianxin1/Firefly">Firefly</a></td>
<td>LoRA、QLoRA</td>
<td>直接操作模型结构，为所有全连接添加 Adapter</td>
</tr>
<tr class="odd">
<td><a
href="https://github.com/axolotl-ai-cloud/axolotl">Axolotl</a></td>
<td>LoRA、QLoRA、ReLoRA</td>
<td>peft 包、编写 ReLoRA 流程</td>
</tr>
</tbody>
</table>
<h2 id="开发包">开发包</h2>
<p>Bitsandbytes：训练过程中的参数量化、压缩和加速</p>
<p>Peft（没有Adapter方法的实现）：参数高效微调技术，旨在通过最小化微调参数的数量和计算复杂度来提高预训练模型在新任务上的性能（可以实现）</p>
<p>Adapters：HuggingFace Transformers
的附加库，将各种适配器方法集成到最先进的预训练语言模型中，以最小的训练和推理编码开销。</p>
<p>DeepSpeed：提供了从训练到推理的全面优化方案，包括参数初始化、训练加速、内存优化等多个方面</p>
<h2 id="开源数据集">开源数据集</h2>
<p>MMLU, C-Eval、CMMLU、MNLI、</p>
<p>MMLU C-Eval GSM8K MATH HumanEval MBPP BBH CMMLU</p>
<p>SQuADv2.0、XSum 、C4、GLUE</p>
<h2 id="测评方法">测评方法</h2>
<p>Acc、F1、Rouge-2</p>
<h1 id="微调算法">微调算法</h1>
<blockquote>
<p>不包括，微调图像大模型 Stable Diffusion：lokr
loha；将文本到图像的扩散模型适应下游任务：正交矩阵OFT、蝴蝶微调 Boft
；</p>
</blockquote>
<ul>
<li>基于加法
PEFT：增加额外可训练模型结构或参数（Embedding、MLP、LSTM），如：Prompt
Tuning、Prefix Tuning、Adapter；</li>
<li>基于选择 PEFT：直接微调大模型中的一部分参数，如：BitFit、LN
Tuning；</li>
<li>基于重参数化
PEFT：在已经训练好的参数矩阵的基础上，增加可训练、可合并的参数，如：LoRA、AdaLoRA、PISSA、GaLore；</li>
<li>融合上述三种方法的 PEFT：混合上述三种方法，如：MAM
Adapter、UniPELT；</li>
<li>高效量化：不降低训练效果的前提下，降低计算资源的需求，如：QLoRA；</li>
<li>并行训练：提升训练速度，SLoRA；</li>
<li>工程系统角度的优化：梯度检查点、内存卸载；</li>
</ul>
<p>可训练参数量比较：IA3 &lt; BitFit &lt; Prompt-Tuning &lt; Lora =
Prefix-Tuning</p>
<h2 id="bitfit">BitFit</h2>
<p><strong>只训练模型中的 bias 部分</strong>，冻结其他参数。</p>
<p>优势：</p>
<ul>
<li>简单高效，资源占用量小；</li>
<li>泛化性强，适用各种场景；</li>
</ul>
<p>不足：</p>
<ul>
<li>可解释性差；</li>
<li>强依赖于大模型自身的质量；</li>
</ul>
<h2 id="ln-tuning">LN-Tuning</h2>
<p>单独调整 LayerNorm 模块的 weight 和 bias
参数，训练参数量少，便于和其他的微调方法一起使用。</p>
<style>.cxrzgdqunenw{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723134459641.png" class="cxrzgdqunenw" alt="image-20240723134459641">
<p>实验表明，Prefix-Tuning + LN 的微调效果最佳。</p>
<p>优势</p>
<ul>
<li>参数量低，效果不错；</li>
</ul>
<h2 id="prompt-tuning">Prompt-Tuning</h2>
<p>冻结预训练大模型的全部参数，在训练的输入数据前加入一小段
Prompt，<strong>只训练 Prompt 的表示层，即一个 Embedding
模块</strong>。</p>
<p>其中，Prompt 存在两种形式，分别是 hard prompt 和 soft prompt。hard
prompt 通过人工输入的提示词初始化 prompt，soft prompt 随机初始化
prompt。二者相比，hard prompt 可控性更强，训练速度更快。为了优化Soft
prompt 的效果，后续研究提出了 P-Tuning。</p>
<blockquote>
<p>可训练模块：Embedding(virtual_token, hidden_size)</p>
</blockquote>
<style>.gjmdcubthnqi{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240712160012402.png" class="gjmdcubthnqi" alt="image-20240712160012402">
<p>优势：</p>
<ul>
<li>简化了不同任务之间的转换和迁移，使得模型可以更加灵活地应用于各种场景；</li>
</ul>
<p>不足：</p>
<ul>
<li>预训练模型较小或在Few-shot场景下，微调结果不佳；</li>
</ul>
<h2 id="p-tuning">P-Tuning</h2>
<p>在 Prompt-Tuning 的 Soft Prompt 基础上，对 Prompt
部分进行进一步的编码计算，加速收敛。具体来说，PEFT
包中支持两种编码方式，一种是 LSTM，一种是 MLP。</p>
<blockquote>
<p>可训练模块：Embedding(virtual_token, hidden_size) + MLP/LSTM</p>
</blockquote>
<img data-src="/LLM-PEFT/PT.png" class="" title="PT.png">
<p>优势：</p>
<ul>
<li>通过 Encoder 将离散的 Prompt Embedding 转变为连续的，减少了 Prompt
Embedding 初始化对微调性能的影响，提高了训练的稳定性；</li>
</ul>
<p>不足：</p>
<ul>
<li>增加的 Prompt Encoder 需要额外的计算资源；</li>
</ul>
<h2 id="prefix-tuning">Prefix Tuning</h2>
<p>将可学习的前缀以 past_key_values 的形式放到模型的 attention 的 KV
头部，每个 MHA 中的注意力层共享同一组前缀。</p>
<blockquote>
<p>past _key_values：Transformer 模型中历史计算过的 key 和 value
的结果，最早是用于生成类模型解码加速，解码逻辑是根据历史输入，每次预测一个新的
Token，然后将新的 Token 加入输入，再预测下一个
Token。这个过程中，会存在大量的重复计算，因此可以将 key 和 value
的计算结果缓存，作为 past_key_values
输入到下一次的计算中，这一技术又被称之为 kv cache。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可训练模块：Embedding + MLP，与 P-Tuning 的区别是 prefix 映射到了模型的隐藏层上</span></span><br><span class="line">self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)</span><br><span class="line">self.transform = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(token_dim, encoder_hidden_size),</span><br><span class="line">    torch.nn.Tanh(),</span><br><span class="line">    torch.nn.Linear(encoder_hidden_size, num_layers * <span class="number">2</span> * token_dim),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transformers.models.bloom.BloomAttention 源码实现</span></span><br><span class="line"><span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    past_key, past_value = layer_past</span><br><span class="line">    key_layer = torch.cat((past_key, key_layer), dim=<span class="number">2</span>)</span><br><span class="line">    value_layer = torch.cat((past_value, value_layer), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<style>.tygcieeyswur{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240712161951488.png" class="tygcieeyswur" alt="image-20240712161951488">
<p>优势：</p>
<ul>
<li>拟合速度快；</li>
</ul>
<p>不足：</p>
<ul>
<li>直接修改模型结构，依赖预训练模型本身的结构；</li>
<li>实现复杂，相比于直接修改输入 Prompt Embedding 没有显著优势；</li>
<li>相比于其他的 Soft Prompt 方法，Prefix Tuning
可训练参数量大，每一层的 K V 都需要可训练前缀；</li>
</ul>
<h2 id="p-tuning-v2">P-Tuning V2</h2>
<p>与 Prefix Tuning
最大的区别在于：移除重参数化的编码器，即没有MLP。</p>
<p>通过共享连续提示同时优化多个任务，多任务学习是 P-Tuning v2
的可选功能，可以通过多任务共享前缀进一步提升性能。</p>
<style>.fsmqrthxupgc{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240712162302772.png" class="fsmqrthxupgc" alt="image-20240712162302772">
<p>优势：</p>
<ul>
<li>移除重参数化的编码器，对于较小的模型，会影响模型的表现，且对于大模型提升不大；</li>
<li>引入多任务学习概念，先在多任务的Prompt上进行预训练，然后再适配下游任务；</li>
</ul>
<p>不足：</p>
<ul>
<li>prefix长度在 100~200
时，微调结果最优，但由于模型可接受的最大输入长度有限，随着软提示的参数量增多，实际输入序列的最大长度也会相应减小；</li>
<li>直接优化 prompt 和 prefix 是非单调的，比较难以收敛；</li>
</ul>
<h2 id="diff-pruning">Diff pruning</h2>
<p>不是修改模型的结构，而是通过一个特定任务的 diff
向量扩展基础模型。学习特定于任务的差异向量，该向量扩展了原始预训练参数。diff
向量 <span class="math display">\[\delta_{task}\]</span>
在训练过程中自适应修剪，并加入 𝐿0 范数惩罚的可微近似来鼓励稀疏性。</p>
<p><span
class="math display">\[\theta_{task}=\theta_{pretrained}+\delta_{task}\]</span></p>
<p>不足：</p>
<ul>
<li>需要底层实现来加速非结构化稀疏矩阵的计算，不能直接使用现有的框架；</li>
<li>训练过程中需要存储完整的∆矩阵，相比于Full
finetune并没有降低计算成本；</li>
</ul>
<h2 id="lora">LoRA</h2>
<p>深度网络由大量 Dense
层构成，这些参数矩阵通常是低秩的。相关工作表明，在适应特定任务时，预训练模型是过度参数化的，hidden_size
实际上存在于一个较低的内在维度上，即高维数据实际是在低维子空间中。</p>
<p>因此，将 <span class="math inline">\(\Delta\mathbf{W}\)</span>
用两个更参数量更小的矩阵 <span
class="math inline">\(\mathbf{A}\in\mathbb{R}^{d\times r}\)</span> 和
<span class="math inline">\(\mathbf{B}\in\mathbb{R}^{r\times d}\)</span>
低秩近似（r&lt;d）。其中，矩阵 B 通过高斯函数初始化，矩阵 A
为全零初始化，使得训练开始之前旁路对原模型不造成影响。</p>
<p>具体而言，冻结预训练的模型权重，并将可训练的秩分解矩阵注入到大模型的每个
Attention
层的线性变换中。输入分别与原始权重和两个低秩矩阵进行计算，共同得到最终结果。</p>
<style>.uopkimhqgseu{zoom:40%;}</style>
<img data-src="/LLM-PEFT/image-20240712170241205.png" class="uopkimhqgseu" alt="image-20240712170241205">
<p>训练完成后，可以将两个低秩矩阵与原始模型中的权重进行合并，合并后的模型与原始模型无异，避免了推理期间
Prompt 系列方法带来的额外计算量。</p>
<p><span
class="math display">\[\mathbf{h}=(\mathbf{W}_0+\Delta\mathbf{W})\mathbf{x}=\mathbf{W}_0\mathbf{x}+\mathbf{B}\mathbf{A}\mathbf{x}\]</span></p>
<p>优势：</p>
<ul>
<li>可以将原权重与训练后权重合并，在推理时不存在额外的计算开销；</li>
<li>不增加输入参数，不改变模型结构，简单易用，泛化性强；</li>
<li>提出低秩分解矩阵的思路，在减少计算资源需求的同时，不会丢失预训练原有的通用能力；</li>
</ul>
<p>不足：</p>
<ul>
<li>预先指定每个增量矩阵的内在秩 r
相同，忽略了在微调预训练模型时，权重矩阵的重要性在不同模块和层之间存在显著差异，在实验中可以发现
Wq
的秩更大，即存储的信息更多；因此无法完全捕捉预训练模型的特征，或无法有效地将原始模型的知识迁移到目标任务上；</li>
<li>只训练了self-attention，没有训练 feed-forward networks，事实上 FFN
更重要；</li>
</ul>
<h2 id="ia3">IA3</h2>
<p>抑制和放大内部激活，通过可学习的向量对激活值进行抑制或放大。具体来说，通过一组可训练参数直接调整
K、V、FFN三部分的值，针对不同 batch
的同一位置进行相同的调整（即乘上相同可学习参数），训练过程中冻结原始模型的权重，只更新新增的参数。</p>
<p>训练完成后，与 Lora
类似，可以将学习部分的参数与原始权重合并，没有额外推理开销。</p>
<blockquote>
<p>由于不同大模型结构不同，源码实现有差别。bloom 预训练模型的 QKV 共用
Linear 层 (hidden_size, hidden_size*3)，IA3
的可训练参数同时作用于三者之上。</p>
</blockquote>
<style>.hnzhimvyfkvp{zoom:67%;}</style>
<img data-src="/LLM-PEFT/image-20240718191324572.png" class="hnzhimvyfkvp" alt="image-20240718191324572">
<p>优势：</p>
<ul>
<li>调节的参数少于 Lora，微调效果也不错；</li>
</ul>
<p>不足：</p>
<ul>
<li>学习率对微调效率影响较大，论文中的建议是 3e-3；</li>
</ul>
<h2 id="adalora">AdaLora</h2>
<p>AdaLora
在微调过程中，采用“先探索参数空间，再关注最重要的权重”的方法，不预先指定矩阵的秩，而是动态更新增量矩阵的秩，因为研究发现权重矩阵的重要性在不同模块和层之间存在显著差异。在该微调方法中，需要先找到更加重要的矩阵，给其分配更多的参数（更大的秩），并裁剪不重要的矩阵。相比于Lora，该方法在提升模型效果的同时，降低了参数计算量。</p>
<p>区别于 Lora 的低秩矩阵分解，AdaLora
采用参数化矩阵来模拟奇异值分解（SVD），并舍弃不重要的奇异值，保留奇异向量。由于对一个大矩阵进行精确
SVD
分解的计算消耗非常大，这种方法可以加速计算，同时保留未来恢复的可能性并稳定训练。</p>
<p><span class="math display">\[W=W^{(0)}+\Delta=W^{(0)}+P \Lambda
Q\]</span></p>
<p>计算三元组 QKV
的重要性分数，修剪不太重要的奇异值，将更多预算留给优先级较高的增量矩阵。</p>
<p><span
class="math display">\[S_{k,i}=s\left(\lambda_{k,i}\right)+\frac{1}{d_1}\sum_{j=1}^{d_1}s\left(P_{k,ji}\right)+\frac{1}{d_2}\sum_{j=1}^{d_2}s\left(Q_{k,ij}\right)\]</span></p>
<p>通过灵敏度平滑和不确定性量化，加入累计灵敏度的影响来解决随机采样和复杂的训练动态导致灵敏度估计的变异性大、不确定性大的问题</p>
<p><span
class="math display">\[\begin{aligned}&amp;\bar{I}^{(t)}\left(w_{ij}\right)=\beta_1\bar{I}^{(t-1)}\left(w_{ij}\right)+\left(1-\beta_1\right)I^{(t)}\left(w_{ij}\right)\\&amp;\bar{U}^{(t)}\left(w_{ij}\right)=\beta_2\bar{U}^{(t-1)}\left(w_{ij}\right)+\left(1-\beta_2\right)\left|I^{(t)}\left(w_{ij}\right)-\bar{I}^{(t)}\left(w_{ij}\right)\right|\end{aligned}\]</span></p>
<p><span
class="math display">\[s^{(t)}\left(w_{ij}\right)=\bar{I}^{(t)}\left(w_{ij}\right)\cdot\bar{U}^{(t)}\left(w_{ij}\right)\]</span></p>
<p>可以通过梯度下降的方式对<span class="math inline">\(P, \Lambda,
Q\)</span>进行更新，将不需要的奇异值 λ 剪裁 Mask，后续仍可以加入训练</p>
<p><span
class="math display">\[\Lambda_k^{(t+1)}=\mathcal{T}\left(\tilde{\Lambda}_k^{(t)},S_k^{(t)}\right),\text{
with
}\mathcal{T}\left(\tilde{\Lambda}_k^{(t)},S_k^{(t)}\right)_{ii}=\begin{cases}\tilde{\Lambda}_{k,ii}^{(t)}&amp;S_{k,i}^{(t)}\text{
is in the top- }b^{(t)}\text{ of }S^{(t)}\\0&amp;\text{
otherwise}&amp;\end{cases}\]</span></p>
<p>训练损失中添加了额外的惩罚项，以规范奇异矩阵P和Q的正交性，从而避免SVD的大量计算并稳定训练。</p>
<p><span class="math display">\[R(P,Q)=\left\|P^\top
P-I\right\|_\mathrm{F}^2+\left\|QQ^\top-I\right\|_\mathrm{F}^2\]</span></p>
<p>最终，微调的损失函数为</p>
<p><strong>重要结论</strong></p>
<style>.muxykhodbssi{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723112058468.png" class="muxykhodbssi" alt="image-20240723112058468">
<p>AdaLoRA 总是倾向于将更多预算分配给 FFN 和顶层，即 FFN
模块和顶层的权重矩阵对于模型性能更为重要。</p>
<h2 id="adapter-tuning">Adapter Tuning</h2>
<p>在 Transformer Layer 的 Self-Attetion 和 FFN
之后插入一个先降维再升维的 MLP（以及一层残差和Layer
Normalization）来学习模型微调的知识。</p>
<style>.rkpuwvtgpxjy{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723101505442.png" class="rkpuwvtgpxjy" alt="image-20240723101505442">
<p>图中，Adapter 即插入的 FF Down + 非线性激活层 + FF
up。在微调过程中，固定原始模型的参数，只微调适配层。</p>
<p>不足：</p>
<ul>
<li>需要修改原有模型结构，会增加模型参数量；</li>
</ul>
<p>变体：</p>
<ul>
<li><a
href="https://aclanthology.org/2020.findings-emnlp.41.pdf">Paper</a>：仅在
MLP 模块之后和 LayerNorm 之后使用适配器层</li>
</ul>
<h2 id="adapterfusion">AdapterFusion</h2>
<p>整合来自多个任务的知识的方法时，常遇到灾难性遗忘和多任务数据集平衡方面的困难。AdapterFusion
分离知识提取和知识组合这两个阶段，可以以非破坏性的方式有效地利用从多个任务中学到的表示。知识提取阶段学习Adapter
任务特定参数，其封装了特定于任务的信息；知识组合阶段将不同任务的 Adapter
组合到一起。</p>
<style>.fqsvwkiyqncl{zoom:60%;}</style>
<img data-src="/LLM-PEFT/image-20240723102934378.png" class="fqsvwkiyqncl" alt="image-20240723102934378">
<style>.juuidhgfhpif{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723103303792.png" class="juuidhgfhpif" alt="image-20240723103303792">
<p>AdapterFusion
组件将在不同任务上训练的多个适配器的输出作为输入，并学习编码信息的参数化混合器，应用于单个任务多
Adapter 的场景和多任务混合的场景。单任务场景中，可以多个 Adapter
可用于提取同一下游任务不同维度的信息，再将他们融合起来。</p>
<p>优势：</p>
<ul>
<li>优于在单任务和多任务中训练的 Adapter
模型，可以视为多Adapter提取不同维度的信息，AdapterFusion
将他们融合起来。</li>
</ul>
<h2 id="adapterdrop">AdapterDrop</h2>
<p>该方法在不影响任务性能的前提下，动态高效的移除冗余的
Adapter，可以尽可能地减少模型的参数量，提高模型在反向传播（训练）和正向传播（推理）时的效率。</p>
<style>.cnrqbhqmcgzg{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723103450147.png" class="cnrqbhqmcgzg" alt="image-20240723103450147">
<p>优势：</p>
<ul>
<li>通过从较低的 Transformer 层删除可变数量的Adaper来提升推理速度。
当对多个任务执行推理时，动态地减少了运行时的计算开销，并在很大程度上保持了任务性能；</li>
</ul>
<h2 id="mam-adapter">MAM Adapter</h2>
<p>分解了最先进的参数高效迁移学习方法的设计，并提出了一个在它们之间建立联系的统一框架。</p>
<style>.svruklvbktit{zoom: 50%;}</style>
<img data-src="/LLM-PEFT/image-20240723104300363.png" class="svruklvbktit" alt="image-20240723104300363">
<h2 id="unipeft">UniPEFT</h2>
<p>不同的 PEFT
方法在同一任务上的表现不同，因此为特定任务选择最合适的方法并非易事，特别是考虑到新
PEFT 方法和任务数量的快速增长。鉴于模型的多样性和模型选择的难度，UniPEFT
提出了一个统一的框架 UniPEFT，它将不同的PEFT
方法作为子模块，并通过门控机制学习激活最适合当前数据或任务设置的方法。</p>
<style>.xnlmrxehdbtj{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723104910206.png" class="xnlmrxehdbtj" alt="image-20240723104910206">
<h2 id="pissa">PISSA</h2>
<blockquote>
<p>peft 包中有 PISSA 初始化方法
<code>self.pissa_init(adapter_name, init_lora_weights)</code></p>
</blockquote>
<p>PISSA 和 LoRA 主要的区别是初始化方式不同。同样基于低秩特性的假设，但
PISSA 不是去近似，而是直接基于原矩阵操作。</p>
<p>PiSSA 对预训练模型的参数矩阵<span class="math display">\[W\in
R^{m\times n}\]</span> 进行奇异值分解，其中前 r
个奇异值和奇异向量用来初始化适配器 (adapter) 的两个矩阵<span
class="math display">\[A\in R^{m\times r}\]</span> 和<span
class="math display">\[B\in R^{r\times n}\]</span> ，； <span
class="math display">\[r\ll\min(m,n)\]</span>
剩余的奇异值和奇异向量用来构造残差矩阵<span
class="math display">\[W^{res}\in R^{m\times n}\]</span> ，使得<span
class="math display">\[W=AB+W^{res}\]</span>
。因此，适配器中的参数包含了模型的核心参数，而残差矩阵中的参数是修正参数。通过微调参数量较小的核心适配器
A、B，冻结参数量较大的残差矩阵 <span
class="math display">\[\text{Wres}\]</span>
，就达成了用很少的参数近似全参数微调的效果。</p>
<style>.qzfudtmrbgax{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723093258953.png" class="qzfudtmrbgax" alt="image-20240723093258953">
<p>优势</p>
<ul>
<li>相比于 LoRA，收敛速度更快，性能更好</li>
</ul>
<h2 id="vera">VeRA</h2>
<p>区别于Lora，Vera 将 A 和 B
矩阵按照高斯分布随机初始化并冻结，只训练两组直接和A、B相乘的一维参数。虽然直观看起来A和B像两个无用的张量，但实际上它们仍然是必不可少的，实验证明即使是随机张量也可以用于微调。</p>
<style>.cvbmapxkbpip{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723100503853.png" class="cvbmapxkbpip" alt="image-20240723100503853">
<p>优势：</p>
<ul>
<li>VeRA显著减少了可训练参数的数量（LoRA相比参数减少了10倍），而精度没有损失；</li>
<li>缩放向量尺寸小，可以将许多版本驻留在单个GPU的有限内存中，从而大大提高了服务效率，适合于需要频繁交换大量微调模型的场景，比如针对个人用户个性化的基于云的人工智能服务；</li>
</ul>
<h2 id="dora">DoRA</h2>
<p>将每个高阶矩阵都分解为 1*k 大小矩阵和 d*k 方向矩阵的乘积，LoRA
倾向于同时改变幅度和方向，DoRA可以更容易地将二者分开调整，或者用另一个的负变化来补偿一个的变化。</p>
<style>.eusjkhbdllby{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240723110508921.png" class="eusjkhbdllby" alt="image-20240723110508921">
<h2 id="slora">SLORA</h2>
<p>一个 GPU 上并行执行多个 lora adapters 的微调</p>
<p>S-LoRA 能够在单个 GPU 上或跨多个 GPU 以较小的开销为数千个 LoRA
适配器提供服务。该方法将所有 LoRA
模块存储在主内存中，并将当前运行的查询使用的适配器获取到 GPU
内存，提出采用统一分页技术，使用统一的内存池来管理具有不同等级的动态适配器权重和具有不同序列长度的
KV 缓存张量。此外，该方法采用新颖的张量并行策略和高度优化的定制 CUDA
内核，可视为定制化的 LoRA微调高效计算流程。</p>
<h2 id="galore">GaLore</h2>
<p>梯度低秩投影（GaLore）是一种全量参数学习的训练策略，但比常见的低秩自适应方法（LoRA）更节省内存。其关键思想是利用权重矩阵
W 的梯度 <span class="math display">\[G\in \mathbb{R}^{m\times
n}\]</span> 缓慢变化的低秩结构，而不是试图将权重矩阵本身近似为低秩。</p>
<p>优势：</p>
<ul>
<li>节省内存（三倍）；</li>
<li>性能不错；</li>
</ul>
<p>不足：</p>
<ul>
<li>训练速度慢（三倍）；</li>
</ul>
<h2 id="lora-1">LoRA+</h2>
<p>LoRA 中的适配器矩阵 A 和 B 以相同的学习率更新，实验表明对 A 和 B
使用相同的学习率并不能实现有效的特征学习，LoRA+ 通过精心选择的固定比率为
LoRA 适配器矩阵 A 和 B 设置不同的学习率，纠正 LoRA 的这种次优性。</p>
<style>.gjrcvwtzefci{zoom: 67%;}</style>
<img data-src="/LLM-PEFT/image-20240725102452954.png" class="gjrcvwtzefci" alt="image-20240725102452954">
<h2 id="longlora">LongLoRA</h2>
<p>LongLoRA
扩展了模型的上下文，同时保留了其原始架构，并且与大多数现有技术兼容。一方面，虽然推理过程中需要密集的全局注意力，但微调过程更需要稀疏的局部注意力实现有效且高效反向传播。由此，该方法提出转移稀疏注意力</p>
<p><span class="math display">\[S^2-Attn\]</span></p>
<p>有效地实现了上下文扩展。另一方面，除了在线性层中训练 LoRA
权重之外，LongLoRA
还进一步使嵌入层和归一化层变得可训练，由此可以表现出较好的性能。从实现上来看，该方法在训练中只需两行代码即可实现，且在推理中是可选的，不会占用额外计算资源。</p>
<img data-src="/LLM-PEFT/image-20240725143254778.png" class="" title="image-20240725143254778">
<h2 id="rslora">RsLoRA</h2>
<p>LoRA通过在选定层添加可训练的低秩Adapter来实现参数有效的微调。每个LoRA由两个低秩矩阵乘积组成，并乘以一个与秩相关的因子，传统的LoRA采用“直接除以秩的因子”的方法过于激进，导致高秩Adapter的学习速度减缓，性能受限。因此，在实际应用中，LoRA通常仅限于使用非常低的秩。</p>
<p>rsLoRA深入研究了 LoRA 的缩放因子对学习过程的影响，并证明了 LoRA
应该除以秩的平方根而不是秩，通过使用较大的秩在训练期间增加计算资源以获得更好的微调性能，同时不改变推理计算成本。</p>
<style>.fblydzasdnll{zoom:67%;}</style>
<img data-src="/LLM-PEFT/image-20240903102620304.png" class="fblydzasdnll" alt="image-20240903102620304">
<h2 id="llama-pro">LLaMA Pro</h2>
<p>该方法提出的Block
Expansion方法，即块扩展，在保持预训练模型参数不变的基础上，增加新的block来适应新的训练任务。这些新加入的block与原有block协同工作，既保留了模型原有的知识，又能够适应新的训练数据和任务需求。</p>
<style>.igqfesfrmusl{zoom:50%;}</style>
<img data-src="/LLM-PEFT/image-20240729142017433.png" class="igqfesfrmusl" alt="image-20240729142017433">
<h2 id="方法性能对比">方法性能对比</h2>
<p>从方法类型、是否存储高效、是否内存高效、反向传播成本、推理开销五个维度比较
PEFT 方法：</p>
<img data-src="/LLM-PEFT/image-20240714150443014.png" class="" title="image-20240714150443014">
<p>各种参数高效方法的参与训练的参数量、最终模型与原始模型的改变参数（delta值）以及论文中参与评估的模型的范围：</p>
<img data-src="/LLM-PEFT/image-20240714150638355.png" class="" title="image-20240714150638355">
<p>PEFT
方法在何种模型大小上进行过评估，以及在论文中通常使用的可训练参数数量。我们所说的可训练参数数量是指由梯度优化算法更新的参数数量，而不是我们用“更改参数”表示的原始模型与最终模型之间的差异。对于重参数化方法，我们报告了重参数化前后的参数数量。由于S4模型在不同层使用了不同的方法，因此估算其更新后的参数数量比较复杂。我们报告了在已发表的文献中对这些方法进行评估的范围。</p>
<p>近年来，基于Transformer架构的大语言模型（LLM），在众多自然语言处理（NLP）任务达到了最佳性能表现。目前，构建领域专用LLM的主流策略是：在海量通用数据集上预训练的大语言模型（PLM），并通过针对特定下游任务的微调（Fine-tuning）。相较于直接采用PLM，针对下游任务的微调能显著提升模型性能。然而，随着LLM参数规模急剧增长，在消费级硬件上进行微调变得不切实际。</p>
<p>为应对此挑战，研究者们近年来提出了参数高效微调技术（Parameter-Efficient
Fine-Tuning,
PEFT），其核心思想在于固定PLM的大部分参数，仅调整一小部分关键参数或引入额外参数，以接近直接微调PLM的效果。这种方法不仅保留了PLM的广泛知识库，还显著降低了计算与存储需求，为在资源受限环境下高效微调LLM开辟了新路径。</p>
<h1 id="高效量化">高效量化</h1>
<h2 id="qlora">QLoRA</h2>
<p>创新点：</p>
<ul>
<li>4位NormalFloat（NF4）：这是一种针对正态分布权重设计的信息理论上最优的量化数据类型。相较于传统的4位整数和4位浮点数，NF4为正态分布数据提供了更优异的实证性能。</li>
<li>双重量化：QLora引入了对量化常数的二次量化，进一步减小了缓存占用。这种双重量化机制包含对普通参数的一次量化和对量化常数的再量化，从而在不牺牲精度的前提下进一步压缩模型。</li>
<li>分页优化器：这是一种智能的内存管理技术，当GPU内存不足时，它可以将部分数据移到系统内存中，并在需要时调回GPU内存。这有助于处理长序列或大批量数据时的内存峰值问题。</li>
</ul>
<p>优势：</p>
<ul>
<li>提出了高效且有效的量化技术，使在单个GPU上微调超大型参数模型成为可能；</li>
</ul>
<h2 id="loftq">LoftQ</h2>
<p>该方法一种新的量化框架，在对 LLM 进行量化的同时，为 LoRA
微调找到合适的低秩初始化。这种初始化减轻了量化模型和全精度模型之间的差异，并显着提高了下游任务的泛化能力。</p>
<h2 id="gptq">GPTQ</h2>
<h2 id="awq">AWQ</h2>
<p>GGUF</p>
<h1 id="微调方法适配">微调方法适配</h1>
<blockquote>
<p>如何将微调模型应用在 自定义/未被官方适配 的大模型上</p>
</blockquote>
<p>通过 <code>target_modules</code> 和 <code>modules_to_save</code>
实现自定义可训练参数和模块</p>
<p>Merge Peft Model：将预训练好的 Lora 或 IA3
的模型参数融入预训练模型中，实现不修改原模型的结构和参数量</p>
]]></content>
      <categories>
        <category>LLM微调</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM-RLHF</title>
    <url>/LLM-RLHF/</url>
    <content><![CDATA[<p>TODOLIST</p>
<p>RM + PPO</p>
<p>DPO</p>
<p>ReMax和GRPO</p>
<p>RLOO</p>
]]></content>
  </entry>
  <entry>
    <title>RAG 架构和 Langchain 框架</title>
    <url>/LLM-RAG-Langchain/</url>
    <content><![CDATA[<p>当下，微调（Fine-Tuning）和检索增强生成（Retrieval-Augmented
Generation，简称RAG）是大型语言模型（LLM）与专有数据之间融合贯通的最主流的两种方法。微调对数据集和硬件要求高，如果没有足够大的平台很难深入研究，因此本文主要涉及检索增强生成（RAG架构）领域知识。</p>
<h1 id="概念">概念</h1>
<p>RAG 是一种使用额外数据增强 LLM 知识的技术，是 2020 年发表的论文 <a
href="https://arxiv.org/abs/2005.11401">面向知识密集型 NLP
任务的检索增强生成</a>中提出的新思想。LLM
通过外部知识源获取额外信息，从而生成更准确、更符合上下文的答案，并减少错误信息（或称为“幻觉”，即模型对用户意图的误解或在处理特定指示时产生了不准确的推断）。典型的
RAG 应用程序有两个主要组件：</p>
<ul>
<li>索引：用于引入源数据并对其进行<strong>索引</strong>的管道，通常离线运行。
<ul>
<li>加载：将数据库中的大段文本读入系统</li>
<li>拆分：大块数据不便于搜索，且模型的上下文窗口有限，因此需要拆分数据</li>
<li>存储：采用向量数据库和索引存储数据</li>
</ul></li>
<li>检索和生成：实际的 RAG
链，它在运行时接受用户查询并从索引中检索相关数据，然后将其传递给模型。
<ul>
<li>检索：将用户的查询通过嵌入模型转化为向量，以便与向量数据库中的其他上下文信息进行比对。通过这种相似性搜索，可以找到向量数据库中最匹配的前
k 个数据</li>
<li>生成：将用户的查询和检索到的额外信息一起嵌入到一个预设的提示模板中，这个经过检索增强的提示内容会被输入到大语言模型
(LLM) 中，以生成所需的输出</li>
</ul></li>
</ul>
<p>从原始数据到响应生成最常见的流程图如下：</p>
<img data-src="/LLM-RAG-Langchain/x7ta8v77.png" class="" title="x7ta8v77">
<p>RAG 测评指标：</p>
<style>.pvofhatyoehi{zoom:50%;}</style>
<img data-src="/LLM-RAG-Langchain/image-20240820085527076.png" class="pvofhatyoehi" alt="image-20240820085527076">
<h1 id="langchain">Langchain</h1>
<p>LangChain
采用组件化设计的思想，将语言模型开发分为多个子任务：<strong>对话历史
Memory、提示工程 Prompt、输出解析 Parase、LLM链 Chain、索引
Indexes、代理 Agents</strong>。Langchain
模块化设计的中心思想是提升代码维护、扩展和重用的能力，可以快速开发多轮提示以及解析输出的应用。</p>
<p>此外，Langchain
还提供了对话过程中需要的基本功能，如：<strong>文档加载器
UnstructuredBaseLoader、文档分割器 TextSplitter、向量数据库存储和搜索
BaseChatMemory、多种工具类链调用 MapReduceDocumentsChain</strong></p>
<p>还有额外的功能如：用量记录、数据流返回 acall、缓存
SQLite、支持多种模型接口 OpenAI、向量数据库 langchain.vectorstores</p>
<h2 id="模型链-chain">模型链 Chain</h2>
<p>链（Chains）通常将大语言模型（LLM）与其他组件组合在一起来构建更复杂的链。比如，LLMChain
允许我们对创建的 Prompt 使用大模型；</p>
<p>Chain 基类是所有 Chain
对象的起点，处理输入、输出、历史和回调等功能，支持同步和异步调用，内部组件也可以通过回调进行交互；</p>
<p>自定义 Chain 需要继承 Chain 基类，实现 _call/_acall
方法定义调用逻辑；</p>
<h2 id="对话历史-memory">对话历史 Memory</h2>
<p>根据需求可以将历史存储在
SQLite、qdrant等数据库中，下面代码将历史存储在缓存中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.memory <span class="keyword">import</span> ConversationBufferMemory</span><br><span class="line">memory = ConversationBufferMemory(</span><br><span class="line">    memory_key=<span class="string">&quot;history&quot;</span>, <span class="comment"># 与 prompt 的输入变量保持一致。</span></span><br><span class="line">    return_messages=<span class="literal">True</span> <span class="comment"># 将以消息列表的形式返回聊天记录，而不是单个字符串</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="提示工程-prompt">提示工程 Prompt</h2>
<p>Prompt 很少是写明不变的，通常从多个组件构建而成的。 PromptTemplate
负责构建这个输入。</p>
<p><strong>输出解析 Parase</strong> 是提示工程的一种：</p>
<ul>
<li><code>get_format_instructions() -&gt; str</code>：方法，返回一个包含有关如何格式化语言模型输出的字符串，即提示
Prompt。</li>
<li><code>parse(str) -&gt; Any</code>：方法，接受一个字符串（假定为语言模型的响应）并将其解析为某个结构。</li>
<li><code>parse_with_prompt(str) -&gt; Any</code>：一个方法，它接受一个字符串（假设是语言模型的响应）和一个提示（假设是生成这样的响应的提示），并将其解析为某种结构。提示在此大多数情况下是为了提供信息以便
OutputParser 重新尝试或以某种方式修复输出。</li>
</ul>
<h2 id="索引-indexes">索引 Indexes</h2>
<p>结构化文档，以便 LLM 可以与外部文档交互。
LangChain有许多模块可帮助您加载、结构化、存储和检索文档。<a
href="https://yeqiuo.github.io/LLM%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/#%E7%A7%81%E6%9C%89%E6%95%B0%E6%8D%AE%E7%9F%A5%E8%AF%86%E5%BA%93">详见</a></p>
<h2 id="代理-agents">代理 Agents</h2>
<p>代理涉及 LLM
做出行动决策（Observation）、执行该行动（Action）、查看一个观察结果（Observation），并重复该过程直到完成。LangChain
提供了一个标准的代理接口，一系列可供选择的代理，以及端到端代理的示例。</p>
<ul>
<li>使用工具并观察其输出</li>
<li>生成相应返回给用户</li>
</ul>
<h1 id="文本匹配任务">文本匹配任务</h1>
<blockquote>
<p>RAG
将匹配分为多个阶段，可分为：粗召回、细召回、粗排序、精排序、再排序。该任务包括多个子任务，如文本相似度计算、问答匹配、对话匹配，类似于RAG的文本抽取式阅读理解和多项选择</p>
</blockquote>
<p>RAG 采取召回（IF-IDF、BM25）、粗排（双塔 Bert
模型、Sentence-Bert、text2vec、uniem）、精排（单塔 Bert
模型），得到相关的文档输入 LLM 中。</p>
<h2 id="基于词匹配">基于词匹配</h2>
<blockquote>
<p>传统方法将词匹配、词距等分数作为特征，用线性模型或树模型预测相关性，效果远不如深度学习。</p>
</blockquote>
<p>将查询文本分词，词在文档 d 中出现的次数越多，则查询文本和文档 d
越相关</p>
<h3 id="if-idf">IF-IDF</h3>
<p><strong>概念</strong></p>
<p>词袋模型：（bag of words）只考虑词频，不考虑词的顺序和上下文</p>
<p>词频 TF：每个词在文档中出现的次数的集合；<span
class="math inline">\(\sum_{t\in\mathcal{Q}}\operatorname{tf}_{t,d}\)</span></p>
<ul>
<li>缺陷：文档越长，TF 越大；解决：除以文档长度，归一化；<span
class="math inline">\(\sum_{t\in
Q}\frac{\mathrm{tf}_{t,d}}{l_d}\)</span></li>
<li>缺陷：每个词重要性不同；解决：语义重要性（term
weight），在文档中出现的越多，权重越低；</li>
</ul>
<p>文档频率 DF：词 t 在多少文档中出现过，定义“词”区别文档的能力；</p>
<p>逆文档排序 IDF：衡量一个词在 N 个文档中的重要性；<span
class="math inline">\(\mathrm{idf}_t=\mathrm{log}\frac
N{\mathrm{df}_t}\)</span></p>
<p><span class="math inline">\(\mathrm{TFIDF}(\mathcal{Q},d) =
\sum_{t\in\mathcal{Q}} \frac{\mathrm{tf}_{t,d}}{l_{d}} \cdot
\mathrm{idf}_{t}.\)</span></p>
<p>其中，查询词q的分词后得到 Q 集合，它与文档 d 的相关性用 TF-IDF
衡量；结果还取决于所采取的分词算法；</p>
<h3 id="bm25">BM25</h3>
<p>IF-IDF 的变种，k 和 b 是参数（通常设置为 k∈[1.2, 2]，b=0.75）</p>
<p><span class="math inline">\(\sum_{t\in
Q}\frac{\mathrm{tf}_{t,d}\cdot(k+1)}{\mathrm{tf}_{t,d}+k\cdot\left(1-b+b\cdot\frac{l_d}{\mathrm{mean}(l_d)}\right)}\cdot\ln\left(1+\frac{N-\mathrm{df}_t+0.5}{\mathrm{df}_t+0.5}\right)\)</span></p>
<h3 id="基于词距">基于词距</h3>
<p>两个词在文档中出现位置之间，间隔的词越少越可能相关；</p>
<p>简而言之，查询词切分后的 term
在文档中出现的次数越多越好，任意两个词之间的距离越近越好；</p>
<p>eg：<strong>OkaTP</strong></p>
<h2 id="基于深度学习">基于深度学习</h2>
<p><strong>基于交互策略的单塔模型</strong> 准确度更高</p>
<p>Bert 输出 similarity，二分类任务（相似/不相似）</p>
<p><strong>基于向量匹配的双塔模型 </strong>速度更快</p>
<p>Bert 输出 Sentence_Embedding，拟合 cos_Similarity（回归任务）</p>
<h1 id="评价指标">评价指标</h1>
<p>二分类评价指标 AUC</p>
<style>.lltkybonzgvg{zoom: 33%;}</style>
<img data-src="/LLM-RAG-Langchain/image-20240815155230728.png" class="lltkybonzgvg" alt="image-20240815155230728">
<h1 id="可信度-rag">可信度 RAG</h1>
<p>当检索返回的结果有错误或信息丢失时，会导致LLM回复出现幻觉。</p>
<p>为解决这个问题的三个指标：可信度（Faithfulness）、答案相关性（Answer
Relevance）、上下文相关性（Context Relevance）</p>
<h1 id="query-预处理">Query 预处理</h1>
<p>同义Query，</p>
<p>意图识别</p>
<h1 id="召回">召回</h1>
<p>索引技术：倒排索引、压缩倒排索</p>
<p>检索模型：BM25、BERT</p>
<p>引入上下文信息，更好地理解用户意图</p>
]]></content>
      <categories>
        <category>LLM应用开发</category>
      </categories>
      <tags>
        <tag>RAG架构</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础</title>
    <url>/Machine-Learning/</url>
    <content><![CDATA[<h1 id="偏差方差">偏差方差</h1>
<p><a
href="https://www.cnblogs.com/makefile/p/bias-var.html#fn2">参考文章</a></p>
<h2 id="泛化误差">泛化误差</h2>
<p>模型的平方预测误差的期望</p>
<p><span
class="math display">\[Err(\mathbf{x})=E\left[\left(y-f(\mathbf{x};D)\right)^2\right]=\mathbb{E}_D\left[\left(f(\boldsymbol{x};D)-\bar{f}\left(\boldsymbol{x}\right)\right)^2\right]+\left(\bar{f}(\boldsymbol{x})-y\right)^2+\mathbb{E}_D\left[(y_D-y)^2\right]\]</span></p>
<h2 id="方差">方差</h2>
<blockquote>
<p>方差是指模型对于不同训练集的预测结果的波动程度（即泛化能力），<strong>刻画了数据扰动所造成的影响</strong>。高方差意味着模型过于复杂，对训练数据中的噪声和细节过于敏感，表示模型<strong>过拟合</strong></p>
</blockquote>
<p>使用样本数相同的不同训练集产生的方差</p>
<p><span
class="math display">\[\mathrm{var}(\mathbf{x})=\mathrm{E}_\mathrm{D}\left[\left(\mathrm{f}(\mathbf{x};\mathrm{D})-\overline{\mathrm{f}}(\mathbf{x})\right)^2\right]\]</span></p>
<p>其中，算法 𝑓 对测试样本 𝑥 的期望预测（不同训练集 𝐷 上模型 𝑓
对测试样本 𝑥 的预测输出的期望）为</p>
<p><span
class="math display">\[\overline{f}(\mathbf{x})=E_D[f\left(\mathbf{x};D\right)]\]</span></p>
<h2 id="偏差">偏差</h2>
<blockquote>
<p>偏差是指模型的预测值与真实值之间的差异，<strong>模型本身的拟合能力</strong>。高偏差意味着模型过于简单，无法捕捉到数据的复杂模式，表示模型<strong>欠拟合</strong></p>
</blockquote>
<p>期望预测与真实标记的误差</p>
<p><span
class="math display">\[\mathrm{bias}^2(\mathbf{x})=\left(\overline{\mathrm{f}}(\mathbf{x})-\mathrm{y}\right)^2\]</span></p>
<h2 id="噪声">噪声</h2>
<blockquote>
<p>表达了当前任务上任何模型所能达到的期望泛化误差的下界，<strong>刻画了学习问题本身的难度</strong>。</p>
</blockquote>
<p>真实标记与数据集中的实际标记间的偏差</p>
<p><span
class="math display">\[\varepsilon^2=E_D\left[(y_D-y)^2\right]\]</span></p>
<h2 id="偏差-方差窘境">偏差-方差窘境</h2>
<img data-src="/Machine-Learning/606386-20180722194316424-288674381.png" class="" title="bias-variance-tradeoff">
<ul>
<li>偏向左侧时，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导泛化误差，称为<strong>欠拟合现象</strong>
<ul>
<li>增加模型的迭代次数；更换描述能力更强的模型；生成更多特征供训练使用；降低正则化水平</li>
</ul></li>
<li>偏向右侧时，模型的拟合能力非常强，数据轻微变化都能导致模型发生变化，如果过分学习训练数据的特点，则会发生<strong>过拟合现象</strong>
<ul>
<li>扩增训练集；减少训练使用的特征的数量；降低模型复杂度；提高正则化水平</li>
</ul></li>
</ul>
<p>为了解决“偏差和方差在一定程度上是有冲突的”的问题，可以采用K折交叉验证或正则化，它们分别从数据划分和模型复杂度两个角度入手，提升模型的泛化能力和稳健性：</p>
<ul>
<li>K折交叉验证通过多次划分和验证，减少因数据划分不当而导致的性能估计偏差</li>
<li>正则化在损失函数中加入一个与模型复杂度相关的惩罚项，以限制模型的复杂度并防止过拟合
<a href="https://www.cnblogs.com/zingp/p/10375691.html">参考文章</a>
<ul>
<li>L1正则化是指权值向量w中各个元素的绝对值之和，可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，进而可以用于特征选择。二维平面图像：<img data-src="/Machine-Learning/o_L1.png" class="" title="image"></li>
<li>L2正则化是指权值向量w中各个元素的平方和然后再求平方根，让权值尽可能小，如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是<strong>抗扰动能力强</strong>。二维平面图像：<img data-src="/Machine-Learning/o_L2.png" class="" title="image"></li>
</ul></li>
</ul>
<h1 id="模型训练">模型训练</h1>
<h2 id="优化器">优化器</h2>
<p>Adam：自适应学习率，通过计算梯度的一阶矩（均值）和二阶矩（未中心化的方差）来调整学习率</p>
<p>AdamW：带有权重衰减和学习率预热的Adam优化器。改进了Adam，将权重衰退和梯度更新解耦，把L2正则化移到权重更新时而不是梯度计算时</p>
<h1 id="训练方法">训练方法</h1>
<h2 id="监督学习">监督学习</h2>
<p>训练数据既有特征(feature)又有标签(label)，通过训练，让机器可以自己找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签。</p>
<h2 id="无监督学习">无监督学习</h2>
<p>数据只有特征(feature)无标签(label)，是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。</p>
<h2 id="半监督学习">半监督学习</h2>
<p>利用同时包含有标签和无标签的数据来构建一个模型，使得模型能够在测试阶段更好地泛化到新的、未见过的数据。</p>
<h2 id="强化学习">强化学习</h2>
<p>让一个智能体（agent）在环境（Enviroment）中通过尝试和错误来学习行为（Action）策略。智能体通过与环境进行交互，根据奖励信号来调整其行为策略，以达到最大化累积奖励的目标。</p>
<p>针对 PPO 算法的优化：</p>
<ul>
<li>归一化奖励分数、将奖励分数做白化处理</li>
</ul>
<img data-src="/Machine-Learning/overview_chatgpt.png" class="" title="overview_chatgpt">
<h3 id="rlhf">RLHF</h3>
<blockquote>
<p>Reinforcement Learning from Human
Feedback，以<strong>强化学习方式依据人类反馈优化语言模型</strong>，是
OpenAI 提出的生成领域（Decoder_only）的新训练范式</p>
</blockquote>
<p>Policy Gradient</p>
<style>.rcsshtxxrksr{zoom:50%;}</style>
<img data-src="/Machine-Learning/image-20240716101748533.png" class="rcsshtxxrksr" alt="image-20240716101748533">
<p>RM+PPO</p>
<p>步骤一、预训练语言模型</p>
<style>.glvvsqhprcsi{zoom: 50%;}</style>
<img data-src="/Machine-Learning/image-20240715191912939.png" class="glvvsqhprcsi" alt="image-20240715191912939">
<p>步骤二、训练奖励模型 RM</p>
<style>.hmdmoqlgqhlt{zoom: 50%;}</style>
<img data-src="/Machine-Learning/image-20240715191936226.png" class="hmdmoqlgqhlt" alt="image-20240715191936226">
<p>步骤三、用强化学习 PPO 微调</p>
<style>.hmxpzplobavc{zoom:50%;}</style>
<img data-src="/Machine-Learning/image-20240715192056550.png" class="hmxpzplobavc" alt="image-20240715192056550">
<p>DPO</p>
<p>KTO</p>
<p>ORPO</p>
<h1 id="损失函数">损失函数</h1>
<p>MSELoss 回归任务</p>
<p>CrossEntropyLoss 单标签分类</p>
<p>BCEWithLogitsLoss 多标签分类</p>
<h1 id="torch">Torch</h1>
<p><code>torch.nn.Parameter()</code>将一个不可训练的 tensor
转换成可以训练的类型 parameter，并将这个 parameter 绑定到这个 module
里面</p>
]]></content>
  </entry>
  <entry>
    <title>LLM</title>
    <url>/LLM/</url>
    <content><![CDATA[<blockquote>
<p>大模型核心技术点在于训练资源、数据清洗和数据配比等，其难点或许不在于模型训练和模型结构搭建</p>
</blockquote>
<p>前缀语言模型 Encoder-Decoder</p>
<p>ChatGLM</p>
<p>掩码语言模型 自编码模型 Encoder-Only</p>
<p>因果语言模型 自回归模型 Decoder-Only</p>
]]></content>
  </entry>
  <entry>
    <title>计算机网络</title>
    <url>/Network/</url>
    <content><![CDATA[<h1 id="概览">概览</h1>
<p>OSI
七层模型：应用层、表示层、会话层、传输层、网罗层、数据链路层、物理层</p>
<p>TCP/IP 四层模型：应用层、传输层、网络层、网络接口层</p>
<ul>
<li><p>应用层，直接服务于用户。唯一运行在用户态的层：HTTP、FTP、Telnet、DNS、SMTP、DHCP</p></li>
<li><p>传输层，应用层的技术支持。应用层数据超过 MSS 会将其切分成 TCP
段，传输层报文中包括了端口号，用于区分报文属于哪个应用：TCP、UDP</p></li>
<li><p>网络层，负责数据在网络中的传输，即寻址和路由。超过 MTU
会切分，IP地址代表网络号和主机号：IP、ARP、ICMP</p></li>
<li><p>传输层，关注链路级别的传输</p></li>
</ul>
<p>应用数据转换到传输块的过程：</p>
<style>.bcjetmjykslb{zoom: 50%;}</style>
<img data-src="/Network/12.jpg" class="bcjetmjykslb" alt="img">
<h2 id="访问网站的过程">访问网站的过程</h2>
<style>.yrqwlehqzvls{zoom: 67%;}</style>
<img data-src="/Network/image-20240730153032143.png" class="yrqwlehqzvls" alt="image-20240730153032143">
<ol type="1">
<li>解析 URL</li>
<li>DNS 查询服务器 IP
地址（本地域名服务器-&gt;根域名服务器-&gt;顶级域名服务器）</li>
<li>操作系统协议栈，包括传输层和网络层协议实现</li>
<li>TCP 建立连接（SYN_SEND、SYN_RECE、ESTABLISHED）</li>
<li>基于端口的 TCP 可靠传输</li>
<li>基于地址的 IP 定位传输</li>
<li>头部加上发送和接收方的 MAC 头部，ARP 查找接收端的 MAC</li>
<li>网卡，帧校验符</li>
<li>交换机，以太网设备，基于 MAC 地址表转发，没有缓存就广播</li>
<li>路由器，和网卡类似，各个端口都具有 MAC 和
IP。通过路由表确定要到目的地 IP 需要走哪个端口，并替换帧的源、目标
MAC</li>
</ol>
<p>在网络包传输的过程中，源 IP 和目标 IP 始终是不会变的（前提：没有使用
NAT 网络的），一直变化的是 MAC 地址，因为需要 MAC
地址在以太网内进行两个设备之间的包传输</p>
<p>校验过程：MAC 地址、IP 地址、序列号、端口</p>
<ol start="11" type="1">
<li>TCP 四次挥手（FIN_WAIT_1、FIN_RECE、FIN_WAIR_2、LAST_ACK）</li>
</ol>
<h2 id="linux-实现数据包收发">Linux 实现数据包收发</h2>
<p><strong>发送</strong></p>
<p>NAPI
机制：不采用中断的方式读取数据，而是首先采用中断唤醒数据接收的服务程序，然后
poll 的方法来轮询数据</p>
<p>DMA 技术：将网络包写入到指定的内存地址，接着网卡向 CPU
发起硬件中断，当 CPU
收到硬件中断请求后，根据中断表，调用已经注册的中断处理函数</p>
<p>硬中断先把收到的数据存在内存中，暂时屏蔽 CPU
中断，后续软中断统一处理先前的数据</p>
<p><strong>接收</strong></p>
<ol type="1">
<li>将用户待发送的数据从用户态拷贝到内核态 sk_buff
内存，并将其加入到发送缓冲区；</li>
<li>克隆一份 sk_buff 的副本，用于网络层传输，传输层保留原始 sk_buff
直到接收到 ACK，以保证可靠传输；</li>
<li>当 IP 层发现数据包大于 MTU 时，会申请额外的
sk_buff，并将原来的数据包拆分成多个小的 sk_buff；</li>
</ol>
<h1 id="应用层-http">应用层 HTTP</h1>
<blockquote>
<p>HTTP
是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」</p>
</blockquote>
<p>常用字段</p>
<p><strong>Content-Length</strong>：HTTP 协议通过设置回车符、换行符作为
HTTP header 的边界，通过 Content-Length 字段作为 HTTP body
的边界，这两个方式都是为了解决“粘包”的问题。</p>
<p>HTTP/1.1 优化</p>
<ul>
<li>长连接，复用 TCP 连接</li>
<li>缓存：强制缓存和协商缓存</li>
<li>压缩</li>
<li>管道传输</li>
<li>灵活、易于扩展</li>
</ul>
<p>HTTP/2 优化</p>
<ul>
<li>头部压缩</li>
<li>并发传输</li>
<li>服务端主动推送</li>
<li>二进制帧格式传输</li>
</ul>
<p>HTTP/3 优化</p>
<ul>
<li>提出了 QUIC 传输层协议代替
tcp。quic是基于udp实现的，在无连接的基础上实现了可靠和安全
<ul>
<li>更快的请求多路复用</li>
<li>避免队头阻塞</li>
<li>更快的建立连接</li>
</ul></li>
</ul>
<p>HTTPS</p>
<ul>
<li>加密传输：在应用层和传输层之间加了 SSL/TLS 安全协议；建立连接时需要
SSL/TLS 的握手过程；使用时需要向 CA 申请数字证书</li>
<li>摘要算法 + 数字签名
<ul>
<li>混合加密：建立连接用非对称加密：公钥和私钥；传输数据用对称加密：一个密钥</li>
<li>摘要算法：用摘要算法（哈希函数）来计算出内容的哈希值，哈希值是唯一的，且无法通过哈希值推导出内容</li>
<li>数字签名：通过「私钥加密，公钥解密」的方式，来确认消息的身份</li>
<li>数字证书：通过第三方 CA 保证服务器的公钥私钥没有被别人替换过</li>
</ul></li>
<li>建立连接：RSA算法
<ul>
<li>ClientHello：客户端生产随机数 Client Random</li>
<li>SeverHello：服务端生产随机数 Server Random、数字证书</li>
<li>客户端回应：获取证书中的服务器公钥、加密通信算法改变通知、公钥加密随机数
pre-master key</li>
<li>三个随机数算出<strong>会话密钥</strong>，双方都通知对方后续请求都使用会话密钥加密通信</li>
</ul></li>
</ul>
<h1 id="传输层-tcp">传输层 TCP</h1>
<blockquote>
<p>负责为应用层提供网络支持，实现应用到应用的数据传输</p>
</blockquote>
<p>TCP ：面向连接的、可靠的、基于字节流的</p>
<p>三次握手</p>
<style>.knofrpygkuwj{zoom:50%;}</style>
<img data-src="/Network/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.drawio.png" class="knofrpygkuwj" alt="TCP 三次握手">
<p>四次挥手【closed_wait；last_ack】</p>
<style>.mvxgkqlnsmmm{zoom: 67%;}</style>
<img data-src="/Network/format,png-20230309230614791.png" class="mvxgkqlnsmmm" alt="客户端主动关闭连接 —— TCP 四次挥手">
<ul>
<li>首先确认客户端不会再发送请求给服务端</li>
<li>然后服务端在发送完数据后，再向客户端发送断开连接</li>
</ul>
<p>为什么这么设计？因为TCP是双向通信的可靠的协议，需要确保客户端和服务端都有收发数据包的能力；为后续传输做准备：序列号</p>
<p>流量控制、超时重传、拥塞控制</p>
<p><a
href="https://blog.csdn.net/dl962454/article/details/115796513">拥塞控制</a>：慢开始（每经过一个传输轮次，拥塞窗口
cwnd 就加倍）、拥塞避免（慢开始门限
ssthresh）、快重传（连续收到三个重复确认就立即重传对方尚未收到的报文段，不等待重传计时器到期）、快恢复</p>
<h1 id="网络层-ip">网络层 IP</h1>
<blockquote>
<p>负责路径传输过程中路径和节点的选择，实现设备到设备的数据传输</p>
</blockquote>
<p>寻址能力：网络号+主机号（子网掩码）</p>
<p>路由：数据包每到达一个节点，通过路由算法决定下一步走哪条路径</p>
<p>分片重组：以太网的 MTU（最大传输单元）是 1500
字节。在分片传输中，一旦某个分片丢失，则会造成<strong>整个 IP
数据报作废</strong>，所以 TCP 引入了 MSS 也就是在 TCP 层进行分片不由 IP
层分片，那么对于 UDP 我们尽量不要发送一个大于 MTU 的数据报文</p>
<p>ICMP：诊断和通知出错</p>
<p>NAT：网络地址转换</p>
<p>DHCP：动态获取 IP 地址</p>
<p>ARP：下一跳的 MAC 地址</p>
<p>DNS：域名解析</p>
<p>IGMP：组播</p>
]]></content>
      <categories>
        <category>八股文</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统</title>
    <url>/OS/</url>
    <content><![CDATA[<p>重点：线程进程、死锁</p>
<h1 id="内存管理">内存管理</h1>
<h2 id="虚拟内存">虚拟内存</h2>
<p>CPU
只会访问虚拟内存地址，在操作总线前，通过一个地址转换硬件将虚拟内存地址转换为物理内存地址</p>
<p>进程隔离：将虚拟内存映射到物理内存，进程之间的地址空间相互隔离，互不干扰</p>
<p>虚拟内存空间：</p>
<style>.nmmpgdzpdfyg{zoom:50%;}</style>
<img data-src="/OS/image-20240518105657086.png" class="nmmpgdzpdfyg" alt="image-20240518105657086">
<p>用户态虚拟内存空间是相互隔离相互独立的；内核虚拟内存空间是各个进程共享的；</p>
<h1 id="进程管理">进程管理</h1>
<p>数据结构 PCB</p>
<p><strong>进程和线程的区别</strong></p>
<p>线程是调度的基本单位，而进程则是资源拥有的基本单位。</p>
<ul>
<li>进程是资源（包括内存、打开的文件等）分配的最小单位，线程是 CPU
调度的最小单位；</li>
<li>进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈，线程之间共享地址空间和文件等资源；</li>
<li>线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系；</li>
<li>线程能减少并发执行的时间和空间开销；</li>
</ul>
<h2 id="进程间通信方式">进程间通信方式</h2>
<p>匿名管道 / 命名管道</p>
<p>消息队列</p>
<p>共享内存+信号量</p>
<p>Socket</p>
<h2 id="乐观锁悲观锁">乐观锁悲观锁</h2>
<p>CAS 是乐观锁没错，但是 CAS 和自旋锁不同之处，自旋锁基于 CAS 加了while
或者睡眠 CPU
的操作而产生自旋的效果，加锁失败会忙等待直到拿到锁，自旋锁是要需要事先拿到锁才能修改数据的，所以算悲观锁。</p>
<h2 id="线程崩溃">线程崩溃</h2>
<p>1、如果线程是<strong>非法访问内存</strong>引起的崩溃，其对应进程一定会崩溃。</p>
<p>2、进程崩溃的本质是：操作系统对进程发出了信号，例如非法访问内存的信号是
SIGSEGV（序号 11）</p>
<p>3、想要防止进程奔溃，需要自定义信号处理函数去拦截 SIGSEGV 信号。参考
JVM 中线程崩溃但 JVM 进程不会崩溃</p>
<h1 id="网络系统">网络系统</h1>
<h2 id="io-多路复用">I/O 多路复用</h2>
<p><strong>单个进程或线程同时监视多个文件描述符</strong>，如网络连接或文件句柄。当这些描述符中的任何一个就绪时，比如有数据可读或可写，多路复用机制就能够通知应用程序进行相应的读写操作。常见的IO多路复用技术包括
select、poll 和 epoll 等。</p>
<p>select</p>
<p>将已连接的 Socket 都放到一个文件描述符集合，然后调用 select
函数将文件描述符集合拷贝到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过遍历文件描述符集合的方式，当检查到有事件产生后，将此
Socket 标记为可读或可写，
接着再把整个文件描述符集合拷贝回用户态里，然后用户态还需要再通过遍历的方法找到可读或可写的
Socket，然后再对其处理。<strong>2
次「遍历」文件描述符集合</strong>，<strong>2
次「拷贝」文件描述符集合</strong>。</p>
<p>poll</p>
<p>它使用一个 pollfd
结构来表示被监视的文件描述符及其事件，与select相比，poll没有文件描述符数量的限制，因为它基于链表来存储。<strong>都是使用「线性结构」存储进程关注的
Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的
Socket，时间复杂度为
O(n)，而且也需要在用户态与内核态之间拷贝文件描述符集合</strong></p>
<p>epoll</p>
<img data-src="/OS/640.webp" class="" title="图片">
<ul>
<li>epoll
只在初始时完成一次文件描述符的注册，避免了每次调用时的拷贝开销；</li>
<li>当有一个或多个文件描述符就绪时，会调用回调函数将 Socket
集合一次性传递到就绪事件列表中，并通知用户空间，这使得 epoll
在处理大量文件描述符时仍然能保持高效；</li>
<li>当用户调用 epoll_wait()
返回有事件发生的文件描述符的个数，明确指出了哪些文件描述符是就绪的，无需像
select 和 poll 遍历所有 socket 集合；</li>
</ul>
<p><strong>触发时机</strong></p>
<ul>
<li>边缘触发【epoll】：只有当文件描述符的状态从不就绪变为就绪时，epoll
才会发出通知；一旦通知过后，除非状态再次发生变化，否则不会再次通知；</li>
<li>水平触发【select/poll】：只要文件描述符处于就绪状态，epoll就会持续发出通知，直到数据处理完毕；</li>
</ul>
]]></content>
      <categories>
        <category>八股文</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformers 使用入门</title>
    <url>/Transformers/</url>
    <content><![CDATA[<p>NLP
任务：情感分析、文本生成、命名体识别、阅读理解、掩码填充、文本摘要、机器翻译、特征提取、对话机器人</p>
<p><strong>万物之始 Transformer</strong> Attention is all you need</p>
<style>.fbxbnygyvgjf{zoom:70%;}</style>
<img data-src="/Transformers/image-20240714162218754.png" class="fbxbnygyvgjf" alt="image-20240714162218754">
<ul>
<li>直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征；</li>
<li>Self Attention 对于增加计算的并行性有直接帮助作用；</li>
</ul>
<h1 id="transformers及相关库">Transformers及相关库</h1>
<p>Transformers：核心库，模型加载、模型训练、流水线并行等</p>
<p>Tokenizer：分词器，对数据进行预处理，文本到token序列的互相转换。具体工作包括：分词、索引、填充截断、attention_mask、token_type</p>
<p>Datasets：数据集库，提供了数据集的加载、处理等方法</p>
<p>Evaluate：评估函数，提供各种评价指标的计算函数</p>
<p>Trainer：将模型训练流程打包，包括：创建优化器、创建DataLoader、tensor转移到CUDA上</p>
<p>PEFT：高效微调模型的库，提供了几种高效微调的方法，小参数量动大模型</p>
<p>Accelerate：分布式训练，提供了分布式训练解决方案，包括大模型的加载与推理解决方案</p>
<p>Optimum：优化加速库，支持多种后端，如Onnxruntime、OpenVino等</p>
<p>Gradio：可视化部署库，几行代码快速实现基于Web交互的算法演示系统</p>
<h2 id="分布式训练">分布式训练</h2>
<p>Accelerate 包，融合了 Deepspeed、DDP、FSDP；</p>
<h1 id="nlp">NLP</h1>
<p>NLP
数据集大多是未标注的文档，因此采用自监督学习的方式训练模型，如预测下一个词
LM、完形填空 MLM。</p>
<p>早些年，流行的是词嵌入 Word
Embedding，目标是采用向量的形式表示词元，采用一组向量表示一段话，可以用于计算词/句子相似度。</p>
<p>现阶段，Transformer
的出现促使三种类型的模型的出现：Encoder-only-only-Decoder。Encoder-only
适用于机器翻译、句子分类、命名体识别和问答；Decoder-only
适用于文本预测；Encoder-Decoder 适用于文本压缩。</p>
]]></content>
  </entry>
  <entry>
    <title>[paper]PAFT</title>
    <url>/paper-PAFT/</url>
    <content><![CDATA[<p><a
href="https://gorilla.cs.berkeley.edu/blogs/9_raft.html">Paper</a></p>
<h1 id="创新点">创新点</h1>
<p>提出了新颖的特定 RAG 场景下的大模型微调方法（有明确文档域的
RAG）；</p>
<p>基于 COT，进一步完善了模型的推理能力；</p>
<h1 id="背景">背景</h1>
<p>当前的开放域对话模型做的是”闭卷考试“；</p>
<p>RAG 做的是”开卷考试“，其结果依赖于检索器的效果；</p>
<p>PAFT
应用于”特定领域的开卷考试“，模型先在特定域文档内学习如何做”开卷考试“，可以提高模型
RAG 的能力，还能减少幻觉的产生；</p>
<h1 id="实验">实验</h1>
<h2 id="方法">方法</h2>
<p>前置条件：<span class="math inline">\(\mathrm{Q}\)</span>问题； <span
class="math inline">\(\mathrm{A}^{*}\)</span> 答案； <span
class="math inline">\(\mathrm{D}^{*}\)</span> 答案来源文档； <span
class="math inline">\(D_n\)</span> 无关文档。</p>
<p>正例样本：<span class="math inline">\(Q+ \mathrm{D^*}
+D_1+D_2+\dots+D_k=&gt;\mathrm{A}^{*}\)</span></p>
<p>负例样本： <span
class="math inline">\(\mathrm{Q}+\mathbf{D}_{1}+\mathbf{D}_{2}+\dots+\mathbf{D}_{\mathbf{k}}=&gt;\mathbf{A}^{\star}\)</span></p>
<p>论文中基于数据集做全量 SFT 训练</p>
<h2 id="数据集">数据集</h2>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">Question: The Oberoi family is part of a hotel company that has a head office in what city?</span><br><span class="line"></span><br><span class="line">context: [The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group]...[It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two towers that comprises a hotel and the Airlangga Apartment respectively]...[The Oberoi Group is a hotel company with its head office in Delhi.]</span><br><span class="line"></span><br><span class="line">Instruction: Given the question, context and answer above, provide a logical reasoning for that answer. Please use the format of: <span class="params">##</span>Reason: &#123;reason&#125; <span class="params">##</span>Answer: &#123;answer&#125;.</span><br><span class="line"></span><br><span class="line">CoT Answer: <span class="params">##</span>Reason: The document <span class="params">##</span>begin<span class="built_in">_</span>quote<span class="params">##</span> The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group. <span class="params">##</span>end<span class="built_in">_</span>quote<span class="params">##</span> establishes that the Oberoi family is involved in the Oberoi group, and the document <span class="params">##</span>begin<span class="built_in">_</span>quote<span class="params">##</span> The Oberoi Group is a hotel company with its head office in Delhi. <span class="params">##</span>end<span class="built_in">_</span>quote<span class="params">##</span> establishes the head office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel company whose head office is in Delhi. <span class="params">##</span>Answer: Delhi</span><br></pre></td></tr></table></figure>
<p>从数据集 <code>Answer</code>
中可以看出，提问者引导模型以思维链的方式去回答问题，先说原因
<code>Reason</code> 再说答案 <code>Answer</code></p>
<p>文中采用 ##begin_quote## 和 ##end_quote##
表示直接从上下文复制粘贴的引用，研究人员发现这是防止模型产生幻觉并坚持提供的上下文的有效方法。</p>
<h2 id="实验结果">实验结果</h2>
<p>DFS（Domain-Specific Finetuning）：特定领域内微调</p>
<p>PAFT：特定领域微调+特定领域 RAG</p>
<img data-src="/paper-PAFT/image-20240815185100137.png" class="" title="image-20240815185100137">
<h1 id="总结">总结</h1>
<p>将模型在特定领域的文档上做思维链微调</p>
<ul>
<li>通过特定领域微调，模型可以学会如何做“开卷考试“；</li>
<li>当领域内数据发生一些变化时，模型依然能找到答案，即使没有正确答案也不会出现幻觉；</li>
<li>可以认为，模型需要专业的微调来训练其在特定领域的
RAG（开卷考试）能力；</li>
</ul>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>大模型微调</tag>
      </tags>
  </entry>
  <entry>
    <title>实习日志</title>
    <url>/%E5%AE%9E%E4%B9%A0%E6%97%A5%E5%BF%97/</url>
    <content><![CDATA[<p>海康-算法实习</p>
<p><strong>第一周</strong></p>
<ul>
<li>看论文看综述，还是分不清 Prompt Tuning 及其变种有什么区别；</li>
<li>看 B 站教程，从 peft 源码理解；</li>
<li>写大模型微调博客，狠狠沉淀自己；</li>
</ul>
<p><strong>第二周</strong></p>
<ul>
<li>Lora 微调 Qwen_1.8B，训练模型的自我认知能力，效果还可以；</li>
<li>写微调调研报告，总结了近二十种微调方法，还有微调数据集、测评指标和微调开发包；</li>
</ul>
<p><strong>第三周</strong></p>
<ul>
<li>阅读 llamafactory 源码，基于 llamafactory 实现 AdaLora
微调，直接调用 llamafactory 包微调，为后续封装 API 做准备；</li>
<li>手搓 Lora、AdaLora 微调方法（当然也是调包），微调 Qwen_1.8B；AdaLora
的实现有些问题，占用接近46G 显存；</li>
</ul>
<p><strong>第四周</strong></p>
<ul>
<li>应用两种数据预处理方法：alpaca、sharegpt</li>
<li>在部分 NaturalConv 数据集上 lora、adalora 监督微调
Qwen-1.8B，并测评微调结果，跑通全流程；</li>
</ul>
<p><strong>第五周</strong></p>
<ul>
<li>应用分布式微调方法：deepspeed、accelerate</li>
<li>在全量 NaturalConv 上 lora 监督微调
Qwen-14B、Qwen2-7B-Instruction、Llama3-8B，并测评；</li>
</ul>
<p><strong>第六周</strong></p>
<ul>
<li>新增两类数据集
COIG-CQIA、alpaca-gpt4-data-zh，在其之上做微调和测评；</li>
<li>调研 Llama-Factory、axolotl 开源工具的实现方法；</li>
</ul>
<p><strong>第七周</strong></p>
<ul>
<li>封装测评工具，实现功能：1. sqlite 存储微调状态；2. mutiprocess
新建子进程后台执行模型微调和测试；3. API 获取微调状态；4. 基础功能
swagger 文档和 logger 持久化；</li>
<li>进一步测评 reLora、dora、loraplus 微调方法；</li>
</ul>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>intern</tag>
      </tags>
  </entry>
  <entry>
    <title>实习笔试题汇总</title>
    <url>/%E5%AE%9E%E4%B9%A0%E7%AC%94%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<ol start="2024" type="1">
<li></li>
</ol>
<h1 id="美团笔试">2024.4.27 美团笔试</h1>
<p>和携程差不多，签到题速通，后续疯狂折磨人，过不了一点。感觉题目难度差的好大，要么我秒杀它，要么它秒杀我</p>
<p><strong>第三题</strong></p>
<blockquote>
<p>回溯写了半天，结果只能过20%。看别人是用前缀和+dp，那就是和腾讯第四题类似了，当时学会了现在又忘了，真难崩。</p>
</blockquote>
<p>小美拿到了一个数组，她每次操作可以将两个相邻元素合并为一个元素，合并后的元素为原来两个元素之和。小美希望最终数组的最小值不小于
k。她想知道有多少种不同的合并结果?</p>
<p>输入描述：第一行输入两个正整数n,k，代表数组大小和数组的最大值。第二行输入几个正整数q:，代表小美拿到的数组。1
≤ n,k,ai &lt; 200</p>
<p>输出描述：输出一个整数，代表小美可以得到多少种不同的结果。由于结果可能很大，输出对10^9+7取模的结果。</p>
<p>eg：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">4 4</span><br><span class="line">2 3 4 5</span><br></pre></td></tr></table></figure>
<p>输出 4</p>
<p><strong>回溯</strong>解法，只过20%（加了dp保存过程值也没用）</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution_3</span> &#123;</span><br><span class="line">    <span class="type">int</span>[] dp;</span><br><span class="line">    <span class="type">int</span>[] a;</span><br><span class="line">    <span class="type">int</span> k;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">solve</span><span class="params">(<span class="type">int</span>[] a, <span class="type">int</span> k)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.dp = <span class="keyword">new</span> <span class="title class_">int</span>[a.length];</span><br><span class="line">        <span class="built_in">this</span>.a = a;</span><br><span class="line">        <span class="built_in">this</span>.k = k;</span><br><span class="line">        Arrays.fill(dp, -<span class="number">1</span>);</span><br><span class="line">        System.out.println(backtrack(<span class="number">0</span>, <span class="number">0</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> <span class="title function_">backtrack</span><span class="params">(<span class="type">int</span> sum, <span class="type">int</span> index)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (index==a.length) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (sum==<span class="number">0</span> &amp;&amp; dp[index]!=-<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> dp[index];</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">        <span class="type">boolean</span> <span class="variable">isBegin</span> <span class="operator">=</span> sum==<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i=index; i&lt;a.length; i++) &#123;</span><br><span class="line">            sum += a[i];</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (sum&gt;=k) &#123;</span><br><span class="line">                cnt += backtrack(<span class="number">0</span>, i+<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (isBegin) dp[index] = cnt;</span><br><span class="line">        <span class="keyword">return</span> cnt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Scanner</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> in.nextInt(), k = in.nextInt();</span><br><span class="line">        <span class="type">int</span>[] a = <span class="keyword">new</span> <span class="title class_">int</span>[n];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) &#123;</span><br><span class="line">            a[i] = in.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Solution_3</span>().solve(a, k);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>自己试试写<strong>前缀和+dp</strong>（思路重要真的很重要，二十分钟就写出来了）</p>
<blockquote>
<p>dp[i] 表示<del>数字 0-i 的方案数</del>[0, i)
的字串可以拆分的种类，状态转移方程为<span class="math inline">\(dp[i] =
\sum dp[j]\)</span>（j-i 大于等于 k 的数量总和，用前缀和求出）</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Main</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Scanner</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> in.nextInt(), k = in.nextInt();</span><br><span class="line">        <span class="type">int</span>[] sum = <span class="keyword">new</span> <span class="title class_">int</span>[n+<span class="number">1</span>];</span><br><span class="line">        sum[<span class="number">1</span>] = in.nextInt();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">2</span>; i&lt;=n; i++) &#123;</span><br><span class="line">            sum[i] += sum[i-<span class="number">1</span>] + in.nextInt();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="type">int</span>[] dp = <span class="keyword">new</span> <span class="title class_">int</span>[n+<span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; i; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (sum[i] - sum[j] &gt;= k) &#123;</span><br><span class="line">                    dp[i] += dp[j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(dp[n]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Q1</strong>：前缀和算法初始化时为什么都要设置
<code>[0, 0] = 1</code> 呢？ 如: <code>dp[0] = 1</code> or
<code>map.put(0,1)</code></p>
<p><strong>A1</strong>：想不明白呢，需要对状态转移数组 dp[]
的理解非常正确才行</p>
<p><strong>第四题</strong></p>
<p>小美拿到了一棵树，其中有一些节点被染成红色。</p>
<blockquote>
<p>大佬的思路：预打表小质数，然后dfs遍历连通块，统计块内每个红点的质因子，得到结点编号的乘积的质因子分解形式，根据质因子数量计算因子个数。对于我来说，需要背一下并查集的模板。</p>
</blockquote>
<p>小美定义一个红色连通块的权值为:所有节点编号乘积的因子数量。小美想知道，所有红色连通块的权值之和是多少?由于答案过大，请对10^9+
7取模。</p>
<p><strong>输入描述</strong>：</p>
<p>第一行输入一个正整数n，代表节点数量。
第二行输入一个长度为几的、仅由'R"和"W组成的字符串，第i个字符为'R'代表;号节点被染成红色，"W代表未被染色。保证至少有一个节点被染成红色。</p>
<p>接下来的n -
1行，每行输入2个正整数4,U，代表u号节点和u号节点有一条边连接。1 ≤n ≤
10^5, 1 ≤u,v≤n</p>
<p><strong>输出描述</strong>：</p>
<p>一个整数，代表所有红色连通块的权值之和。</p>
<h1 id="华为测评">2024.4.25 华为测评</h1>
<p>华子免笔试，只有测评了。看网上好多人测评挂了我都惊呆了，还好没和其他的厂一样直接做测评而是上我搜了下，其他的厂可以不去但华子我必然拿下（小心谨慎哈哈哈）</p>
<p>华子要什么样性格的人？（前后回答要一致）</p>
<ul>
<li>不需要凸显自己的能力、表达自己的个性，更多的是团队合作</li>
<li>遇到挫折要坚持，遇到问题会咨询</li>
<li>热爱工作，吃苦耐劳，积极向上</li>
</ul>
<p>（准备了又好像没准备，就这样吧，希望能过！阿弥陀佛财神爷保佑！）</p>
<p><strong>复盘</strong>：有些过于没有压力了，前面每一题都认真思考结果
40min 过完才做了一半，人家都是 40min
就做完了，实在是有些拖拉（但这个要认真选真的很难啊，得回忆自己平常的所思所想所做才然后选个答案，后续时间紧张直接主观臆断速度还是很快的）。<strong>只要时间不紧张就慢慢做，一旦时间紧张了才开始全神贯注做，这可真的是一个坏毛病，得改。</strong></p>
<p>后续学习工作中都应该有这个意识，主动养成专注的习惯，要做就认真仔细专注的做事情！</p>
<h1 id="携程笔试">2024.4.16 携程笔试</h1>
<p>前两道签到题，后面一个半小时纯折磨</p>
<p>第三题</p>
<p>游游拿到了一个数组，她每次操作可以将相邻的两个素数元素进山314行合并，合并后的新数为原来的两个数之和，并删除原来两个数。游游希望最终数组的元素数量尽可能少，你能帮帮她吗?</p>
<blockquote>
<p>第一行输入一个正整数n，代表数组的大小。
第二行输入几个正整数a_i，代表数组的元素 1 ≤ n &lt; 10^5 1 ≤ a_i &lt;
10^6</p>
</blockquote>
<p>返回合并结束后的元素的数量</p>
<p><strong>题目分析</strong>：素数是只能被1和自身整除的数。针对这道题可以用到，它的一大特性：除了
2 之外两个素数之和一定是偶数（即，一定不是素数）</p>
<p>因此只要先把所有 2
和素数合并，后续只需要判断还有多少相邻的素数即可。</p>
<p>做题的时候没有考虑到 <code>7 2 2 3</code> 这种情况，即素数和 2
合并完的结果如果是素数，还需要和判断他的左边还有没有
2，有的话还要合并，即代码中的 <code>i--</code> 部分。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PrimeMerge</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Scanner</span> <span class="variable">scanner</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> scanner.nextInt();</span><br><span class="line">        List&lt;Integer&gt; numbers = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取数组元素</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            numbers.add(scanner.nextInt());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并素数，优先处理2</span></span><br><span class="line">        mergePrimesWithPriorityToTwo(numbers);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出最终数组</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> num : numbers) &#123;</span><br><span class="line">            System.out.print(num + <span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查一个数是否是素数的辅助函数</span></span><br><span class="line">    <span class="keyword">static</span> <span class="type">boolean</span> <span class="title function_">isPrime</span><span class="params">(<span class="type">int</span> num)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (num &lt;= <span class="number">1</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">2</span>; i * i &lt;= num; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (num % i == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">mergePrimesWithPriorityToTwo</span><span class="params">(List&lt;Integer&gt; numbers)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并2与其相邻的素数，得到一个新的素数</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; numbers.size(); ) &#123;</span><br><span class="line">            <span class="keyword">if</span> (numbers.get(i) == <span class="number">2</span>) &#123;</span><br><span class="line">                <span class="comment">// 检查左边是否有素数可以合并</span></span><br><span class="line">                <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; isPrime(numbers.get(i - <span class="number">1</span>)) &amp;&amp; isPrime(numbers.get(i - <span class="number">1</span>) + <span class="number">2</span>)) &#123;</span><br><span class="line">                    numbers.set(i - <span class="number">1</span>, numbers.get(i - <span class="number">1</span>) + <span class="number">2</span>);</span><br><span class="line">                    numbers.remove(i);</span><br><span class="line">                    i--;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i &lt; numbers.size() - <span class="number">1</span> &amp;&amp; isPrime(numbers.get(i + <span class="number">1</span>)) &amp;&amp; isPrime(numbers.get(i + <span class="number">1</span>) + <span class="number">2</span>)) &#123;</span><br><span class="line">                    <span class="comment">// 检查右边是否有素数可以合并</span></span><br><span class="line">                    numbers.set(i, numbers.get(i) + numbers.get(i + <span class="number">1</span>));</span><br><span class="line">                    numbers.remove(i + <span class="number">1</span>);</span><br><span class="line">                    i--;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// 没有可以合并的素数，继续下一个数</span></span><br><span class="line">                    i++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并剩余的素数对</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; numbers.size() - <span class="number">1</span>; ) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isPrime(numbers.get(i)) &amp;&amp; isPrime(numbers.get(i + <span class="number">1</span>))) &#123;</span><br><span class="line">                numbers.set(i, numbers.get(i) + numbers.get(i + <span class="number">1</span>));</span><br><span class="line">                numbers.remove(i + <span class="number">1</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第四题</p>
<p>定义一棵树的直径为：任意两个节点的距离的最大值。现在定义f(i)：对i号节点上再连接一个新的子叶节点后，树的直径长度。现在要求f(1)到f(n)的值。</p>
<p>输入描述：第一行输入一个正整数n，代表树的节点数量。</p>
<p>接下来的n-1行，每行输入两个正整数u和v，代表u号节点和v号节点之间有一条长度为1的边连接。</p>
<p>变量的范围：1≤n≤10的五次方，1≤u，v≤n。</p>
<p><strong>题目分析：</strong> 树的直径引出树形 dp 算法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TreeNode</span> &#123;</span><br><span class="line">    <span class="type">int</span> val;</span><br><span class="line">    List&lt;TreeNode&gt; children;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">TreeNode</span><span class="params">(<span class="type">int</span> val)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.val = val;</span><br><span class="line">        children = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TreeDiameter</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span>[] f; <span class="comment">// 存储每个节点的最大直径</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Scanner</span> <span class="variable">scanner</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> scanner.nextInt(); <span class="comment">// 树的节点数量</span></span><br><span class="line"></span><br><span class="line">        f = <span class="keyword">new</span> <span class="title class_">int</span>[n + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        TreeNode[] nodes = <span class="keyword">new</span> <span class="title class_">TreeNode</span>[n + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            nodes[i] = <span class="keyword">new</span> <span class="title class_">TreeNode</span>(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 连接节点之间的关系</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; n - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">u</span> <span class="operator">=</span> scanner.nextInt();</span><br><span class="line">            <span class="type">int</span> <span class="variable">v</span> <span class="operator">=</span> scanner.nextInt();</span><br><span class="line">            nodes[u].children.add(nodes[v]);</span><br><span class="line">            nodes[v].children.add(nodes[u]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历每个节点，添加一个新的子叶节点，计算f(i)</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            <span class="comment">// 添加新的子叶节点</span></span><br><span class="line">            nodes[i].children.add(<span class="keyword">new</span> <span class="title class_">TreeNode</span>(-<span class="number">1</span>));</span><br><span class="line">            <span class="comment">// 计算当前最大直径</span></span><br><span class="line">            dfs(nodes[i], <span class="literal">null</span>, i);</span><br><span class="line">            <span class="comment">// 删除添加的子叶节点</span></span><br><span class="line">            nodes[i].children.remove(nodes[i].children.size()-<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">            System.out.print(f[i] + <span class="string">&quot; &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 递归计算以当前节点为根的子树的直径</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">dfs</span><span class="params">(TreeNode node, TreeNode parent, <span class="type">int</span> i)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">maxDepth1</span> <span class="operator">=</span> <span class="number">0</span>; <span class="comment">// 最大深度</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">maxDepth2</span> <span class="operator">=</span> <span class="number">0</span>; <span class="comment">// 次大深度</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历当前节点的子节点</span></span><br><span class="line">        <span class="keyword">for</span> (TreeNode child : node.children) &#123;</span><br><span class="line">            <span class="keyword">if</span> (child == parent) <span class="keyword">continue</span>; <span class="comment">// 不访问父节点，防止死循环</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">childDepth</span> <span class="operator">=</span> dfs(child, node, i) + <span class="number">1</span>; <span class="comment">// 递归计算子节点的深度</span></span><br><span class="line">            <span class="keyword">if</span> (childDepth &gt; maxDepth1) &#123;</span><br><span class="line">                maxDepth2 = maxDepth1;</span><br><span class="line">                maxDepth1 = childDepth;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (childDepth &gt; maxDepth2) &#123;</span><br><span class="line">                maxDepth2 = childDepth;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更新当前状态的最大直径</span></span><br><span class="line">        f[i] = Math.max(f[i], maxDepth1 + maxDepth2);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> maxDepth1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="恒生笔试">2024.4.9 恒生笔试</h1>
<ol type="1">
<li>下列代码的返回值</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">count</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">5</span>/<span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>*<span class="number">3</span>;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a
href="https://docs.oracle.com/javase/specs/jls/se7/html/jls-14.html#jls-14.20.2">根据Java语言规范文档规定</a>。在
try-catch-finally 语句块中，finally 语句块中的
return/抛出异常（立即结束语句）的优先级最高，程序会优先返回 finally
语句块中的立即结束语句的结果，此时 try-catch 语句块中的
return/抛出异常（立即结束语句）的结果就会被丢弃掉。</p>
<ol start="2" type="1">
<li>SQL 语句</li>
</ol>
<p>联结语法、INNER JOIN 语法</p>
<ol start="3" type="1">
<li>初始资金 M，N 天价格信息，K 次交易后，能赚多少钱？（动态规划）</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 根据输入计算最大收益</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> M double浮点型 初始资金</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> N int整型 历史价格天数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> historyPrices double浮点型一维数组 N天历史价格</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> K int整型 最大允许交易次数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> double浮点型</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">double</span> <span class="title function_">get_max_profit</span> <span class="params">(<span class="type">double</span> M, <span class="type">int</span> N, <span class="type">double</span>[] historyPrices, <span class="type">int</span> K)</span> &#123;</span><br><span class="line">    <span class="type">double</span>[][] dp = <span class="keyword">new</span> <span class="title class_">double</span>[K+<span class="number">1</span>][N];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">1</span>; j&lt;N; j++) &#123;</span><br><span class="line">        dp[<span class="number">0</span>][j] = M;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;=K; i++) &#123;</span><br><span class="line">        dp[i][<span class="number">0</span>] = M;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">1</span>; i&lt;=K; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">1</span>; j&lt;N; j++) &#123;</span><br><span class="line">            <span class="type">double</span> <span class="variable">max</span> <span class="operator">=</span> dp[i][j-<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> t=<span class="number">0</span>; t&lt;j; t++) &#123;</span><br><span class="line">                <span class="type">double</span> <span class="variable">rest</span> <span class="operator">=</span> dp[i-<span class="number">1</span>][t]%historyPrices[t], buy = (dp[i-<span class="number">1</span>][t]-rest)/historyPrices[t];</span><br><span class="line">                max = Math.max(max, buy*historyPrices[j]+rest);</span><br><span class="line">            &#125;</span><br><span class="line">            dp[i][j] = max;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[K][N-<span class="number">1</span>]-dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="腾讯笔试">2024.3.31 腾讯笔试</h1>
<p>没做好记录，只记了没做出来的第四题，其他四道题 AC
了三道，还有一道通过 97% 我怀疑是样例有问题[手动狗头]</p>
<ol start="4" type="1">
<li>给你 n 个数，这 n 个数要被拆成 k
份，对每份中的数字求异或，求各份异或总和的最大值（和恒生第二题的思路基本一样）</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    <span class="type">Scanner</span> <span class="variable">in</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scanner</span>(System.in);</span><br><span class="line">    <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> in.nextInt();</span><br><span class="line">    <span class="type">int</span> <span class="variable">k</span> <span class="operator">=</span> in.nextInt();</span><br><span class="line">    <span class="type">long</span>[][] dp = <span class="keyword">new</span> <span class="title class_">long</span>[k + <span class="number">1</span>][n + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        dp[<span class="number">1</span>][i + <span class="number">1</span>] = in.nextInt() ^ dp[<span class="number">1</span>][i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">long</span>[] prefix = dp[<span class="number">1</span>];</span><br><span class="line">    <span class="comment">// 拆分为 i 份</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">2</span>; i &lt;= k; i++) &#123;</span><br><span class="line">        <span class="comment">// 从头到尾遍历：将第 1~j 个元素拆成 i 份</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> i; j &lt;= n; j++) &#123;</span><br><span class="line">            <span class="type">long</span> <span class="variable">max</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="comment">// 将第 [1,s] 个元素拆成 i-1 份，(s,j] 是最后一份</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">s</span> <span class="operator">=</span> j - <span class="number">1</span>; s &gt;= i - <span class="number">1</span>; s--) &#123;</span><br><span class="line">                <span class="comment">// 异或的优先级小于加减法</span></span><br><span class="line">                max = Math.max(max, dp[i - <span class="number">1</span>][s] + (prefix[j] ^ prefix[s]));</span><br><span class="line">            &#125;</span><br><span class="line">            dp[i][j] = max;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    System.out.println(dp[k][n]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>笔试记录</tag>
      </tags>
  </entry>
  <entry>
    <title>实习面试题汇总</title>
    <url>/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<h1 id="海康技术主管面">2024.6.24 海康技术主管面</h1>
<h1 id="谐云技主管面">2024.6.13 谐云技主管面</h1>
<ol type="1">
<li>讲一讲和算法相关的经历</li>
</ol>
<ul>
<li>专利：基于异构图网络的情绪支持对话</li>
<li>项目：LLM应用开发</li>
</ul>
<ol start="2" type="1">
<li>这个项目都在使用别人开发好的技术，那么你在其中有什么贡献？</li>
</ol>
<ul>
<li>目前很多开源项目只停留在科研方面，如何落地到具体项目中，就比如 LLM
出现错误宕机的时候，重启容器化运行的 LLM
实例并采用一致性轮询方法，访问其他正常运行的容器</li>
</ul>
<ol start="3" type="1">
<li>做过最大的项目</li>
</ol>
<ul>
<li>实习的时候做的数据中台项目</li>
</ul>
<ol start="4" type="1">
<li>对未来的计划</li>
</ol>
<ul>
<li>作为一个刚进入工作的学生，要不断学习拓宽自己的技术面</li>
</ul>
<h1 id="华为主管面">2024.5.22 华为主管面</h1>
<h1 id="谐云技术面">2024.5.21 谐云技术面</h1>
<p>并发</p>
<h1 id="华为技术面">2024.5.10 华为技术面</h1>
<ul>
<li>让我调了下摄像头，好展示我的颜，然后自我介绍</li>
<li>手撕算法（由于太紧张写代码一直在絮絮叨叨）</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">业务发送数据到对端，由于网络原因会出现概率丢包；现发送1W条数据到对端，指定输入N为丢包数量，随后列出丢包数据的具体位置；你有机会进行补包，M为可补包的数量，请返回补包后最大可连续发送数据的数量</span><br><span class="line"></span><br><span class="line">无需实现交互式输入，只需实现如下函数</span><br><span class="line"><span class="type">int</span> <span class="title function_">maxContinuousPackages</span><span class="params">(<span class="type">int</span> n, <span class="type">int</span>[] lostPackages, <span class="type">int</span> m)</span></span><br></pre></td></tr></table></figure>
<p>测试样例</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">示例1：</span><br><span class="line">3</span><br><span class="line">1 5 10</span><br><span class="line">1</span><br><span class="line">-----</span><br><span class="line">9995</span><br><span class="line">本示例中，丢包数量N为3，丢包位置分布为第1、5、10三个位置，补包数量M为1；在第10个位置补包后可连续发包数量最大，为10000 - 5 = 9995</span><br><span class="line"></span><br><span class="line">示例2：</span><br><span class="line">4</span><br><span class="line">4 6 20 9990</span><br><span class="line">2</span><br><span class="line">-----</span><br><span class="line">9994</span><br><span class="line"></span><br><span class="line">示例3：</span><br><span class="line">4</span><br><span class="line">1500 4100 5000 8000</span><br><span class="line">2</span><br><span class="line">---</span><br><span class="line">6499</span><br><span class="line"></span><br><span class="line">示例4：</span><br><span class="line">4</span><br><span class="line">4 699 700 9990</span><br><span class="line">2</span><br><span class="line">---</span><br><span class="line">9985</span><br><span class="line"></span><br><span class="line">示例5：</span><br><span class="line">6</span><br><span class="line">100 2700 5100 7498 7499 7500</span><br><span class="line">2</span><br><span class="line">---</span><br><span class="line">7397</span><br></pre></td></tr></table></figure>
<p>AC 代码，中途写错了还考虑了动态规划，导致三十分钟用满了，说是最多 30
min 再满一点我就 GG 了</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">maxContinuousPackages</span><span class="params">(<span class="type">int</span> n, <span class="type">int</span>[] lostPackages, <span class="type">int</span> m)</span> &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> lostPackages.length, ans = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span>[] data = <span class="keyword">new</span> <span class="title class_">int</span>[len+<span class="number">1</span>];</span><br><span class="line">    data[<span class="number">0</span>] = lostPackages[<span class="number">0</span>]-<span class="number">1</span>;</span><br><span class="line">    data[data.length-<span class="number">1</span>] = <span class="number">10000</span> - lostPackages[len-<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">1</span>; i&lt;data.length-<span class="number">1</span>; i++) &#123;</span><br><span class="line">        data[i] = lostPackages[i] - lostPackages[i-<span class="number">1</span>] - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    m++;</span><br><span class="line">    <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;m; i++) &#123;</span><br><span class="line">        sum += data[i];</span><br><span class="line">    &#125;</span><br><span class="line">    ans = sum;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=m; i&lt;data.length; i++) &#123;</span><br><span class="line">        sum -= data[i-m];</span><br><span class="line">        sum += data[i];</span><br><span class="line">        ans = Math.max(ans, sum);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ans+m-<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>算法思路讲一遍，中间做错了删掉的代码也要讲为什么做错了，然后把代码截了个图</li>
<li>说一下自己的技术栈</li>
</ul>
<p>Java基础、JUC、JVM、SpringBoot、MySQL</p>
<ul>
<li>挑一个自己熟悉的讲一讲，选了 MySQL 索引</li>
</ul>
<p>B+ 树结构，聚簇索引、二级索引、和 B 树的对比</p>
<ul>
<li>如果让你建一张表，你会如何设计索引</li>
</ul>
<p>联合索引、覆盖索引、前缀索引</p>
<ul>
<li>IOC 容器的理解</li>
</ul>
<p>代理所有单例的创建，防止频 GC</p>
<ul>
<li>那为什么不直接用单例模式而用 IOC 容器的 Bean</li>
</ul>
<p>还能实现 Bean 的生命周期管理，可以实现 AOP</p>
<ul>
<li>用 Python 能否实现 AOP</li>
</ul>
<p>我说不能，AOP 实现需要操作字节码，他说行，我道歉</p>
<ul>
<li>设计模式讲一下</li>
</ul>
<p>讲了 SpringMVC 中 HanderAdapter 用到的适配器模式</p>
<ul>
<li>Rest 和 Rpc 有什么区别</li>
</ul>
<p>首先说了实习的时候 Rpc 底层有用到 Rest
传输数据包，而这没有本质区别</p>
<p>http 头部冗余，效率低；但 http3 效率挺好但没普及</p>
<p>Rpc 更适合后台内部调用，效率高；Rest 适合所有场景，适配做的好</p>
<ul>
<li>OS 为什么需要虚拟地址</li>
</ul>
<p>没复习到的八股，说了下虚拟地址映射到物理地址，页的映射，道歉</p>
<ul>
<li>会什么语言</li>
</ul>
<p>还会点 Python，不懂 C</p>
<ul>
<li>反问，进去需要什么技术栈</li>
</ul>
<p>Java 微服务开发等</p>
<h1 id="小红书一面">2024.4.29 小红书一面</h1>
<blockquote>
<p>以后有机会的话可以问问 HR
这个部门是干什么的，再做相应的准备，不然临时抱佛脚可能抱错佛了。</p>
</blockquote>
<p>他温我哭，面试官又帅又贴心，看到我紧张也不催我。但是他一直在叹气，搞得我压力好大。面试总共30min，后续手撕了一道算法题20min，==已经好几次算法题读题不准确了，连续字串和子序列最容易理解错==</p>
<ol type="1">
<li><p>自我介绍</p></li>
<li><p>深入讲一下实习经历（完全没想到会问这个）</p></li>
</ol>
<p>数据中台项目</p>
<ul>
<li>了解项目架构，微服务和多模块开发，数据流过程</li>
<li>尝试开发字典结构</li>
<li>AOP 日志：运行时间、调用接口、速度、ip</li>
<li>调用微服务获取运维大屏数据，并展示：线程池、CompletableFuture
线程先后调用关系</li>
</ul>
<ol start="3" type="1">
<li>日志 AOP，能不能查找、过滤逻辑，为什么不用
ELK。如果有监控平台的话，为什么不存到监控平台里（看得出来面试官很奇怪怎么只有
AOP 记录日志，没有其他的相关功能）</li>
</ol>
<p>Leader 安排的，直接存到 MySQL 中即可（搪塞一下）</p>
<ol start="4" type="1">
<li>AOP 底层原理</li>
</ol>
<p>CGLib、Java 原生，SpringBoot 中只使用 CGLib 代理（一定要说 Spring
是可以配置的）</p>
<ol start="5" type="1">
<li>行，继续说（我超，怎么说啊说啥啊）</li>
</ol>
<p>扯到 Rpc 服务端接口代理；底层原理，编译成字节码，读入 JVM
前做一层代理</p>
<ol start="6" type="1">
<li>有 A 类中的 BC 方法。B 方法使用注解代理，再用 C 方法通过 this.B()
调用，会出现什么逻辑</li>
</ol>
<p>会直接调用 B 方法</p>
<ol start="7" type="1">
<li><strong>那如何让 this.B() 的代理生效呢</strong></li>
</ol>
<p>获取到被代理的 Bean，再调用 B 方法【如何获取被 AOP 代理的
Bean？】</p>
<ol start="8" type="1">
<li>修改一个数据，存储层、缓存层一起用的话怎么办</li>
</ol>
<p>双删一致性</p>
<ol start="9" type="1">
<li>单例模式实际应用场景，Spring 底层的设计有哪些用到了单例模式</li>
</ol>
<p>Spring 单例模式的实现</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> sychronize Singleton <span class="title function_">getInstace</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(instance==<span class="literal">null</span>)&#123;</span><br><span class="line">        <span class="keyword">synchronized</span>(Singleton.class)&#123;</span><br><span class="line">            <span class="keyword">if</span>(instance==<span class="literal">null</span>) instance=<span class="keyword">new</span> <span class="title class_">Singleton</span>(); </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> instance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>==底层针对单例模式的使用：IOC
容器、BeanFactory==、工具类、线程池、ApplicationContext自身、</p>
<ol start="10" type="1">
<li>底层创建 Bean 是线程安全的吗？一定会用到 <code>synchronized</code>
吗？</li>
<li>启动时直接把所有 Bean 都创建好，可不可以保证线程安全？</li>
</ol>
<p>Bean
生命周期中，初始化和依赖注入是两个阶段，不会相互影响，因此是安全的</p>
<ol start="10" type="1">
<li>MySQL 的索引机制</li>
</ol>
<p>聚簇索引；相比于跳表，B+ 树可以有效降低树的高度；【InnoDB 的 B+
树到达四层的情况下能装下多少数据？】</p>
<ol start="10" type="1">
<li>B+ 和 B 树的区别</li>
</ol>
<p>顺序访问叶子节点；中间节点不会存到下层节点</p>
<ol start="11" type="1">
<li>MySQL 乐观锁和悲观锁</li>
</ol>
<p><del>MVCC</del>
时间戳或版本号字段；间隙锁行锁表锁【悲观锁底层如何实现的？】</p>
<ol start="12" type="1">
<li><strong>update 用的是行锁还是表锁</strong></li>
</ol>
<p>在 update 语句的 where
条件没有使用索引，就会全表扫描，于是就会对所有记录加上 next-key
锁（记录锁 + 间隙锁），相当于把整个表锁住了</p>
<ol start="13" type="1">
<li>二级索引、联合索引、最左前缀匹配原则</li>
<li>Redis 怎么实现过期删除策略（不是内存淘汰策略）</li>
<li>缓存穿透</li>
</ol>
<p>太紧张了，回答成缓存雪崩了</p>
<ol start="13" type="1">
<li><strong>Redis 大 Key 问题的定义，解决方法</strong></li>
<li><strong>线程池的核心线程池的个数是如何设置的？</strong></li>
</ol>
<p>线程等待时间所占比例越高，需要越多线程；线程计算时间所占比例越高，需要越少线程；</p>
<ul>
<li>CPU密集型任务：N+1</li>
<li>IO密集型任务：2N</li>
</ul>
<p>手撕算法：【300. 最长递增子序列】一开始贪心没写出来，后面用 dp
写出来的</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> in.nextInt();</span><br><span class="line"><span class="type">int</span>[] data = <span class="keyword">new</span> <span class="title class_">int</span>[n];</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) &#123;</span><br><span class="line">    data[i] = in.nextInt();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="variable">ans</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span>[] dp = <span class="keyword">new</span> <span class="title class_">int</span>[n];</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) &#123;</span><br><span class="line">    dp[i] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>; j&lt;i; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (data[j]&lt;data[i]) &#123;</span><br><span class="line">            dp[i] = Math.max(dp[i], dp[j]+<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ans = Math.max(ans, dp[i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">System.out.println(ans);</span><br></pre></td></tr></table></figure>
<p><strong>总结QA</strong></p>
<p><strong>Q</strong>：JDK 动态代理和 cglib 的底层实现的区别？</p>
<p><strong>A</strong>：JDK：目标类加载后，使用 Native
方法在<strong>运行时动态生成代理类</strong>，将切面织入到代理类中；CGLib：目标类加载后，字节码构建框架
<strong>ASM
构建字节码</strong>文件，并生成目标类的子类，将切面逻辑加入到子类中；AspectJ：<strong>编译期间</strong>，将切面织入代理类中，得到代理后的字节码；</p>
<p><strong>Q</strong>：类上有 AOP
注解，如何分别获取被代理前和代理后的这个类呢？</p>
<p><strong>A</strong>：代理前：直接 new 一个对象；代理后：通过
<code>ApplicationContext</code> 或 <code>@Autowired</code>
注解来获取代理 Bean
对象，即<strong>容器中只会存放被代理过的类</strong>；</p>
<p><strong>Q</strong>：单例模式 Bean 是线程安全的吗？</p>
<p><strong>A</strong>：主要取决于 Bean 是否是无状态的，即 Bean
中是否包含可变的状态信息；</p>
<p><strong>Q</strong>：MySQL 如何实现乐观锁和悲观锁？</p>
<p><strong>A</strong>：乐观锁：版本号或时间戳字段；悲观锁：InnoDB
所有的行锁算法都是基于索引实现的，锁定的都是索引或索引区间；</p>
<p><strong>Q</strong>：update 用的是行锁还是表锁【TODO】</p>
<p><strong>A</strong>：Where
条件判断有索引就走索引，走索引就锁索引，索引锁了就是行锁，不然就是表锁</p>
<p><strong>Q</strong>：Redis 怎么实现过期删除策略</p>
<p><strong>A</strong>：惰性删除+定期删除（不采用定时删除）<img data-src="/%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB/%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5.jpg" class="" title="img"></p>
<p><strong>Q</strong>：Redis 大 Key 问题的定义，解决方法</p>
<p><strong>A</strong>：从应用服务、持久化角度分别作答</p>
<p><strong>定义</strong>：某个 key 所对应的 value 过大。具体来说，对于
String 类型的数据，如果其大小超过 10KB，一般被认为是大 key；而对于
set、zset、hash 等集合类型的数据，如果其包含的元素数量超过 5000
条，也通常被视为大 key。</p>
<p><strong>解决办法</strong>：</p>
<ul>
<li>把大 key 拆分成一个一个小 key；</li>
<li>将大 Key 拆分成多个小 Key 并用 Hash 结构存储；</li>
<li>定时检查 Redis 是否存在大 key ，如果该大 key 是可以删除的，不要使用
DEL 命令删除，因为该命令删除过程会阻塞主线程，而是用 unlink 命令（Redis
4.0+）删除大 key，因为该命令的删除过程是异步的，不会阻塞主线程；</li>
</ul>
<p>造成的不良影响：</p>
<ul>
<li>应用服务：Redis 是单线程的，大 Key
写入和删除会阻塞其他命令；内存分布不均，失去内存时空局部性特性；网络阻塞，IO
压力大；</li>
<li>持久化：Always 参数下数据同步到硬盘时会阻塞主线程；经常性触发AOF
重写机制；AOF 重写机制和 RDB 快照（bgsave 命令）的过程，都会分别通过
fork()
函数创建一个子进程来处理任务，会导致阻塞父进程（主线程）；集群模式下内存分布不均</li>
</ul>
<h1 id="恒生一面">2024.4.10 恒生一面</h1>
<p>虽然面试只有
17min，但感觉再长一点我真的要蚌埠住了，哭（在杭电十二教线下面的）</p>
<p>反思：自我介绍准备不足，都要开始面试了还在考虑怎么改（其实也怪他们面试通知太晚了，不怪我嘻嘻）平时学的不扎实，说不出
AQS 的底层实现原理我可以接受，但连 AQS
都没提到实在是太蠢了，果然还是准备的不够充分。谢谢恒生一巴掌拍醒了我，认清了自己还是个小菜鸡。摆正心态，继续努力！！！</p>
<ol type="1">
<li>自我介绍</li>
</ol>
<p>面试官看了简历，说“不愧是软件工程内容比别人多多了”，面试官真的是很认真的把简历从头看到尾（泪目）</p>
<ol start="2" type="1">
<li>Rpc 项目具体是什么</li>
</ol>
<p>因为自我介绍里说了为什么要做 RPC 项目，面试官顺势问了。</p>
<p>简单说了上段实习中用到了自定义 Cluster 集群策略，因为 Dubbo 自带的
ClusterRules（广播、失败立刻返回）没有涉及到轮询检查离线任务是否完成的功能，所以
Leader 让我利用 Dubbo 的 SPI
机制做了一个查询微服务的计算过程是否完成的方法，然后挂载在
META-INF/dubbo 下就 Dubbo
框架就可以直接调用该集群执行方式（当时说的并没有这么清晰，还是太菜了呜呜）</p>
<ol start="3" type="1">
<li>如何分析 dump 文件</li>
</ol>
<p>感觉简历里唯一吸引面试官的就是如何处理 OOM Crash 了。</p>
<p>测试报服务异常，线程池等待队列过长；Leader 带我用 jstack
获得线程的状态；发现该线程池中的所有线程都处在阻塞状态，并且都阻塞在父线程；查看父子线程调用关系和
Executors.<em>newFixedThreadPool</em>(25) 底层原理，发现 FixedThreadPool
使用了无限长的阻塞队列，父线程未执行完成，且子线程只能在队列中等待，造成死锁所以队列一直在变长</p>
<ol start="4" type="1">
<li>synchronized Reentrantlock 的具体使用场景有什么不同</li>
</ol>
<p>只知道底层实现原理不同且都是可重入锁，并不知道具体使用场景的区别（哪来的这么真实业务多场景啊喂）</p>
<ol start="5" type="1">
<li>MySQL 调优</li>
</ol>
<p>愚蠢的我只知道构建索引、优化表结构和增大
BufferPool，好在后续上网也没找到什么很好的方法。或许==预编译 SQL
语句==是个不错的想法，根据 MySQL 运行过程推出的优化方法。</p>
<h1 id="面试-prepare">面试 Prepare</h1>
<h2 id="自我介绍">自我介绍</h2>
<blockquote>
<p>由于错误使用 Dubbo 注解导致的 JVM 异常问题；Rpc 中间件采用 Netty
传输数据、Zookeeper
注册服务;调度各微服务，聚合并缓存数据；我还和同事一起分析解决了线上告警事故；</p>
</blockquote>
<p>面试官您好，很荣幸能来参加华为的面试，我叫王哲文，目前在杭州电子科技大学读研，本科就读于浙大城市学院。</p>
<p>本科期间我加入了黑胡桃实验室，参与了智慧实验室和时间序列预测等项目，还在实验室举办的
2020
年谷歌开发者大会上分享了自己的项目经验。我在浙江华坤道威数据科技有限公司实习期间，主要负责日志切面和运维大屏的开发，在这期间我积累了企业级分布式微服务项目的开发经验。</p>
<p>在读研期间，我加入了大数据与城市计算实验室，主要负责三友阴极板项目的后端开发和某涉密项目的大语言模型应用开发。在项目开发过程中，我不仅更加深入掌握了
SpringBoot、MySQL、Redis 等主流技术，还了解了 Langchain、PyTorch、Milvus
等前沿技术。在工作之余，我还学习了 Rpc
中间件的架构和设计思想，并开发了一个简易的轻量级 Rpc
中间件。此外，我还搭建了个人博客网站，定期发布学习笔记和技术总结，不断沉淀自己的开发经验。</p>
<p>最后，我非常希望能有机会加入华为，将所学知识应用到实际工作中，并与优秀的团队一起共同成长。谢谢！</p>
<h2 id="项目困难解决方法">项目困难/解决方法</h2>
<p>实习</p>
<ul>
<li>分布式微服务架构复杂 === 请教前辈、打断点弄明白数据流；</li>
<li>多线程并发，编排任务执行流程 === 美团技术博客 CompletableFuture
原理和实践；</li>
<li>沟通协作，和同事交流，理解别人的想法的同时清楚地表达自己的想法；让领导清楚自己在做什么，工作量和重难点在哪
=== 多沟通多交流，先理清自己的想法再和同事、领导沟通；</li>
</ul>
<p>LLM 应用开发</p>
<ul>
<li>LLM 部署，技术选型 FastChat、FastApi、VLLM 加速 ===
实践尝试，选择最合适的方法；</li>
<li>思考如何实现需求 ===
参考别人发表的经验和思路，比如看论文、技术博客、线上宣讲会、开源项目等，在针对需求改进和优化；</li>
<li>Prompt 构建，数据切分、数据集构建：分词、分段、Embedding 计算</li>
</ul>
<p>RPC 中间件</p>
<ul>
<li>Netty 数据传输</li>
<li>RPC架构==Bean 生命周期，分布式注册中心：zk（CP、zab算法）</li>
</ul>
<p>三友</p>
<ul>
<li>数据库缓存一致性设计</li>
<li>新需求的开发</li>
</ul>
<h2 id="qa">Q&amp;A</h2>
<p><a
href="https://www.bilibili.com/read/cv13183086/#:~:text=%EF%BC%88%E7%AE%80%E5%8C%96%E7%89%88%EF%BC%89,%E5%8D%8E%E4%B8%BA%E7%9A%84%E6%A0%B8%E5%BF%83%E4%BB%B7%E5%80%BC%E8%A7%82%E6%9C%89%E5%9B%9B%E4%B8%AA%E6%96%B9%E9%9D%A2%EF%BC%9A1%E3%80%81%E4%BB%A5%E5%AE%A2%E6%88%B7%E4%B8%BA%E4%B8%AD%E5%BF%83%EF%BC%8C%E5%AE%83%E5%8F%AF%E4%BB%A5%E7%A1%AE%E5%AE%9A%E5%A5%8B%E6%96%97%E7%9A%84%E6%96%B9%E5%90%91%3B2%E3%80%81%E4%BB%A5%E5%A5%8B%E6%96%97%E8%80%85%E4%B8%BA%E6%9C%AC%EF%BC%8C%E5%AE%83%E5%8F%AF%E4%BB%A5%E6%8F%90%E4%BE%9B%E6%B4%BB%E5%8A%9B%E7%9A%84%E6%BA%90%E6%B3%89%3B3%E3%80%81%E9%95%BF%E6%9C%9F%E8%89%B0%E8%8B%A6%E5%A5%8B%E6%96%97%EF%BC%8C%E8%BF%99%E6%98%AF%E5%AF%B9%E4%BA%BA%E4%BF%AE%E8%BA%AB%E7%9A%84%E4%B8%80%E4%B8%AA%E8%BF%87%E7%A8%8B%3B4%E3%80%81%E5%9D%9A%E6%8C%81%E8%87%AA%E6%88%91%E6%89%B9%E5%88%A4%EF%BC%8C%E5%9C%A8%E6%89%B9%E5%88%A4%E4%B8%AD%E8%83%BD%E5%A4%9F%E5%BE%97%E5%88%B0%E4%BF%AE%E5%BF%83%E3%80%82">华为核心价值观</a></p>
<ul>
<li>以客户为中心，以奋斗者为本，长期艰苦奋斗，坚持自我批判</li>
</ul>
<p>说说自己对于艰苦奋斗的看法（精神上的）</p>
<ul>
<li>我认为艰苦奋斗是一种积极向上、不断进取的品质，是为了实现自身价值，努力过上充实且有意义的人生，而不仅仅是为了眼前的利益；</li>
<li>结合我自己的经历来看，虽然中考失利了，但我仍保持努力拼搏的信念，在后续的高考、考研中不断超越自己，才有机会接触到了更多有挑战有意义的事情；</li>
</ul>
<p>科研&amp;项目：</p>
<ul>
<li><p>担任角色；负责的工作；项目背景；前期调研；创新点</p></li>
<li><p>项目有哪些难点</p></li>
<li><p>开发过程中遇到问题；如何解决</p></li>
<li><p>科研过程中遇到问题；如何解决</p>
<ul>
<li>一开始对科研内容一窍不通，通过不断的学习试错总结慢慢积累了经验</li>
</ul></li>
</ul>
<p>接下来的一年里有什么计划</p>
<ul>
<li>专利修改，秋招找工作，写大论文+毕业答辩</li>
<li>学习大数据技术栈（Hadoop、Map-Reduce、Hbase），出国旅行</li>
</ul>
<p>未来职业规划</p>
<ul>
<li>进入优秀的团队中工作学习；</li>
<li>前四年深耕技术，不断学习，沉淀技术栈，拓宽视野；</li>
<li>抓住机遇，第六、七年应聘管理层，接业务，带团队，一个人的力量终究是有限的，只有激发团队的力量才能办成大事；</li>
</ul>
<p>为了达成这些规划具体会做些行动</p>
<ul>
<li>学习大数据相关的技术栈；</li>
<li>抓住现有机会，积极投身工作中，不断提升技术能力，扩大人脉。耐心地等待机遇，当机遇来临时果断抓住它；</li>
</ul>
<p>绩点排名</p>
<ul>
<li>大一只有中游水平，后面几年排名逐年上升，大三拿到了学业奖学金并考研成功。在读研期间也能有中上游的绩点，拿到了学业奖学金；</li>
</ul>
<p>如何看待华为公司？为什么想来华为公司进行实习？</p>
<ul>
<li>无论是在芯片、操作系统还是在手机、汽车等领域，华为都是中国的一线品牌，华为当下在做的事情相当于五十年前的核武器研究；</li>
<li>参加华为举办的竞赛感受到华为的人文关怀做得很好，比赛场地布置的很用心，比赛结束后还举办了活动，吃了大餐，临走还送和一袋纪念品，包括所有人的合照；</li>
<li>体验大厂的氛围和管理制度，和优秀的团队一起成长，努力工作，争取毕业留在华为；</li>
</ul>
<p>能够实习多长时间？</p>
<ul>
<li>半年，立刻到岗</li>
</ul>
<p>对什么感兴趣并做了什么努力？</p>
<ul>
<li>对大语言模型很感兴趣；LLM
的训练和微调对硬件和数据集的要求高，但实验室只有 A6000，更加适合 LLM
的应用开发，看了很多论文、开源项目和技术博客，最终让基于大语言模型实现一个具体需求的过程非常有成就感；</li>
</ul>
<p>科研期间最大的困难</p>
<ul>
<li>对语言模型没有任何了解，实验室也是第一次接语言模型的项目，没有任何沉淀，一开始上手很艰难，只能不断重复去看网课和教程；</li>
</ul>
<p>如何获取资料</p>
<ul>
<li>论文、技术博客、开源项目、社群宣讲会</li>
</ul>
<p>什么东西起了很大的作用</p>
<ul>
<li>技术博客，因为它更加贴近实际落地的工作，大佬们留下的经验往往给我灵感</li>
</ul>
<p>重大的挫折</p>
<ul>
<li>人生中最大的挫折应该是中考，我当初冲重高保底优高，最后发挥失常只考上了职高；</li>
<li>高中
保持学习的劲头，稳扎稳打，高考超重点线；这段经历让我任何时候都不会轻言放弃，即使失败了也没事，总结反思之后再出发，家人和朋友一直都在，成功与否只是我自己的选择；</li>
</ul>
<p>压力特别大的时候</p>
<ul>
<li>跑步、爬山、骑行</li>
</ul>
<h2 id="科研内容">科研内容</h2>
<p>应用背景：将知识图谱应用在大模型领域，提升 LLM
响应的准确性和逻辑性；</p>
<p>创新点：不同于目前流行 RAG架构 思想，并不是通过 Prompt
帮助模型获取外部知识，而是将知识图谱中的数据训练到模型中，微调后的模型不仅得到了图谱中的外部知识，还获取了图谱中节点之间的逻辑性（对话过程中的）；</p>
<p>具体实现：</p>
<p>（1）
提出了动态策略模型，跟踪历史对话中用户语句多类特征的动态变化，抽取对话中深层次的关系和属性。</p>
<p>（2）设计了一个基于注意力机制的异构图网络，将用户的意图、历史策略和上下文对话进行交互，选择最优的支持策略以生成有效的支持响应。</p>
<p>（3）考虑到全局策略控制着整个对话流，DSA设计了监控全局策略信息的子任务和上下文到全局状态和全局状态到上下文的门限控制网络。</p>
<p>实验：应用于情感支持对话领域，将情感支持策略视作一种类型的知识图谱，同时引入常识性知识图谱，微调
BlenderBot 模型学习知识图谱的节点特征，提高模型的在 ESConv
数据集上的指标；</p>
<p><strong>步骤</strong></p>
<p>【3】</p>
<p>3 Method【2】</p>
<ul>
<li>3.1 overall
<ul>
<li>提出xxx，有效解决xxx，比别人好在哪xxx。包含了三个模块，介绍xxx，数据流xxx，提高了哪些xxx；由于这三个特征不能有效表示xxx，所以用异构图网络xxx，可得到两个状态，xxx。因此，提出的模型，有一样也有有不一样【总分总】</li>
</ul></li>
<li>3.2 archieve</li>
<li>3.3</li>
<li>3.4</li>
</ul>
<p>4 实验【3】</p>
<ul>
<li></li>
<li>指标，解释，为什么好，好在哪</li>
</ul>
<p>5 结果【1】</p>
<p>6 总结【3】</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>面试记录</tag>
      </tags>
  </entry>
  <entry>
    <title>AugESC</title>
    <url>/2023/AugESC/</url>
    <content><![CDATA[<p><strong>Title:</strong> Large-scale Data Augmentation for Emotional
Support Conversation with Pre-trained Language Models</p>
<blockquote>
<p>研究开放式对话数据增强，采用大语言模型 GPT-3 拓展了 ESConv
数据集大小</p>
</blockquote>
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<h2 id="abstract">Abstract</h2>
<ul>
<li>利用LLM进行数据增强，使用公开的对话帖子触发各种主题的对话</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li><p>目前工作的缺陷</p>
<ul>
<li>成本高、耗时长</li>
<li>预算限制，所收集的对话规模小，主题少</li>
</ul></li>
<li><p>本文主要贡献</p>
<ul>
<li>关键发现</li>
<li>使用 GPT-J 和公开对话帖子触发各种主题的对话</li>
<li>构建机器增强数据集AUGES，具有更广泛和多样化的主题覆盖范围，可以提供更有效的情感支持
## Related Work</li>
</ul></li>
<li><p>预训练模型</p></li>
<li><p>预训练模型的数据增强 ## Key Findings</p></li>
<li><p>语言模型优于对话模型</p>
<ul>
<li>语言模型存储了从大规模训练语料库中学习到的更丰富的知识，有助于更好地泛化到各种对话主题</li>
<li>与会话模型 BlenderBot
相比，gpt生成的对话具有更好的对话连贯性和一致性</li>
</ul></li>
<li><p>语言模型比交互式仿真更适合开放式对话数据增强</p></li>
<li><p>提示GPT不如微调GPT模型</p>
<ul>
<li>提示型GPT-3生成可控性差</li>
<li>只有微调才能掌握任务场景和所需特征</li>
</ul></li>
<li><p>少样本（Few-shot）微调导致更好的泛化和更高的多样性</p>
<ul>
<li>保持语言模型的内在知识</li>
<li>增加调优样本或训练步骤会导致对域外主题的泛化能力差</li>
<li>在大规模自动数据增强的帮助下，训练对话模型可能只需要少量手动策划的对话样本</li>
</ul></li>
<li><p>信息性查询（第一个对话帖子）是触发主题对话的必要条件</p>
<ul>
<li>泛型和无信息的查询往往导致离题和肤浅的对话 ## Methodology</li>
</ul></li>
<li><p>主干模型：GPT-3，微调后的GPT-J</p></li>
<li><p>提示模板：对话场景+情感支持</p></li>
<li><p>将第一个对话框作为触发查询，模型生成后续的对话</p></li>
<li><p>不采用Prompt提示，使用ESConv微调GPT-J</p></li>
<li><p>触发Query</p>
<ul>
<li>数据来源：EmpatheticDialogues（移情对话数据集）Reddit（心理健康相关的帖子）</li>
<li>保留带有负面情绪的Query</li>
</ul></li>
<li><p>过滤结果，删除非法对话 ## AUGESC
相比ESConv对话轮次更少，内容更长。语料库规模的扩大导致唯一二元分词的数量</p></li>
<li><p>ESConv中的对话话题与数据收集时期(如covid, pandemic,
christmas)密切相关</p></li>
<li><p>AUGESC-ED
涵盖了更多关于日常生活的主题(例如，汽车、狗、房子、邻居)</p></li>
<li><p>AUGESC-Reddit 涵盖了关于心理健康的主题(例如，抑郁、焦虑、治疗师)
## Quality Evaluation 在信息一致性、话题一致性和对话基础等方面存在问题
## Interactive Evaluation &gt;
AUGESC是对ESConv的一种补充，用AUGESC+ESConv训练出来的模型表现优于只使用ESConv的模型</p></li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>AUGESC能够显著增强对话模型提供情感支持的能力</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
        <tag>对话数据集增强</tag>
      </tags>
  </entry>
  <entry>
    <title>Control Globally, Understand Locally</title>
    <url>/2023/Control%20Globally,%20Understand%20Locally/</url>
    <content><![CDATA[<p><strong>Title:</strong> Control Globally, Understand Locally. A
Global-to-Local Hierarchical Graph Network for Emotional Support
Conversation.</p>
<p>创新点：</p>
<ul>
<li>在 Encoder 和 Decoder 之间加了一层 GCNConv 和 RGCNConv</li>
</ul>
<img data-src="/2023/Control%20Globally,%20Understand%20Locally/image-20240414083742062-17138602623661-17138606243731.png" class="" title="image.png">
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<h2 id="abstract">Abstract</h2>
<ul>
<li><p>目前研究的缺陷</p>
<ul>
<li>关注序列上下文信息，忽略了全局原因和局部心理意图与其的层次关系</li>
</ul></li>
<li><p>本文</p>
<ul>
<li>提出了一个全局到局部的<strong>层次图网络</strong>来捕获多源信息(全局原因、局部意图和对话历史)并建模它们之间的层次关系，该网络由一个多源编码器、一个层次图推理器和一个全局引导解码器组成</li>
<li>设计了一个新的训练目标来监测全局的语义信息 ## Introduction</li>
</ul></li>
<li><p>探索求助者情绪问题的原因：<strong>全局</strong>地控制情感支持对话的整个流程</p></li>
<li><p>了解求助者的心理意图：帮助系统<strong>局部</strong>了解求助者当前时的心理状态</p></li>
<li><p>本文目标</p>
<ul>
<li>捕获全局原因和局部心理意图</li>
<li>建模全局与局部之间的关系</li>
</ul></li>
<li><p>解决方法【全局到局部层次图网络GLHG】</p>
<ul>
<li>多源编码器：COMET提取局部心理意图</li>
<li>层次图推理机：全局原因(对话级)、局部心理意图(句子级)和对话历史之间的层次关系进行建模</li>
<li>解码器中设计了一个新的训练目标来监控全局原因的语义信息 ## Related
Work</li>
</ul></li>
<li><p>图建模对话</p>
<ul>
<li>GCN 利用自我和对话者间依赖性来模拟会话上下文</li>
<li>EGAE 使用图网络捕获面向任务对话中的对话模式</li>
</ul></li>
<li><p>常识性知识</p>
<ul>
<li>与本人有关的心理状态：xReact、xIntent ## Approach</li>
</ul></li>
<li><p>问题定义</p></li>
<li><p>多源编码器</p>
<ul>
<li>BlenderBot Encoder + Max-pooling</li>
<li>上下文 + 全局原因 + 局部原因</li>
</ul></li>
<li><p>分层图推理机</p>
<ul>
<li>GAT
图注意力网络：其他邻域信息的特征传播到当前节点，具有确定节点之间重要性和相关性的优点</li>
<li><img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701777471738-5656cb06-b30d-458c-9dd1-432cf59fc949.png" class="" title="img">
注意力函数（2017出版)</li>
<li><img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701777606138-33d97b8c-8966-4a59-9359-ce99195ba4db.webp" class="" title="image.png">
<img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701777482262-fd172bc6-5f7f-4b43-b667-c12cd9b52505.png" class="" title="img">
注意力机制</li>
<li><img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701777489757-3d64bf9e-8b95-4298-a754-239e59ec6e7b.png" class="" title="img"></li>
<li><img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701777519559-801ad4da-0c7f-47f2-8ab6-ed20e4e19101.png" class="" title="img">
注意力机制</li>
<li><img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701777507833-1ff3f1f2-17cb-4ba0-991b-a61b42818cfc.png" class="" title="img"></li>
</ul></li>
<li><p>Global-guide 解码器</p>
<ul>
<li>响应生成
<ul>
<li><img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701777873314-d88aa900-555f-40b2-8a87-4589351ce738.png" class="" title="img">
，v表示图神经网络得到的全局变量</li>
</ul></li>
<li>监督全局语义信息：预测问题类型
<ul>
<li><img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701777996376-86594b70-a6b5-4f21-90e2-e6bbdbc222b0.png" class="" title="img"></li>
</ul></li>
</ul></li>
<li><p>联合训练</p>
<ul>
<li>对数似然损失+交叉熵损失</li>
<li><img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701778036470-9378fc9b-a533-40a9-9990-69ffe0036656.png" class="" title="img">
<img data-src="/2023/Control%20Globally,%20Understand%20Locally/1701778063569-1fc4c281-c256-4804-ba39-785ddd2e4b1e.png" class="" title="img">
<h2 id="experiments">Experiments</h2></li>
</ul></li>
<li><p>指标：plexity (PPL), BLEU-n (B-n), ROUGE-L (R-L),
Distinct-1(D-1), and Distinct-2 (D-2)</p></li>
<li><p>由于有了意图特征，提出建议更具体有效 ## Conclusion</p></li>
<li><p>全局到局部的层次图网络(Global-to-Local Hierarchical Graph
network, GLHG)来捕获多源信息并从全局到局部的角度建模层次关系</p></li>
<li><p>新的训练目标“预测Seeker遇到问题的类型” # 关注的问题 /
本文的优势</p></li>
</ul>
<h1 id="解决方法-创新点">解决方法 / 创新点</h1>
<ul>
<li><strong>多源编码器</strong>利用情境信息并将心理意图与COMET结合，捕获全局原因和局部意图</li>
<li><strong>分层图推理机</strong>在全局原因、局部心理意图和对话历史之间进行交互，建模不同层次的关系（hierarchical
graph reasoner） # 实验结论</li>
</ul>
<h1 id="有待提升的部分">有待提升的部分</h1>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
        <tag>异构图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>DQ-HGAN</title>
    <url>/2023/DQ-HGAN/</url>
    <content><![CDATA[<p><strong>Title:</strong> A heterogeneous graph attention network based
deep Q-learning for emotional support conversation generation</p>
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<h2 id="abstract">Abstract</h2>
<ul>
<li>关注的问题
<ul>
<li>动态建模对用户状态，包含个体的意图和情感</li>
<li>综合各类因素选择最合适的支持策略</li>
</ul></li>
<li>提出的方法【基于异构图注意力网络的深度Q-learning情感支持对话生成】
<ul>
<li>为了捕获用户意图、情感和历史对话之间的交互关系，基于意图词典和情感分类器，构建了异构图注意力网络</li>
<li>采用基于DQN的最优响应策略以指导响应生成，优于传统的基于规则或启发式方法
## Introduction</li>
</ul></li>
<li>目前研究
<ul>
<li>ESC任务要求能够确定求助者的心理意图和情绪状态，以便提供适当的支持。因此整合意图识别和情感识别对于提高情感支持对话的质量至关重要，且目前的方法对用户状态建模不充分。</li>
</ul></li>
<li>关注的问题
<ul>
<li>建模用户状态</li>
<li>选择最优策略，以产生有效的保障响应</li>
</ul></li>
<li>提出的方法
<ul>
<li>设计了基于注意力的<strong>异构图网络</strong>，与用户的意图、情感和历史对话交互，可以有效地捕获和建模图中不同类型的节点和边</li>
<li>构建<strong>意图词典</strong>和<strong>情感分类器</strong>来捕捉求助者在语境中的细微情感表达</li>
<li><strong>DQN算法</strong>对用户未来反馈的期望值进行估计，帮助系统选择获得最优长期值的策略。其允许系统从用户的反馈中学习，调整其策略，以提供最有效的支持响应。</li>
</ul></li>
<li>主要贡献
<ul>
<li>提出了一种新的方法DQ-HGAN，将<strong>意图和情感识别与策略生成相结合</strong>，以提高情感支持对话系统的质量和个性化</li>
<li>构建<strong>意图词典和情感分类器</strong>，捕捉求助者在语境中的细微情感表达并跟踪其状态</li>
<li>设计了一种基于注意力机制的<strong>异构图网络</strong>，与用户的意图、情感和历史对话进行交互，并选择最优的支持策略以生成有效的支持响应</li>
<li>ESC生成中使用ESC生成中使用<strong>强化学习</strong>，具体来说，使用DQN算法（Deep
Q-Network）估计用户未来反馈的期望值，动态调整策略以提供最有效的支持响应
## Related Work</li>
</ul></li>
<li>对话中的意图和情感识别【在模型中融合了“意图”这个特征】
<ul>
<li>多头注意力机制
<ul>
<li>多头注意力机制来捕捉用户的意图和情感。缺点：缺乏有效捕捉用户细微情感表达的能力</li>
<li>使用预训练模型，增强PLM对话相关性，识别对话意图、推断对话情感。缺点：不是专门为ESC任务定制的，性能差</li>
</ul></li>
<li>词典
<ul>
<li>词典包含特定意图或情感相关的词汇和短语，利用基于规则的算法将context与意图词典进行匹配，并分配相应的意图标签。缺点：只将单个单词与标签匹配，可能会忽略整个句子的意图或情感含义</li>
</ul></li>
</ul></li>
<li>图建模【捕获会话系统中用户意图、情感和对话历史之间的复杂关系】
<ul>
<li>同构图【忽略了用户意图和情感的异构性】
<ul>
<li>GAT
图注意力网络，利用自注意力机制来捕获对话图中意图和情感节点之间的交互</li>
<li>GCN
图卷积网络，利用图结构在节点之间传播信息，并捕获对话数据中的上下文依赖</li>
</ul></li>
<li><strong>异构图注意力网络是专为表示图中不同类型的节点和边而设计的</strong>，它擅长对<strong>不同的节点</strong>类型进行建模，如用户话语、系统响应、情感状态和意图，从而更全面地了解用户的情感状态；还擅长捕捉<strong>不同类型的边</strong>，包括顺序依赖、自依赖和状态依赖，从而能够更准确地表示用户的情感状态。此外，它还包含了一种<strong>注意力机制</strong>来进行重要性加权，允许它在聚合过程中专注于最相关的信息，从而更全面地了解用户的状态。</li>
</ul></li>
<li>策略选择
<ul>
<li>基于规则或启发式方法</li>
<li>强化学习方法（如：Q-learning）
<ul>
<li>采用DQN估计不同对话动作的期望值，并学习了一种最大化该值的策略。从用户反馈中学习，并生成更有吸引力和信息量的响应</li>
</ul></li>
</ul></li>
<li>响应生成
<ul>
<li>目前流行的Encoder-Decoder模型往往专注于根据对话历史生成回复，而没有考虑用户的意图、情感以及合适的支持策略
## Preliminaries</li>
</ul></li>
<li>ESConv：标记对话，并将其转换为词嵌入，以将其输入到模型中</li>
<li>COMET：使用COMET初始化模型的词嵌入，并在ESConv数据集上进行微调，以提高其构建意图词典的有效性</li>
<li>ATOMIC：得到意图或目的（xIntent）</li>
<li>NRC VAD
词典：得到情感词典，每个单词对应的效价-觉醒-支配（Valence-Arousal-Dominance）</li>
<li>问题定义：上下文+策略+Query
=&gt;响应Yt。最优策略基于当前状态和期望的长期回报（通过Q-learning预测）
## Method <img data-src="/2023/DQ-HGAN/1701484543763-403c4648-68f4-4644-bf00-e2356c06841c.jpg" class="" title="88c61f5be72a795a087441904fcd0ad9_3_Figure_2_780332990.png"></li>
</ul>
<h3 id="多源编码器">多源编码器</h3>
<img data-src="/2023/DQ-HGAN/1701484574764-c5051f60-2b3c-40cf-a161-5848c1d67bc2.png" class="" title="img">
<ul>
<li>transformer编码器（TransformerEncoder）
<ul>
<li>ht = TransformerEncoder(Ht)</li>
</ul></li>
<li>意图词典（COMET）：通过对ATOMIC
微调，同去意图关键词，构建意图词典（意图关键词，对应的词嵌入）
<ul>
<li><span
class="math display">\[g_{t}=TransformerEncoder\left(\sum_{w_{i}\in\mathscr{F}}\operatorname{softmax}\left(c_{i}^{T}
h_{t}\right)c_{i}\right) \]</span></li>
</ul></li>
<li>情感分类器（NRC VAD词典）
<ul>
<li><span
class="math display">\[e_{t}=\text{TransformerEncoder}\left(\sum_{w_{k}
\in \mathscr{Z}}\operatorname{softmax}\left(z_{k}^{T} h_{t}\right)
g_{t}\right) \text { }\]</span></li>
</ul></li>
</ul>
<h3
id="基于异构图的用户状态跟踪">基于异构图的用户状态跟踪<img data-src="/2023/DQ-HGAN/1701484605842-86e4e5d2-9d20-4ffd-ae60-37220c9258dd.png" class="" title="img"></h3>
<h3 id="dqn强化学习">DQN强化学习<img data-src="/2023/DQ-HGAN/1701484626201-fde66e58-39a7-40f7-9e6e-fe70a7cd8dd7.png" class="" title="img"></h3>
<h3 id="响应生成解码器">响应生成解码器<img data-src="/2023/DQ-HGAN/1701484638477-fd1f5c6b-b6f5-43f0-af14-a22d10dfe762.png" class="" title="img"></h3>
<h2 id="experiments">Experiments</h2>
<p>Conclusion</p>
<h1 id="关注的问题-本文的优势">关注的问题 / 本文的优势</h1>
<h1 id="解决方法-创新点">解决方法 / 创新点</h1>
<h1 id="实验结论">实验结论</h1>
<h1 id="有待提升的部分">有待提升的部分</h1>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
        <tag>异构图神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>Feedback-Aware Double</title>
    <url>/2023/FADO/</url>
    <content><![CDATA[<p><strong>Title:</strong> Feedback-Aware Double COntrolling Network for
Emotional Support Conversation</p>
<img data-src="/2023/FADO/1701141604157-75101287-52dc-426c-abea-73184a8b8cda.png" class="" title="img">
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<h2 id="abstract">Abstract</h2>
<ul>
<li><p>双层反馈策略选择器：通过反馈信息预测策略</p></li>
<li><p>双层控制阅读器：通过策略约束上下文响应</p></li>
<li><p>策略词典：丰富策略的语义信息 ## Introduction ## Related Work ##
Problem Formulation ## Approach</p></li>
<li><p>上下文编码器</p>
<ul>
<li>BlenderBot预训练编码器编码上下文历史对话U，得到Ht</li>
</ul></li>
<li><p>双层反馈策略选择器</p>
<ul>
<li>策略选择
<ul>
<li>输入
<ul>
<li>BlenderBot Encoder 编码上下文历史对话U，得到隐状态Ht</li>
<li>BlenderBot Encoder 编码策略S</li>
<li>EmoBERTa Encoder 编码上下文对话U</li>
</ul></li>
<li>公式
<ul>
<li>上下文编码器（策略S同理）：<span
class="math display">\[\boldsymbol{H}=\operatorname{Enc}_{cxt}\left(\boldsymbol{[CLS]},
\boldsymbol{u}_{1}, \boldsymbol{[SEP]}, \boldsymbol{u}_{2}, ...,
\boldsymbol{u}_{M}\right)\]</span>，M为对话数
<ul>
<li><span
class="math display">\[\boldsymbol{H}=\left(\boldsymbol{u}_{1},...,
\boldsymbol{u}_{T}\right)\]</span>，T为Token数</li>
</ul></li>
<li>情感编码器：<span
class="math display">\[\boldsymbol{E}=\operatorname{EmoBERTa}\left(\boldsymbol{[CLS]},
\boldsymbol{u}_{1}, \boldsymbol{[SEP]}, \boldsymbol{u}_{2}, ...,
\boldsymbol{u}_{M}\right)\]</span>，M为对话数
<ul>
<li><span
class="math display">\[\boldsymbol{E}=\left(\boldsymbol{e}_{1},...,
\boldsymbol{e}_{T}\right)\]</span>，T为Token数</li>
</ul></li>
<li>分类：<span
class="math display">\[\boldsymbol{o}=\operatorname{MLP}\left(\tanh
\left(\boldsymbol{W}_{o}^{T}[\boldsymbol{s} ; \boldsymbol{c} ;
\boldsymbol{r}]+\boldsymbol{b}_{\boldsymbol{o}}\right)\right)\]</span>
<ul>
<li>s、c、r 为策略S、上下文H、情感分类E。编码+平均池化操作后得到</li>
</ul></li>
</ul></li>
</ul></li>
<li>双层反馈
<ul>
<li>回合级反馈：局部变量，当前用户的感受。包含每轮对话Seeker情感Δe和Seeker评分变化Δr</li>
<li>对话级反馈：全局变量Δc，用户的全局状态。包含Seeker在谈话后的情绪压力、Supporter对话题的回应的相关性、Supporter对Seeker感受的理解和共情</li>
<li>融合adapter：整合回合级和会话级反馈的两类语义信息。Δs = Δe + Δr +
uΔc【计算损失时，给予正向或负向的反馈】</li>
</ul></li>
</ul></li>
<li><p>双控读取器（模仿情感聊天机器ECM）</p>
<ul>
<li>context-to-strategy：利用上下文信息来选择上下文相关的策略
<ul>
<li><span class="math display">\[\boldsymbol{g}^{c} =
\operatorname{sigmoid}\left(
\boldsymbol{W}_{c}^{T} \boldsymbol{c} + \boldsymbol{b}_{c}
\right)\]</span></li>
</ul></li>
<li>strategy-to-context：编码阶段可以关注与策略相关的上下文，从而生成策略约束的响应
<ul>
<li><span class="math display">\[\boldsymbol{g}^{o} =
\operatorname{sigmoid}\left(
\boldsymbol{W}_{o}^{T} \boldsymbol{o} + \boldsymbol{b}_{o}
\right)\]</span></li>
</ul></li>
<li>残差连接：在原始信息和更新信息之间进行权衡
<ul>
<li><span class="math display">\[\begin{array}{r}
o^{\prime}=(1-\beta) \cdot o+\beta \cdot g^{c} \otimes o \\
h_{t}^{\prime}=(1-\alpha) \cdot h_{t}+\alpha \cdot g^{o} \otimes h_{t}
\end{array}\]</span></li>
</ul></li>
</ul></li>
<li><p>策略字典</p>
<ul>
<li>输入策略令牌的描述，
而不是策略令牌，以便模型对策略进行更深入的理解</li>
<li>Encoder-Decoder
之间的状态传输类似于MISC，采用cross-attention代替self-attention<a
href="https://www.yuque.com/jinzang/lnx420/wrq82cgnp6p47q59?view=doc_embed">MISC:
A MIxed Strategy-Aware Model Integrating COMET for Emotional Support
Conversation</a></li>
</ul></li>
<li><p>响应生成</p>
<ul>
<li>BlenderBot Decoder：<span
class="math display">\[\boldsymbol{p}\left(y_{z} \mid
\boldsymbol{y}_{&lt;z}, \boldsymbol{h}_{\boldsymbol{t}}^{\prime},
\boldsymbol{V}\right)=\text { Generator }\left(\boldsymbol{W}_{y&lt;z},
\boldsymbol{h}_{\boldsymbol{t}}^{\prime},
\boldsymbol{V}\right)\]</span></li>
</ul></li>
<li><p>联合训练</p>
<ul>
<li>策略预测：反馈感知负对数似然 feedback-aware negative log-likelihood
<ul>
<li><span
class="math display">\[\mathcal{L}_{1}=\left\{\begin{array}{ccc}
-\hat{o} \log
\left(\operatorname{softmax}\left(\boldsymbol{o}^{\prime}\right)\right)
&amp; \text { if } &amp; \Delta s&gt;0 \\
-\hat{o} \log
\left(1-\operatorname{softmax}\left(\boldsymbol{o}^{\prime}\right)\right)
&amp; \text { if } &amp; \Delta s \leq 0
\end{array}\right.\\\]</span></li>
</ul></li>
<li>响应生成：标准交叉熵损失优化 cross-entropy
<ul>
<li><span class="math display">\[\mathcal{L}_{2}=-\sum_{z=1}^{Z} \log
\boldsymbol{p}\left(y_{z} \mid \boldsymbol{y}_{&lt;z},
\boldsymbol{h}_{\boldsymbol{t}}^{\prime}, \boldsymbol{V}\right)\]</span>
## Experiment</li>
</ul></li>
</ul></li>
<li><p>采用EmoBERTa-base作为特征提取器，以获取Seeker的情感得分和情感表征，情感得分由softmax函数使用EmoBERTa-base的[CLS]表示获得
## Experimental Results ## Analyses ## conclusion # 关注的问题</p></li>
</ul>
<ol type="1">
<li><p>预测策略只依靠对话历史，而不考虑求助者反馈，导致预测的结果与用户无关</p></li>
<li><p>建模过程只关注上下文到策略，而不关注策略到上下文和与策略相关的上下文
# 解决方法</p></li>
<li><p>双层反馈策略选择器：利用回合级和会话级反馈信息来激励或惩罚策略</p></li>
<li><p>双层控制阅读器：策略到上下文流来生成策略约束响应 # 创新点 /
本文的优势</p></li>
</ol>
<h1 id="实验结论">实验结论</h1>
<h1 id="有待提升的部分">有待提升的部分</h1>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
      </tags>
  </entry>
  <entry>
    <title>HEAL</title>
    <url>/2023/HEAL_%20A%20Knowledge%20Graph%20for%20Distress%20Management%20Conversations/</url>
    <content><![CDATA[<p><strong>Title:</strong> A Knowledge Graph for Distress Management
Conversations</p>
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<h2 id="abstract">Abstract</h2>
<ul>
<li><p>相比于seq2seq的不确定性，聊天机器人利用知识图谱进行推理，被视为端到端模型的高效、万无一失的解决方案</p></li>
<li><p>提出HEAL知识图谱</p>
<ul>
<li>基于1M痛苦叙述及其相应的安慰回应而开发的知识图谱</li>
<li>图谱可视化：表现了对话双方的情绪动态和帮助缓解情绪的有效方法</li>
<li>组成部分
<ul>
<li>22k Node 节点：识别不同类型的stressors, speaker expectations,
responses, feedback types</li>
<li>104k Edge 连接：不同类型的节点之间的关系</li>
<li>每个节点和41种情绪状态相关联 ## Introduction</li>
</ul></li>
</ul></li>
<li><p>神经网络架构模型缺乏可控性和黑箱性质，导致其并不可靠</p></li>
<li><p>使用常识推理和知识图结构表示，可以生成适合的、可预测的、多策略的回应</p></li>
<li><p>相关工作</p>
<ul>
<li>ConceptNet、ATOMIC主要是通过捕获事实知识，在开放对话中嵌入常识推理辅助对话，不适用于移情对话</li>
</ul></li>
<li><p>本文，通过子Reddit精心选择的对压力事件叙述和回应，生成了一个压力对话管理知识图谱HEAL</p>
<ul>
<li>五类节点：压力源、期望、回应类型、反馈类型、情感状态</li>
<li>可以准确描述以痛苦为导向对话的潜在背景，使对话模型可以检索到更具体的上下文响应。提取响应会导致的反馈类型和是否能达到期望等信息，从而产生更为合适的反应
## Related Work</li>
</ul></li>
<li><p>知识图谱可以帮助NLP理解用户的输入，拓展用户输入中的事实和常识性知识</p></li>
<li><p>目前工作关注于知识感知和推理对话，不会捕捉情感推理和移情反应 ##
Methodology</p></li>
<li><p>数据集管理</p>
<ul>
<li>采用reddit数据集，通过Pushshift
API，收集和处理8个子reddit对话主题：mentalhealthsupport、offmychest、sad、anxietyhelp、depression、suicidewatch、depressed、depressionhelp</li>
<li>数据预处理</li>
</ul></li>
<li><p>概要</p>
<ul>
<li>针对过长而超出预训练语言模型输入上限的对话，本文采用SMMRY摘要算法保留叙事本质</li>
</ul></li>
<li><p>凝聚聚类</p>
<ul>
<li>自动聚类：区分对话中的压力源、期望、响应和反馈类型</li>
<li>凝聚聚类法：递归地合并增加最小链接距离的簇对</li>
<li>链接距离：对SentenceBERT生成的embedding使用余弦相似度计算</li>
</ul></li>
<li><p>定义压力源</p>
<ul>
<li>每个阈值计算了各种聚类质量指标，结果显示以0.85的相似度阈值区分压力源最合适，将压力源中的4.7%分为了4363个类。</li>
<li>将聚类结果按照TF-IDF建模，可以明显区分压力源，表明聚类结果的可靠性。</li>
</ul></li>
<li><p>期望、回复、反馈类型</p>
<ul>
<li>提取带有❓的句子作为问题，以此问题提取相关的响应和反馈。使用NLTK分离响应和反馈中的单个对话，方便后续对其进行单一种类的聚类。</li>
<li>聚类方法与压力源一致，每个集群至少有两个不同的集群元素。</li>
</ul></li>
<li><p>情感状态建模</p>
<ul>
<li>使用Pu提出的基于BERT的情感分类器，将每一个簇与某一情感状态相关联，共有41种情感状态。先将每一个簇下的每个文本进行分类，再按照情感出现次数和分类置信度排序，选取最相关的情感状态。
## Statistical Analysis</li>
</ul></li>
<li><p>HEAL知识图谱：2.2k集群节点和情感状态，104k连接</p></li>
<li><p>反馈集群中，负面情绪明显减少，证明HEAL中存在帮助人们降低负面情感状态的有用响应
## Visualization and Interpretation</p></li>
<li><p>表示大多数回答都是正向积极反馈 ## Evaluating the Utility of HEAL
in Responding to Distress Prompts</p></li>
<li><p>获取共情响应</p>
<ul>
<li>从测试集中选取和压力源中现有叙述相似性高于0.75的新对话</li>
<li>根据压力源和响应之间边权的权重、响应簇大小进行排序，选择排名高的响应</li>
</ul></li>
<li><p>自动评估</p>
<ul>
<li>HEAL的响应更加多样化，可以根据给定的情况给出特定的响应</li>
<li>在其他自动指标BLEU、METEOR和ROUGE方面表现不佳</li>
</ul></li>
<li><p>人类评估【DOI: 10.18653/v1/d16-1230】</p>
<ul>
<li>HEAL模型变现更好 ## Discussion and Conclusion</li>
</ul></li>
<li><p>HEAL：利用Reddit上约1M个与痛苦相关的对话得出的知识图谱。在不同类型的压力源、说话者期望、求助者反应和求助者反馈类型之间形成联系，同时将每个节点与41种情感状态中的一种联系起来</p></li>
<li></li>
</ul>
<h1 id="关注的问题-本文的优势">关注的问题 / 本文的优势</h1>
<ul>
<li><p>端到端对话经常会产生通用和重复性对话，缺乏可控性。使用常识推理和知识图结构表示，可以生成适合的、可预测的、多策略的回应。</p></li>
<li><p>目前的知识图谱不适用于移情对话。移情领域缺乏数据集和模型帮助产生移情反应，还缺乏具有上下文-相应之间关系的知识图谱。
# 解决方法 / 创新点</p></li>
<li><p>开发大规模知识图谱HEAL，识别不同的压力源、期望、响应、反馈和该对话的情感状态</p></li>
<li><p>统计和可视化分析，识别导致情绪强度降低的有利反应</p></li>
<li><p>评估通过HEAL检索到的回应，在解决情感困扰问题上的共情性、多样性、可靠性
# 实验结论</p></li>
<li><p>与RoBERTa、Blender进行比较，HEAL能够产生更多样化、更移情的反应</p></li>
<li><p>统计和可视化分析证实了在HEAL中存在有用的反应策略，这些策略降低了遭受痛苦的人的负面情感状态</p></li>
<li><p>使用纯生成模型来解决痛苦存在危险，HEAL通过战略性地识别与给定提示相关的特定压力源来避免不适当的反应
# 有待提升的部分</p></li>
<li><p>只使用了压力源与回复的边缘权重，进一步可以通过将边缘权重与说话者的期望和反馈结合起来来开发</p></li>
<li><p>知识图的信息可以用于增强神经反应生成模型，并为这些模型引入更多的可控性和可解释性，从而提高可靠性</p></li>
<li><p>仅限于识别≈4K的压力源，可以从网络上获取更多数据来增强知识图谱，这将帮助其能够处理更大范围的压力源和期望</p></li>
</ul>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>MISC</title>
    <url>/2023/MISC_%20A%20MIxed%20Strategy-Aware%20Model%20Integrating%20COMET%20for%20Emotional%20Support%20Conversation/</url>
    <content><![CDATA[<p><strong>Title:</strong> A MIxed Strategy-Aware Model Integrating
COMET for Emotional Support Conversation</p>
<img data-src="/2023/MISC_%20A%20MIxed%20Strategy-Aware%20Model%20Integrating%20COMET%20for%20Emotional%20Support%20Conversation/1699523191286-4f020d4d-6fee-4fc2-b943-e0443b1ca667.png" class="" title="image.png">
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<h2 id="abstract">Abstract</h2>
<ul>
<li><p>先前工作的局限性</p>
<ul>
<li>采用对话级别的情感标签，这种标签过于粗粒度，无法捕捉用户的即时精神状态</li>
<li>大多侧重于在回应中表达同理心，而不是逐渐减轻用户的痛苦</li>
</ul></li>
<li><p>本文提出了MISC</p>
<ul>
<li>首先推断用户的细粒度情感状态，然后使用混合策略巧妙地响应</li>
<li>在基准数据集上的实验结果证明了方法的有效性，并揭示了细粒度情感理解和混合策略建模的好处
## Introduction</li>
</ul></li>
<li><p>目前的工作不适用于ESC</p>
<ul>
<li>粗粒度静态的对话级情感标签无法关注到对话过程中用户情感发生的变化</li>
<li>只有移情反应，而不考虑解决求助者的情感问题</li>
</ul></li>
<li><p>本文提出的解决方法</p>
<ul>
<li>有选择地采用COMET（预训练生成式常识推理模型）生成的知识元组进行细粒度情感理解</li>
<li>混合策略，而不是预测单一策略</li>
<li>设计一套注意力机制</li>
</ul></li>
<li><p>实验分析</p>
<ul>
<li>回答策略建模的重要性，能够提高模型的共情能力 ## Related Work</li>
</ul></li>
<li><p>情绪感知响应生成</p></li>
<li><p>NLP中的常识性知识</p></li>
<li><p>策略感知对话模型 ## Preliminaries</p></li>
<li><p>ESConv数据集</p></li>
<li><p>问题设定</p>
<ul>
<li>通过对话历史、场景、求助者最后一句话，预测回复策略和回复内容 ##
Model: MISC</li>
</ul></li>
<li><p>情感状态增强Encoder</p>
<ul>
<li>利用COMET提取场景situation和语句x的常识性知识，将常识性知识输入Encoder得到H's和H'x</li>
</ul></li>
</ul>
<p><span
class="math display">\[\boldsymbol{B}^{s}=\bigcup_{j=1}^{N_{r}}\operatorname{COMET}\left(\mathrm{rel}_{j},\boldsymbol{s}\right)\]</span></p>
<ul>
<li>将H's和H'x分别与历史对话c做cross-attention，得到Hs和Hx</li>
<li>将历史对话c输入Encoder得到C</li>
<li>混合策略学习模块【从VQ-VAE's codebook文章中抄来的】
<ul>
<li>C输入多层感知机+softmax得到Pg（Pg大小是strategy_size）</li>
<li>Pg作为概率分布，hg=Pg*T（T为策略嵌入）
<ul>
<li>长对话回复中可以引入多种策略，且模型学习灵活方便</li>
</ul></li>
</ul></li>
<li>多因素感知Decoder
<ul>
<li>将情绪状态和策略表征传入Decoder里的Cross-attention ##
Experiments</li>
</ul></li>
<li>ESConv中的每十个话语作为一个样本</li>
<li>评价指标
<ul>
<li>策略预测精度：Acc</li>
<li>传统NLP指标：PPL、BLEU、ROUGE-L</li>
<li>相应多样性：Distinct</li>
<li>人类评估</li>
</ul></li>
<li>基准模型
<ul>
<li>MT Transformer、MoEL、MIME、BlenderBot-Joint</li>
</ul></li>
<li>具体实现</li>
<li>实验结果
<ul>
<li>动态细粒度情感标签更能准确给予用户回应</li>
<li>细粒度共情，多策略平稳过渡策略，可以更自然地表达共情并提供帮助</li>
<li>策略作为单独的任务进行预测比单一预测更有利</li>
<li>MISC的知识Know得分最高，成功学习到了COMET中的心理状态知识 ##
Analysis</li>
</ul></li>
<li>消融实验</li>
<li>案例研究</li>
<li>细粒度情感理解
<ul>
<li>用粗粒度的情感标签代替细粒度的心理信息可以显著提高指标</li>
</ul></li>
<li>混合策略感知移情反应
<ul>
<li>混合策略有利于平滑的情感支持</li>
<li>混合策略比单一策略更有效</li>
<li>混合策略适用于ESC框架 ## Conclusions</li>
</ul></li>
<li>引入COMET来捕捉用户的即时心理状态</li>
<li>设计了一个混合策略感知解码器来产生支持响应 # 解决了什么问题 /
怎么解决的</li>
</ul>
<h1 id="该方法的优势">该方法的优势</h1>
<ul>
<li><p>长对话中的过度更加顺畅 # 有什么创新点</p></li>
<li><p>提出seq2seq模型MISC，在ESC中添加了常识性知识和混合反应策略</p></li>
<li><p>提出了不同的策略模型并在对话中给予提示 #
实验结果好在哪里，怎么证明的</p></li>
<li><p>从SOTA延续下来的细粒度情感表现较粗粒度静态情感更好 #
相关工作分析</p></li>
</ul>
<h1 id="可以提升的地方">可以提升的地方</h1>
<ul>
<li>以动态的方式学习混合响应策略</li>
</ul>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
      </tags>
  </entry>
  <entry>
    <title>KEMI</title>
    <url>/2023/Knowledge-enhanced%20Mixed-initiative%20Dialogue%20System%20for%20Emotional%20Support%20Conversations/</url>
    <content><![CDATA[<p><strong>Title:</strong> Knowledge-enhanced Mixed-initiative Dialogue
System for Emotional Support Conversations</p>
<img data-src="/2023/Knowledge-enhanced%20Mixed-initiative%20Dialogue%20System%20for%20Emotional%20Support%20Conversations/1699406609845-c98dd4cb-e2ae-41f2-99b8-aed67482d60d.png" class="" title="image.png">
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<h2 id="abstract">Abstract</h2>
<ul>
<li><p>混和主动性：按照说话者角色和主动类型分为四类</p></li>
<li><p>提出四个情绪支持指标</p></li>
<li><p>提出一种用于 ESC 的知识增强混合主动框架 (KEMI) ##
Introduction</p></li>
<li><p>ESC系统</p>
<ul>
<li>在适当的时候发起讨论，目的是提出建议，并解决问题</li>
</ul></li>
<li><p>相关工作</p>
<ul>
<li>CIS(conversational
information-seeking)可以主动发起对话，澄清交互并探索更多的信息</li>
<li>情感推理用来生成共情反应</li>
<li>identifying the dialogue acts of the utterances</li>
<li>ESC 系统预测下一个对话策略</li>
</ul></li>
<li><p>ESC问题的三个挑战</p>
<ul>
<li>系统应该在对话过程中何时采取主动？</li>
<li>系统发起子对话需要什么样的信息？</li>
<li>系统如何促进混合主动交互？</li>
</ul></li>
<li><p>解决方法</p>
<ul>
<li>策略预测：确定下一回合混合主动策略</li>
<li>知识选择：收集下一回合的必要知识</li>
<li>响应生成：在适当的混合主动策略和知识下产生情感支持响应</li>
</ul></li>
<li><p>提出的新东西</p>
<ul>
<li>混合主动性特征
<ul>
<li>EAFR模式：话语注释为不同类型的说话者角色和主动类型</li>
<li>Expression, Action, Reflection, Feedback</li>
</ul></li>
<li>情感支持指标
<ul>
<li>Proactivity, Information, Repetition, Relaxation</li>
</ul></li>
<li>KEMI
<ul>
<li>使用生成的常识知识作为查询图来扩展用户话语，在知识图谱上执行子图检索</li>
<li>响应生成模块以序列到序列的方式，对策略预测和响应生成进行【多任务学习】，以生成具有外部知识的混合主动响应</li>
</ul></li>
</ul></li>
<li><p>主要贡献</p>
<ul>
<li>EAFR 注释模式和四种情感支持指标</li>
<li>使用用常识知识扩展的查询图，通过子图检索从心理健康知识图谱中检索外部知识
## Related Works</li>
</ul></li>
<li><p>ESC</p>
<ul>
<li>检测用户情绪</li>
<li>将情感信号放入Respond中</li>
<li>情绪感知反应，情感风格转移</li>
<li>共情对话系统
<ul>
<li>情感推理技巧</li>
<li>利用外部知识来提高情绪推理的建模能力：知识图、常识性模型、特定领域知识、常识性知识</li>
</ul></li>
</ul></li>
<li><p>混合主动对话 ## Preliminary Analysis</p></li>
<li><p>EAFR</p>
<ul>
<li>四种注释方法：按照角色和主动类型区分</li>
<li>四种评价指标：【待看】</li>
</ul></li>
<li><p>混合主动性分析</p></li>
<li><p>混和主动性的挑战</p>
<ul>
<li>系统何时采取主动？</li>
<li>系统发起子对话时，需要什么信息？
<ul>
<li>情感识别：识别用户情感状态</li>
<li>因果识别：导致情感状态的压力源</li>
<li>认知识别：解决问题的过程</li>
</ul></li>
<li>根据历史对话，系统自发判断响应的主动或被动</li>
</ul></li>
<li><p>问题定义</p>
<ul>
<li>给定历史对话C和用户情况s，产生相应r
<ul>
<li>策略预测y，细粒度主动性</li>
<li>知识选择k</li>
<li>使用y和k生成混合主动相应r ## Method</li>
</ul></li>
</ul></li>
<li><p>知识获取</p>
<ul>
<li>检索心理健康知识图谱HEAL，弥补常识性知识的不足
<ul>
<li>COMET常识性知识扩展查询：Cp = COMET(p, ut)</li>
<li>构造查询图：û = {ut, {Cp}}
<ul>
<li>expectation：ut</li>
<li>affective：[xReact]</li>
<li>stressor：[xIntent]</li>
<li>responses：[xWant] [xNeed] [xEffect]</li>
</ul></li>
<li>子图检索
<ul>
<li>相似度计算：sentence-BERT</li>
<li>针对每个 ût
中的抽象描述，获取与其最相似的top-K个HEAL中的实体，基于HEAL中的边缘连接，构成候选子图</li>
<li>针对每个 E
中的类型节点，按照子图中的每个节点相似度得分之和排序子图，选择top-N作为检索到的知识K</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>混合主动响应生成</p>
<ul>
<li>使用基于上下文的编码器，编码上下文对话C和知识K</li>
<li>X：[CLS]<context>[know.]&lt;know.&gt;；Y：[strategy]y[response]r</li>
<li>优化损失函数，最大化负对数似然函数L ## Experiment</li>
</ul></li>
<li><p>实验基础</p>
<ul>
<li>数据集：ESConv、MI</li>
<li>评估参数：流利度Perplexity (PPL)、内容保留度BLEU-n
(B-n)、内容保留度ROUGE-L (R-L)</li>
</ul></li>
<li><p>总体表现</p>
<ul>
<li>BlenderBot优于Transformer</li>
<li>GLHG、MISI有效地利用了常识性知识</li>
<li>基于策略的联合学习可以提高性能</li>
<li>KEMI明显优于其他方法：HEAL知识相比常识性知识更有效地支持了预测策略，减轻了对预训练大模型的依赖</li>
</ul></li>
<li><p>人工评价</p>
<ul>
<li>从流畅度、辨识度、舒适度、建议有效性、总体回应五个方面与BlenderBot-joint、MISC进行对比，MISC完胜</li>
</ul></li>
<li><p>消融实验</p>
<ul>
<li>HEAL可以有效提升策略预测准确度</li>
<li>舍弃COMET，可以提高ppl准确度，因为常识性知识不是自然语句；但会降低内容保留度，因为简洁的常识性知识更为精确</li>
<li>认知识别是最有效的</li>
<li>对比Oracle，还有很大的提升空间</li>
</ul></li>
<li><p>混和主动性分析</p>
<ul>
<li>情感支持指标分析
<ul>
<li>KEMI有效的平衡了主动性和非主动性回复</li>
<li>KEMI回复的信息更加丰富</li>
<li>KEMI容易生成重复性回复</li>
<li>KEMI有效地帮助用户解决情绪问题</li>
</ul></li>
<li>会话进度
<ul>
<li>相比BlenderBot和MISC，KEMI在对话过程中，主动性和非主动性的分布更加平衡</li>
<li>KEMI在对话中后期更能主动相应并缓解用户负面情绪</li>
</ul></li>
</ul></li>
<li><p>案例分析 ## Conclusions</p></li>
<li><p>首次提出ESC中混和主动性的特点，并阐述其重要性</p></li>
<li><p>KEMI框架</p>
<ul>
<li>通过查询扩展和子图检索，从大规模心理健康图谱中检索实际病例知识</li>
<li>通过检索到的知识，进行策略预测和响应生成的多任务学习</li>
</ul></li>
<li><p>结合实际案例和分析指标，结果证明KEMI优于现有方法，并在混合主动交互方面具有优势
## Limitations</p></li>
<li><p>评价指标有待改进</p></li>
<li><p>没有考虑不同知识检索方法的不同</p></li>
<li><p>从复杂的KG图中检索知识的方法有待提高</p></li>
<li><p>某些应用的知识图难以获取</p></li>
<li><p>知识库的建立需要具有专业知识的人员 ## Appendix</p></li>
<li><p>混和主动性</p>
<ul>
<li>对话主动性分析
<ul>
<li>主动性分析模型：在ESConv数据集增加混合主动的类型属性，把历史对话和当前话语作为二元输入，判断主动性。微调RoBERTTalarge，分别训练两个模型，判断用户和系统的主动性</li>
<li>情绪强度预测模型：根据用户话语预测负面情绪强度</li>
<li>用户模拟并注释四种评价指标 ：【待看】</li>
</ul></li>
<li>对话流分析
<ul>
<li>ESC在对话中充当主动角色；ED在对话中充当被动角色</li>
</ul></li>
<li>对话过程
<ul>
<li>研究内容：主动性和情绪强度变化的关系</li>
<li>结论
<ul>
<li>交互时间很重要</li>
<li>情绪缓解后才更有利于解决问题</li>
</ul></li>
</ul></li>
<li>ES指标</li>
</ul></li>
<li><p>COMET</p>
<ul>
<li>常识性关系</li>
</ul></li>
<li><p>HEAL</p>
<ul>
<li>情绪压力和安慰回应之间的知识图谱</li>
<li>表现了对话双方的情绪动态，确定缓解情绪的方法 # 解决了什么问题</li>
</ul></li>
<li><p>常识性知识是相当抽象的，没有详细的信息，因此它对ESC系统产生有意义和信息丰富的响应的帮助不大。在这项工作中，我们采用<strong>生成式常识模型</strong>进行查询扩展，从外部知识图中检索实际案例知识。
# 怎么解决的 / 该方法的优势</p></li>
</ul>
<ol type="1">
<li><p>常识知识生成器 COMET：</p></li>
<li><p>常识性知识获取 HEAL： # 有什么创新点</p></li>
<li><p>提出ESC知识增强混合主动框架</p>
<ol type="1">
<li>人类和系统都可以主动引导交互方向</li>
<li>通过子图检索从心理健康知识图谱中检索外部知识</li>
</ol></li>
<li><p>新的分析方法 &gt; 保留评估 和 混合主动性分析
方面均有效优于现有方法</p>
<ol type="1">
<li>按照说话者角色和主动类型将话语注释为不同类型
<ol type="1">
<li>Expression：用户主动</li>
<li>Action：系统主动</li>
<li>Feedback：用户非主动</li>
<li>Reflection：系统非主动</li>
</ol></li>
<li>提出四个情感支持指标来衡量ESC中主动性和非主动性交互的特征
<ol type="1">
<li>Proactivity：系统主动的对话占系统对话的比例</li>
<li>Information：系统首次提出的频繁词占比</li>
<li>Repetition：系统重复用户提出的术语的频次占比</li>
<li>Relaxation：情绪强度的改善 # 实验结果好在哪里，怎么证明的</li>
</ol></li>
</ol></li>
</ol>
<h1 id="相关工作分析">相关工作分析</h1>
<h1 id="可以提升的地方">可以提升的地方</h1>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
      </tags>
  </entry>
  <entry>
    <title>BlenderBot</title>
    <url>/2023/Recipes%20for%20building%20an%20open-domain%20chatbot/</url>
    <content><![CDATA[<p><strong>Title:</strong> Recipes for building an open-domain
chatbot</p>
<blockquote>
<p>FaceBook 在本文中提出了 BlenderBot 编解码器模型</p>
</blockquote>
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<h2 id="abstract">Abstract</h2>
<ul>
<li><p>开放域聊天机器人需要良好的谈话技巧：提出有吸引力的要点，倾听别人的意见，适当展示知识、同理性和个性，保持前后一致</p></li>
<li><p>本文证实：基于适当的训练数据和生成策略，大模型可以学习上述技巧 ##
Introduction</p></li>
<li><p>研究的主要内容</p>
<ul>
<li>混合技巧：模型专注于个性和吸引力、知识、同理心。采用 Blended Skill
Talk
实现，具体是通过提供训练数据和初始化上下文来实现。这种方法可以减少从大语料库中学习到不良特征。</li>
<li>生成策略：解码算法非常重要，对话长度强相关于对话质量，本文实验表明采样优于束搜索</li>
</ul></li>
<li><p>本文的优势和存在的问题 ## Model architectures</p></li>
<li><p>检索</p>
<ul>
<li>将训练集作为候选响应集，每个可能的候选响应都参与Encoder，并做聚类【poly-encoder】</li>
</ul></li>
<li><p>生成</p>
<ul>
<li>使用标准的Seq2Seq Transformer架构生成响应</li>
</ul></li>
<li><p>检索和提炼</p>
<ul>
<li>帮助模型访问没有嵌入其模型参数的外部知识</li>
<li>对话检索：通过检索模型生成响应，并将其附加到Decoder的输入序列中，并用分隔符分开。使用修改后的输入序列生成正常的响应</li>
<li>知识检索：检索Wiki生成初始候选集，使用上文提到的检索模型，排序候选词并选择条件生成的句子。此外，训练一个分类器，通过上下文判断何时需要检索知识
## Training Objectives</li>
</ul></li>
<li><p>检索排序</p>
<ul>
<li>模型训练使用本文回答作为正例，其他对话回答作为负例</li>
</ul></li>
<li><p>响应生成模型的似然训练</p>
<ul>
<li>建模整个序列的概率分布</li>
</ul></li>
<li><p>α-混合检索和提炼</p>
<ul>
<li>生成模型常常不考虑对话检索语句，为确保其被使用，将检索相应中的α%替换成真实响应，α为超参数</li>
<li>知识检索的数据集里，知识条件和响应之间有明确的对应关系</li>
<li>因此训练阶段只使用知识检索的数据，充实模型参数中学到的知识</li>
</ul></li>
<li><p>响应生成模型的非似然损失</p>
<ul>
<li>使用高于真实数据数量的n元语法中的token作为候选负样本，修正已知的偏差
## Decoding 选择解码方法对给定的历史对话的响应</li>
</ul></li>
<li><p>确定性解码方法</p>
<ul>
<li>束搜索</li>
<li>贪心搜索</li>
</ul></li>
<li><p>采样</p>
<ul>
<li>多项采样：根据预测结果概率分布，定义候选Token被选中的概率。防止采样到低概率Token，可以将采样限制在词汇表的子集内，并根据子集概率重采样</li>
<li>平滑分布采样：Temparature+SoftMax+multinomial</li>
<li>top-k
sampling：Temparature+Top-k+SoftMax+multinomial，将top-k外的token概率置为0</li>
<li>sample-and-rank：多次采样，取最高概率的响应</li>
</ul></li>
<li><p>响应长度</p>
<ul>
<li>约束最小生成长度：在实现最小序列长度之前，强制不生成结束标记</li>
<li>预测长度：根据上下文预测相应长度，这是一个四分类问题</li>
</ul></li>
<li><p>子序列分块</p>
<ul>
<li>n-grams：考虑响应和输入上下文中对于n-grams的重复性 ## Training
Details</li>
</ul></li>
<li><p>预训练排序模型</p></li>
<li><p>预训练生成模型</p></li>
<li><p>微调</p>
<ul>
<li>Fairseq-style混合精度训练</li>
</ul></li>
</ul>
<p>Training Data Safety Characteristics Evaluation Methods Related Work
Results &amp; Analysis Released code and models Discussion # 关注的问题
/ 本文的优势</p>
<ul>
<li>成对比较和人性方面优于Meena # 解决方法 / 创新点</li>
</ul>
<h1 id="实验结论">实验结论</h1>
<h1 id="有待提升的部分">有待提升的部分</h1>
<ul>
<li>如果对话中深入询问某一方面，由于缺乏知识模型没法给出详细的回答</li>
<li>模型倾向于简单的回答</li>
<li>模型倾向于产生重复易混淆的句子</li>
</ul>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
      </tags>
  </entry>
  <entry>
    <title>ESConv</title>
    <url>/2023/Towards%20Emotional%20Support%20Dialog%20Systems/</url>
    <content><![CDATA[<p><strong>Title:</strong> Towards Emotional Support Dialog Systems</p>
<span id="more"></span>
<h1 id="论文速览">论文速览</h1>
<ul>
<li>Abstract
<ul>
<li>本文关注于情感支持对话，提供了一个带策略的情绪支持数据集</li>
</ul></li>
<li>Introduction
<ul>
<li>情感支持应用广泛</li>
<li>情绪支持数据集应该包括exploration、understand、suggestion三个过程，还包括有用户的情绪强度变化</li>
<li>目前的研究模型中没有具体有效的情绪支持方法；采集数据集时需要训练有素的支持者</li>
<li>ESC框架：三个阶段，多种策略；众包做数据集；在情绪支持方面有提升</li>
</ul></li>
<li>Related Work
<ul>
<li>移情、同感对话</li>
<li>情绪支持的数据集</li>
<li>情绪支持对话
<ul>
<li>任务定义</li>
<li>ESC框架
<ul>
<li>三个阶段：探索、理解、建议</li>
<li>八种策略</li>
</ul></li>
</ul></li>
</ul></li>
<li>Data Collection
<ul>
<li>获取数据集的方法</li>
</ul></li>
<li>Data Characteristics
<ul>
<li>数据展示</li>
</ul></li>
<li>Experiments
<ul>
<li>基准模型：BlenderBot、DialoGPT</li>
<li>模型变种：不带策略：Vanilla；带策略：Random、Joint、Oracle</li>
<li>评估指标：PPL、B-2、R-L、Extrema</li>
<li>实验结果：
<ul>
<li>带策略的Oracle优于不带策略的Vanilla</li>
<li>带策略的Oracle稍微弱于不带策略的Vanilla，因为一旦策略预测错误，respond会完全不同</li>
<li>BlenderBot 变体始终比 DialoGPT 变体表现更好</li>
</ul></li>
<li>人类互动评价结果：
<ul>
<li>微调后ES能力提升</li>
<li>带策略微调后能更加适合用户需求</li>
<li>正确的策略更重要</li>
</ul></li>
</ul></li>
<li>Conclusion
<ul>
<li>三个阶段、多种策略、ESConv</li>
<li>道德评估 # 解决了什么问题</li>
</ul></li>
</ul>
<ol type="1">
<li><p>目前的对话系统没有针对情感支持的任务和语料库，因此ESC方向还没有被探索到，对话场景中缺乏情感支持。</p></li>
<li><p>情感支持（ES）旨在减少个人的情绪困扰，帮助他们理解和应对所面临的挑战。目前的模型大都只能表示共情而不能解决问题。</p></li>
<li><p>通过社交而不是专业咨询的方式提供情感支持。 #
怎么解决的/该方法的优势</p></li>
<li><p>基于Hill的帮助技能理论，提出了ESC框架</p>
<ol type="1">
<li>情感支持过程
<ol type="1">
<li>理解遇到的困难，exploration</li>
<li>表达理解和同情，insight/comforting</li>
<li>提出解决问题的方法，action</li>
</ol></li>
</ol></li>
<li><p>构建ESConv数据集，有丰富的注释（支持策略）</p>
<ol type="1">
<li>每个聊天会都会标记：
<ol type="1">
<li>问题类别</li>
<li>用户情绪类别</li>
<li>用户情绪强度</li>
<li>遇到困难的简介</li>
</ol></li>
</ol></li>
<li><p>ESC系统</p>
<ol type="1">
<li>选择支持策略，生成策略约束Respond</li>
<li>情绪状态建模，跟踪用户情绪变化【待分析】</li>
<li>提出了评估ES实验效果的新方法 # 有什么创新点</li>
</ol></li>
</ol>
<h1 id="实验结果好在哪里怎么证明的">实验结果好在哪里，怎么证明的</h1>
<h1 id="相关工作分析">相关工作分析</h1>
<ol type="1">
<li>表达情感
<ol type="1">
<li>ECM：情感对话，可以生成情感反应。有情商而不只是智商</li>
<li>ES：有情绪支持的能力，可以减少用户情绪困扰</li>
</ol></li>
<li>共情能力
<ol type="1">
<li>反应出同情心，不能做情绪支持</li>
</ol></li>
<li>数据集
<ol type="1">
<li>数据集中的对话短，不适合ES任务</li>
</ol></li>
<li>ESC系统
<ol type="1">
<li>大多研究都人为预先规定好的规则和回复词典 # 可以提升的地方</li>
</ol></li>
</ol>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>情绪支持对话</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker</title>
    <url>/2024/Docker/</url>
    <content><![CDATA[<img data-src="/2024/Docker/576507-docker1.png" class="" title="img">
<ul>
<li><a href="https://www.runoob.com/docker/docker-compose.html">Docker
Compose</a>：用于定义和运行<strong>多容器 Docker
应用程序</strong>的工具，使多个容器可以在隔离环境中一起运行；</li>
<li><a
href="https://www.runoob.com/docker/docker-dockerfile.html">Dockerfile</a>：用来构建镜像的文本文件，包含了一条条<strong>构建镜像</strong>所需的指令和说明；</li>
<li><a href="https://www.runoob.com/docker/docker-machine.html">Docker
Machine</a>：是一个简化 Docker
安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker；可以集中<strong>管理</strong>所有的
docker 主机，比如快速的给 100 台服务器安装上 docker；</li>
</ul>
<h1 id="常用命令">常用命令</h1>
<p>docker run &lt;IMAGE&gt; [COMMAND]
[ARG...]【创建一个新的容器并运行一个命令】</p>
<ul>
<li>[-d] 后台运行容器</li>
<li>[-p] 指定端口映射</li>
<li>[-v] 映射主机文件卷</li>
<li>[--name &lt;NAME&gt;] 命名容器</li>
<li>[-e] 设置环境变量</li>
</ul>
<p>docker ps [OPTIONS]【列出容器】</p>
<ul>
<li>[-a] 显示所有的容器，包括未运行的</li>
</ul>
<p>docker rm [OPTIONS] &lt;CONTAINER &gt;
【删除一个或多个已经停止的容器】</p>
<ul>
<li>[-f] 强制删除正在运行的容器</li>
<li>[-v] 删除容器挂载的卷</li>
</ul>
<p>docker build [OPTIONS] &lt;PATH&gt; 【创建镜像】</p>
<ul>
<li>[-f] 指定要使用的 Dockerfile 路径</li>
<li>[--rm] 设置镜像成功后删除中间容器</li>
</ul>
<p>docker save [OPTIONS] &lt;IMAGE&gt; 【将指定镜像保存成 tar
归档文件】</p>
<ul>
<li>[-o &lt;xxx.tar&gt;]</li>
</ul>
<p>docker load [OPTIONS] 【导入镜像】</p>
<ul>
<li>[-i &lt;xxx.tar&gt;] 指定导入的文件</li>
</ul>
<p>docker top [OPTIONS] &lt;CONTAINER&gt;
【查看容器中运行的进程信息，支持 ps 命令参数】</p>
<h1 id="实践">实践</h1>
<h2 id="如何容器化-java-应用">如何容器化 Java 应用</h2>
<p>Docker Compose：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3&#x27;</span>  </span><br><span class="line"><span class="attr">services:</span>  </span><br><span class="line">  <span class="attr">db:</span>  </span><br><span class="line">    <span class="attr">image:</span> <span class="string">mysql:5.7</span>  </span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span>  </span><br><span class="line">    <span class="attr">environment:</span>  </span><br><span class="line">      <span class="attr">MYSQL_ROOT_PASSWORD:</span> <span class="string">example</span>  </span><br><span class="line">      <span class="attr">MYSQL_DATABASE:</span> <span class="string">mydb</span>  </span><br><span class="line">      <span class="attr">MYSQL_USER:</span> <span class="string">myuser</span>  </span><br><span class="line">      <span class="attr">MYSQL_PASSWORD:</span> <span class="string">mypassword</span>  </span><br><span class="line">    <span class="attr">ports:</span>  </span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;3608:3306&quot;</span>  <span class="comment"># 将宿主机的 3608 端口映射到容器的 3306 端口  </span></span><br><span class="line">  </span><br><span class="line">  <span class="attr">java-app:</span>  </span><br><span class="line">    <span class="attr">build:</span>  </span><br><span class="line">      <span class="attr">context:</span> <span class="string">.</span>  <span class="comment"># 假设 Dockerfile 在当前目录下  </span></span><br><span class="line">      <span class="attr">dockerfile:</span> <span class="string">Dockerfile</span>  </span><br><span class="line">    <span class="attr">depends_on:</span>  </span><br><span class="line">      <span class="bullet">-</span> <span class="string">db</span>  </span><br><span class="line">    <span class="attr">environment:</span>  </span><br><span class="line">      <span class="attr">DB_HOST:</span> <span class="string">db</span>  <span class="comment"># Java 程序将连接到名为 &quot;db&quot; 的服务（即 MySQL 容器）  </span></span><br><span class="line">      <span class="attr">DB_PORT:</span> <span class="number">3306</span>  <span class="comment"># 在容器内部，MySQL 仍然监听 3306 端口  </span></span><br><span class="line">      <span class="attr">DB_USER:</span> <span class="string">myuser</span>  </span><br><span class="line">      <span class="attr">DB_PASSWORD:</span> <span class="string">mypassword</span>  </span><br><span class="line">      <span class="attr">DB_NAME:</span> <span class="string">mydb</span>  </span><br><span class="line">    <span class="attr">ports:</span>  </span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8080:8080&quot;</span>  <span class="comment"># 假设 Java 程序监听 8080 端口</span></span><br></pre></td></tr></table></figure>
<p>Dockerfile：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 基础镜像Java 8</span></span><br><span class="line">FROM java:8</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置工作目录</span></span><br><span class="line">WORKDIR /app</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制文件到工作目录</span></span><br><span class="line">COPY . /app</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置Java环境变量</span></span><br><span class="line">ENV PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line">ENV JRE_HOME=<span class="variable">$&#123;JAVA_HOME&#125;</span>/jre</span><br><span class="line">ENV CLASSPATH=.:<span class="variable">$&#123;JAVA_HOME&#125;</span>/lib:<span class="variable">$&#123;JRE_HOME&#125;</span>/lib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暴露端口</span></span><br><span class="line">EXPOSE 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行</span></span><br><span class="line">ENTRYPOINT [<span class="string">&#x27;/usr/lib/jvm/java-9-openjdk-amd64/bin/javac&#x27;</span>,<span class="string">&#x27;-jar&#x27;</span>,<span class="string">&#x27;app.jar&#x27;</span>]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>容器虚拟化</tag>
      </tags>
  </entry>
  <entry>
    <title>Git 文档阅读</title>
    <url>/2024/Git/</url>
    <content><![CDATA[<p><a href="https://git-scm.com/book/zh/v2">Git 中文文档</a></p>
<blockquote>
<p>有空可以看看这个 <a href="https://github.com/xirong/my-git">Git
笔记汇总</a></p>
</blockquote>
<span id="more"></span>
<img data-src="/2024/Git/1352126739_7909.jpg" class="" title="img">
<h2 id="chapter-1-起步">Chapter 1 起步</h2>
<h3 id="版本控制">版本控制</h3>
<ol type="1">
<li>集中化版本控制（CVCS）缺点
<ol type="1">
<li>中央服务器的单点故障就无法协同工作</li>
<li>本地只有快照，项目整体和其变更历史只保存在服务器上</li>
</ol></li>
<li>分布式版本控制（DVCS）优点
<ol type="1">
<li>代码仓库完整地镜像下来，包括完整的历史记录</li>
<li>用户在本地保存了所有改动，每一次的克隆都是对仓库的完整备份</li>
<li>可以指定和若干不同的远端代码仓库进行交互</li>
</ol></li>
</ol>
<h3 id="git-简史">Git 简史</h3>
<ol type="1">
<li>并行开发</li>
<li>完全分布式</li>
<li>速度快</li>
<li>数据量大</li>
</ol>
<h3 id="git-简介">Git 简介</h3>
<ol type="1">
<li>大多版本控制工具都是基于差异的，存储文件随时间变化的差异，而Git储存项目快照或索引</li>
<li>Git 安全性优，只做增不做删；完整性好，使用SHA-1 哈希算法</li>
<li>文件的四种状态：已提交（committed）、已修改（modified）、已暂存（staged）、未跟踪（untracked）</li>
<li>项目的三个阶段：工作目录、暂存区域以及 Git 仓库</li>
<li>工作流程：修改文件、暂存文件、将暂存区文件快照永久存在Git目录</li>
</ol>
<h3 id="git-配置">Git 配置</h3>
<blockquote>
<p>git config --list --show-origin</p>
</blockquote>
<ol type="1">
<li>/etc/gitconfig 系统通用配置 git config --system</li>
<li>~/.gitconfig 当前用户配置 git config --global</li>
<li>.git/config 当前项目配置 git config --local</li>
</ol>
<h2 id="chapter-2-基础">Chapter 2 基础</h2>
<blockquote>
<p>Git命令只能控制当前文件夹和子文件夹内的修改</p>
</blockquote>
<h3 id="常用命令">常用命令</h3>
<p>git init 【初始化仓库】</p>
<p>git clone [OPTIONS] &lt;url&gt;【克隆仓库】</p>
<ul>
<li>[-b master]克隆指定分支</li>
</ul>
<p>git status [OPTIONS] 【查看文件状态 】</p>
<ul>
<li>[-s] 简要文件状态信息</li>
</ul>
<p>git add &lt;file&gt;
【跟踪新文件、已修改的文件放到暂存区、合并时把有冲突的文件标记为已解决状态
】</p>
<p>git diff [OPTIONS]
&lt;file&gt;【已修改文件与已暂存文件的内容差异】</p>
<ul>
<li>[--staged] 显示暂存区和上一次提交的差异</li>
</ul>
<p>git rm [OPTIONS] &lt;file&gt; 【删除文件】</p>
<ul>
<li>[-f] 强制移除存在未提交修改的文件</li>
<li>[ --cached] 文件保留在工作区中，从Git仓库和暂存区中删除</li>
</ul>
<p>git commit [OPTIONS] 【提交文件】 - [-m &lt;message&gt;] 提交备注 -
[--amend] 追加提交 - [-a] 直接将工作区和暂存区的所有修改一起提交</p>
<p>git mv &lt;oldfilename&gt; &lt;newfilename&gt; 【移动文件/改名】</p>
<p>git log [OPTIONS] 【回顾提交历史】 - [--stat] 显示每次提交的简要信息
- [-p] 显示每次提交的详细信息 - [--pretty=format:"xxx"] 自定义信息格式 -
[--oneline --graph --all]
显示提交历史、各个分支的指向以及项目的分支分叉情况 - [--since]
限制时间</p>
<p>git reset [OPTIONS]
&lt;HEAD&gt;【回退版本，可以指定退回某一次提交的版本，直接删除某些commit的内容】</p>
<ul>
<li>[--soft] 回退到某个版本</li>
<li>[--mixed]
默认，用于重置暂存区的文件与上一次的提交保持一致（取消暂存），工作区文件内容保持不变</li>
<li>[--hard]
撤销工作区中所有未提交的修改内容，将暂存区与工作区都回到上一次版本，并删除之前的所有信息提交</li>
</ul>
<p>git checkout
[OPTIONS]【在不同的分支之间切换、恢复文件、创建新分支，git switch 和 git
restore 可以代替它】</p>
<ul>
<li>[.] 放弃所有工作区的修改</li>
<li>[-f] 放弃工作区和暂存区的所有修改</li>
<li>[&lt;branch-name&gt;] 从当前分支切换到指定的分支</li>
<li>[-b &lt;local_new_branch_name&gt; &lt;remote_new_branch_name&gt;]
将远程仓库里指定的分支拉取到本地，并在本地创建一个分支与指定远程仓库分支关联起来,并切换到新建的本地分支中</li>
</ul>
<p>git remote [OPTIONS]
【列出当前仓库中已配置的<strong>远程仓库</strong>】</p>
<ul>
<li>[-v] 列出当前仓库中已配置的远程仓库，并显示它们的 URL</li>
<li>[add &lt;remote_name&gt; &lt;remote_url&gt;]
添加远程仓库，指定一个远程仓库的名称和 URL，将其添加到当前仓库中</li>
<li>[remane &lt;old_name&gt; &lt;new_name&gt;]
将已配置的远程仓库重命名</li>
<li>[show &lt;remote_name&gt;] 显示指定远程仓库的详细信息，包括 URL
和跟踪分支</li>
<li>[remove &lt;remote_name&gt;] 从当前仓库中删除指定的远程仓库</li>
<li>[<remote_name> <new_url>] 修改指定远程仓库的 URL</li>
</ul>
<p>git fetch &lt;remote_name&gt; &lt;local_branch_name&gt;
【获取远程仓库的更新，但不自动合并到你的工作】</p>
<ul>
<li>[--all] 获取远程的所有分支</li>
</ul>
<p>git push [OPTIONS]【推送到远程分支】</p>
<ul>
<li><p>[&lt;remote_name&gt;
&lt;local_branch_name&gt;:&lt;remote_branch_name&gt;]</p></li>
<li><p>[&lt;remote_name&gt; --delete &lt;branch_name&gt;]
删除远程分支</p></li>
<li><p>[-u &lt;remote_name&gt; --all]
本地的仓库和远程仓库进行关联</p></li>
<li><p>[--set-upstream &lt;remote_repository&gt;
&lt;local_branch_name&gt;] 将本地分支与远程分支关联</p></li>
<li><p>[--force] 强制提交，会覆盖所有 behind 的提交</p></li>
</ul>
<p>git pull &lt;remote_name&gt; &lt;branch&gt;【<code>git fetch</code>
和 <code>git merge</code> 的组合，拉取到本地分支并合并】</p>
<p>git tag [OPTIONS]【列出已有的标签】</p>
<ul>
<li>[-l &lt;通配符&gt;] 查询某些对应标签</li>
<li>[&lt;version&gt;] 轻量标签（提交校验和存储到一个文件中）</li>
<li>[-a &lt;version&gt; -m &lt;comment&gt;]
创建附注标签（数据库中的一个完整对象， 可以被校验）</li>
<li>[-d] 删除标签</li>
</ul>
<p>git rebase &lt;cur_branch&gt; &lt;target_branch&gt;
【将提交到某一分支上的所有修改都移至另一分支上】</p>
<p>git revert
&lt;commit_id&gt;【生成一次新的commit冲抵原来的commit，不会修改原有的提交历史，而是通过添加新的提交来撤销更改】</p>
<p>git merge &lt;branch&gt; 【合并指定分支到当前分支】</p>
<p>git switch [OPTIONS] &lt;branch-name&gt; 【切换分支，与 git checkout
类似，但提供了更清晰的语义和错误检查】</p>
<ul>
<li>[-c] 创建一个新分支并切换到该分支</li>
</ul>
<p>git show
【对于提交，它显示日志消息和文本差异；对于标签，它显示标签消息和引用对象】</p>
<p>git restore [OPTIONS] 【恢复或撤销文件的更改】 - [.]
还原所有未提交的更改 - [&lt;filename&gt;] 丢弃工作区修改 - [--staged
&lt;filename&gt;] 将暂存区的修改放回工作区 - [--source=&lt;commit&gt;
&lt;file&gt;] 将文件恢复到特定提交的状态</p>
<p>git branch [OPTIONS] 【查看本地分支】</p>
<ul>
<li>[&lt;branch_name&gt;] 创建本地分支</li>
<li>[-b &lt;branch_name&gt;] 创建本地分支并切换</li>
<li>[-v] 每个分支的最后一次提交</li>
<li>[-vv] 跟踪信息</li>
<li>[-r] 查看远程分支</li>
<li>[-D &lt;branch_name&gt;] 删除分支</li>
<li>[--set-upstream-to &lt;remotename&gt;/&lt;remote_branch&gt;
&lt;local_branch&gt;]
跟踪远程分支。在本地新建一个分支后，需要做远程分支关联，目的是在执行git
pull, git push操作时就不需要指定对应的远程分支</li>
<li>[--merged --no-merged] 合并/未合并到当前分支的其他分支</li>
<li>[-m &lt;old_branch_name&gt; &lt;new_branch_name&gt;] 改名</li>
</ul>
<p>git stash [OPTIONS] 【贮藏工作】</p>
<ul>
<li>[list] 列出所有贮藏</li>
<li>[apply] 恢复贮藏</li>
</ul>
<h3 id="忽略不跟踪文件">忽略/不跟踪文件</h3>
<p>.gitignore 正则表达式：<a
href="https://github.com/github/gitignore">官方样例文件</a></p>
<h2 id="chapter-3-分支">Chapter 3 分支</h2>
<blockquote>
<p>相关命令合并到 Chapter2 中</p>
</blockquote>
<ol type="1">
<li>Git 中 HEAD是一个指针，指向当前工作位置</li>
<li>git mergetool 图形化冲突解决工具</li>
<li>有 Git 特性演化出的开发工作流：长期分支、主题分支</li>
</ol>
<h3 id="变基">变基</h3>
<p>如果提交存在于你的仓库之外，而别人可能基于这些提交进行开发，那么不要执行变基。只对尚未推送或分享给别人的本地修改执行变基操作清理历史，
从不对已推送至别处的提交执行变基操作。</p>
<p>工作原理：提取&lt;topicbranch&gt;内的修改并存为临时文件，然后将HEAD指向&lt;basebranch&gt;，最后以此将之前另存为临时文件的修改依序应用</p>
<h2 id="chapter-6-github">Chapter 6 GitHub</h2>
]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>版本管理</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 使用指南</title>
    <url>/2024/Hexo/</url>
    <content><![CDATA[<blockquote>
<p><a href="https://hexo.io/docs/">官方文档</a></p>
</blockquote>
<h1 id="常用命令">常用命令</h1>
<table>
<thead>
<tr class="header">
<th>方法</th>
<th>代码</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>快速部署</td>
<td>hexo g -d</td>
</tr>
<tr class="even">
<td>清除缓存</td>
<td>hexo clean</td>
</tr>
<tr class="odd">
<td>预览</td>
<td>hexo s</td>
</tr>
<tr class="even">
<td>部署</td>
<td>hexo d</td>
</tr>
<tr class="odd">
<td>生成静态页面</td>
<td>hexo generate</td>
</tr>
</tbody>
</table>
<h1 id="初始化配置">初始化配置</h1>
<p>初始化项目结构</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure>
<p>新增标签页，设置属性（type: "tags"）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo new page tags</span><br></pre></td></tr></table></figure>
<p>新增分类，设置属性（type: "categories"）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure>
<p>配置 next 主题设置</p>
<ul>
<li>设置 menu 菜单栏</li>
<li>打开 code 代码复制</li>
</ul>
<p>Debug</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hexo clean --debug</span><br><span class="line">hexo s --debug</span><br></pre></td></tr></table></figure>
<p><strong>Git 上传插件</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>
<p><strong>latex 公式</strong></p>
<p>安装 <code>hexo-renderer-markdown-it-plus</code> 后，Katex 与 mathJax
会重复，需要在 next 中配置</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">tags:</span> <span class="string">none</span></span><br><span class="line"><span class="attr">katex:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">copy_tex:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<p><strong>本地图片</strong></p>
<p><del>配置本地图片上传 <a
href="https://bkystop.github.io/2022/01/05/hexo-markdown%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87/index.html">Link</a>
<a href="https://muxiner.github.io/organize-files/">Link</a></del></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-img --save  // 代替 hexo-asset-image</span><br></pre></td></tr></table></figure>
<p><strong>Markdown 文字高亮</strong></p>
<p><a
href="https://blog.csdn.net/qq_42951560/article/details/123596899">Link</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm un hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-markdown-it-plus --save</span><br></pre></td></tr></table></figure>
<p>_config.yml</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">markdown:</span></span><br><span class="line">    <span class="attr">preset:</span> <span class="string">&quot;default&quot;</span></span><br><span class="line">    <span class="attr">plugins:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">markdown-it-mark</span></span><br></pre></td></tr></table></figure>
<p><strong>添加Sitemap</strong></p>
<p>为自己的网站配置sitemap，可以有效地提升SEO</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save</span><br><span class="line">npm install hexo-generator-baidu-sitemap --save</span><br></pre></td></tr></table></figure>
<p>并在站点配置文件<code>_config.yml</code>的末尾添加：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">sitemap:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">sitemap.xml</span></span><br><span class="line"><span class="attr">baidusitemap:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">baidusitemap.xml</span></span><br></pre></td></tr></table></figure>
<p><strong>改 blockquote 颜色</strong></p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">blockquote</span> &#123;</span><br><span class="line">  <span class="attribute">background</span>: <span class="number">#d0a7020d</span>;</span><br><span class="line">  <span class="attribute">border-left</span>: <span class="number">4px</span> solid <span class="number">#d0a702</span>;</span><br><span class="line">  <span class="attribute">color</span>: <span class="built_in">var</span>(--blockquote-color);</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">0</span> <span class="number">15px</span>;</span><br><span class="line"></span><br><span class="line">  <span class="selector-tag">cite</span><span class="selector-pseudo">::before</span> &#123;</span><br><span class="line">    <span class="attribute">content</span>: <span class="string">&#x27;-&#x27;</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">0</span> <span class="number">5px</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="备份本地博客">备份本地博客</h1>
<p><a
href="https://blog.csdn.net/u014532291/article/details/131805350">利用mklink实现OneDrive自定义文件双向同步</a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">MKLINK [[/D] | [/H] | [/J]] Link Target</span><br><span class="line"></span><br><span class="line">        /D      创建目录符号链接。默认为文件符号链接。</span><br><span class="line">        /H      创建硬链接而非符号链接。</span><br><span class="line">        /J      创建目录联接。</span><br><span class="line">        Link    指定新的符号链接名称。</span><br><span class="line">        Target  指定新链接引用的路径</span><br><span class="line">                (相对或绝对)。</span><br></pre></td></tr></table></figure>
<p>管理员 PowerShell <code>cmd /c mklink /D 目标地址 源地址</code>
将博客<u>硬链接</u>到 OneDrive 文件夹下备份</p>
<h1 id="更新主题-next">更新主题 Next</h1>
<blockquote>
<p>由于 NexT 以前使用的 swig 模版引擎停止维护，去年迁移到了
Nunjucks，但是
目录下的模版文件后缀名没有更改。原因是修改后缀名影响太广，会使用户在执行
进行更新时产生大量的冲突。这次更换仓库为了解决这些历史遗留问题，没有保留之前的
commit 历史。</p>
</blockquote>
<p>2024.4.23 按照<a
href="https://theme-next.js.org/docs/getting-started/">官网</a>配置</p>
<h1 id="debug">DeBug</h1>
<p><a href="https://blog.csdn.net/liuergo/article/details/102640098">Git
提交文件名全变小写问题</a></p>
]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>如何保持专注</title>
    <url>/2024/Stay-Focused/</url>
    <content><![CDATA[<p>磨刀不误砍柴工，弄明白如何做事比糊里糊涂的做更重要。当下的目标是通过不断<strong>实践试错</strong>并<strong>总结纠错</strong>，从而找到最适合我的高效专注的做事方法。在此，再以高中的座右铭激励自己：当你觉得为时已晚的时候，恰恰是最早的时候。从学生变成社会人、从孩子变成家庭支柱、从做事的变成管人的（总会有那天的啦），我还有很多要改进要学习的地方（如何清楚明确的表达自己的工作内容，如何高效明确的和别人沟通），这些变化不是为了适应社会的表演，而是成年人的责任。<del>（tmd这文字像是初中生写出来的，文化沙漠了属于是，有时间还是得多看看书捏）</del></p>
<blockquote>
<p>Thought is already is late, exactly is the earliest time.</p>
</blockquote>
<h1 id="启动">启动</h1>
<p>布置好工作环境，安排好工作内容，让自己发自内心得希望把事情完成。</p>
<ul>
<li><strong>早起，立即开始工作</strong>。早起会立即给自己一个潜意识的信号，让自己在那天努力工作。</li>
<li><strong>制定一个行动计划</strong>。用 10
分钟来筹划接下来的几个小时要做的事情，让事情可以推动自己前进。</li>
<li><strong>提前完成一连串的工作</strong>。清晨就处理完当天最紧要的工作，白天的时间就会显得从容，这也会产生积极的启动，这将持续一整天。</li>
</ul>
<h1 id="无干扰休息">无干扰休息</h1>
<p>休息可以帮助自己以新的方式思考问题，可以清醒头脑，从而重新集中注意力。无干扰的休息意味着休息不会扼杀你的动力，这样你就可以重新振作起来，而不会陷入拖延的弯路。</p>
<ul>
<li><strong>避免参与式的休息</strong>。短视频，游戏，微信等会占据头脑的东西都会打断做事的势头。</li>
<li><strong>专注于放松</strong>。走一小段路、喝一杯水或伸展身体都是不错的选择，可以很容易地从休息中抽身出来回去工作。</li>
</ul>
<p>休息不是为了玩得开心，这才是真正休息的目的。休息是关于战略性地恢复你的能量和注意力，以重新处理手头的工作。把电视、游戏和娱乐活动留给晚上可以无愧疚地放松的时候。</p>
<h1 id="积极工作">积极工作</h1>
<p>被动任务比主动任务更难集中注意力。改变这种状况的关键是，或者。</p>
<ul>
<li><strong>使任务处于活动状态</strong>。将被动学习任务转换为主动学习任务，如写提纲、记录重点。</li>
<li><strong>穿插活动任务</strong>。将被动学习与主动任务穿插在一起，以定期提高注意力，如表达自己见解、用自己的话复述一遍。</li>
</ul>
<blockquote>
<p><strong><a href="https://www.youtube.com/watch?v=FrNqSLPaZLc">Feynman
学习法</a></strong></p>
<p>Step1: choose your concept【写下你要学习的概念】</p>
<p>Step2: Pretend You're Teaching the ldea to a New
Student【假象你在教一位不懂这件事情的人这个概念，尝试将这件事情说明白】</p>
<p>Step3: If You Get Stuck, Go Back to the
Book【如果你在这个过程中遇到了困难，回到课本和视频中进一步学习】</p>
<p>Step4: Simplify and Create
Analogies【尝试精简自己的解释，并将其与其他概念做类比，从而更好地理解概念】</p>
<p>总结一下，当你可以教会某人一个ta完全不懂的概念的时候，说明你真正掌握了这个概念</p>
</blockquote>
<h1 id="固定日程安排">固定日程安排</h1>
<p>努力工作的最好方式就是拥有生活</p>
<ul>
<li><p><strong>保证休息</strong>，应对大量工作任务的办法是保证自己有休息时间。如果你白天不好好工作，却逼迫自己整夜学习的话，你很容易就会筋疲力尽的。无论压力再大任务再多，都要保证休息时间，一颗清醒的大脑决定了工作的质量。</p></li>
<li><p>使用时间分块法或每周/每日目标法，<strong>将工作和生活清晰地分离开</strong>，好好工作好好玩。</p></li>
</ul>
<blockquote>
<p><strong>时间分块法</strong></p>
<p>将一天分成更小的时间分块法。在每一段时间里，专注于一项任务或一组相似的任务。其重点是<strong>规划出需要完成的任务</strong>，然后<strong>划分特定的时间段专注完成这些任务</strong>。</p>
<p>当你把一整天分成时间段后，它会让你专注于任务，并限制其他人占用你的时间。</p>
<p>时间分块法让你每天都有一个要完成的具体任务的时间表，而不是遵循一个不断扩大的待办事项列表，你只需要关注于当下应该做的事情。</p>
</blockquote>
<p><a
href="https://www.scotthyoung.com/blog/2011/11/28/focus-without-caffeine/">Refer</a></p>
]]></content>
      <categories>
        <category>思考</category>
      </categories>
      <tags>
        <tag>保持专注</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux-Tool</title>
    <url>/Linux-Tool/</url>
    <content><![CDATA[<h1 id="vim">Vim</h1>
<h2 id="基本模式切换">基本模式切换</h2>
<ul>
<li><strong><code>i</code></strong>：进入插入模式，从光标前插入文本。</li>
<li><strong><code>a</code></strong>：进入插入模式，从光标后插入文本。</li>
<li><strong><code>Esc</code></strong>：退出插入模式，回到正常模式。</li>
<li><strong><code>v</code></strong>：进入可视模式，用于选择文本块。</li>
<li><strong><code>V</code></strong> 或
<strong><code>vv</code></strong>：进入可视行模式，选择整行文本。</li>
<li><strong><code>Ctrl + v</code></strong>：进入可视块模式，选择矩形区域。</li>
</ul>
<h2 id="光标移动">光标移动</h2>
<ul>
<li><strong><code>w</code></strong>：光标移动到下一个单词的开头。</li>
<li><strong><code>b</code></strong>：光标移动到上一个单词的开头。</li>
<li><strong><code>0</code></strong>：移动到行首。</li>
<li><strong><code>$</code></strong>：移动到行尾。</li>
<li><strong><code>gg</code></strong>：移动到文件的第一行。</li>
<li><strong><code>G</code></strong>：移动到文件的最后一行。</li>
<li><strong><code>H</code></strong>：移动到屏幕的顶部。</li>
<li><strong><code>M</code></strong>：移动到屏幕的中部。</li>
<li><strong><code>L</code></strong>：移动到屏幕的底部。</li>
</ul>
<h2 id="编辑文本">编辑文本</h2>
<ul>
<li><strong><code>x</code></strong>：删除光标所在的字符。</li>
<li><strong><code>dw</code></strong>：删除从光标到单词末尾的内容。</li>
<li><strong><code>dd</code></strong>：删除当前行。</li>
<li><strong><code>d$</code></strong>：删除从光标到行末的内容。</li>
<li><strong><code>yy</code></strong>：复制当前行。</li>
<li><strong><code>p</code></strong>：粘贴到光标之后。</li>
<li><strong><code>u</code></strong>：撤销上一步操作。</li>
<li><strong><code>Ctrl + r</code></strong>：重做撤销的操作。</li>
<li><strong><code>r</code></strong>：替换光标下的字符。</li>
<li><strong><code>ciw</code></strong>：删除光标所在的单词并进入插入模式。</li>
<li><strong><code>c$</code></strong>：删除从光标到行尾的内容并进入插入模式。</li>
</ul>
<h2 id="搜索和替换">搜索和替换</h2>
<ul>
<li><strong><code>/pattern</code></strong>：向下搜索
<code>pattern</code>。</li>
<li><strong><code>?pattern</code></strong>：向上搜索
<code>pattern</code>。</li>
<li><strong><code>n</code></strong>：跳到下一个搜索结果。</li>
<li><strong><code>N</code></strong>：跳到上一个搜索结果。</li>
<li><strong><code>:s/old/new/g</code></strong>：将当前行的所有
<code>old</code> 替换为 <code>new</code>。</li>
<li><strong><code>:%s/old/new/g</code></strong>：将整个文件中的所有
<code>old</code> 替换为 <code>new</code>。</li>
</ul>
<h2 id="文件操作">文件操作</h2>
<ul>
<li><strong><code>:w</code></strong>：保存文件。</li>
<li><strong><code>:q</code></strong>：退出 Vim。</li>
<li><strong><code>:wq</code></strong>：保存并退出。</li>
<li><strong><code>:q!</code></strong>：不保存强制退出。</li>
<li><strong><code>:e filename</code></strong>：打开文件
<code>filename</code>。</li>
<li><strong><code>:r filename</code></strong>：在当前文件中插入
<code>filename</code> 的内容。</li>
</ul>
<h1 id="screen">Screen</h1>
<h2 id="基本命令">基本命令</h2>
<ul>
<li><strong><code>screen</code></strong>：启动一个新的
<code>screen</code> 会话。</li>
<li><strong><code>screen -S session_name</code></strong>：启动一个命名为
<code>session_name</code> 的 <code>screen</code> 会话。</li>
<li><strong><code>screen -ls</code></strong>：列出当前所有的
<code>screen</code> 会话。</li>
<li><strong><code>screen -r</code></strong>：重新连接到一个
<code>screen</code> 会话（如果只有一个会话）。</li>
<li><strong><code>screen -r session_name</code></strong>：重新连接到一个名为
<code>session_name</code> 的 <code>screen</code> 会话。</li>
<li><strong><code>screen -d session_name</code></strong>：在其他地方的终端断开某个会话的连接。</li>
<li><strong><code>screen -d -r session_name</code></strong>：强制从其他地方的终端断开并重新连接到
<code>session_name</code> 会话。</li>
<li><strong><code>screen -X quit</code></strong>：关闭所有
<code>screen</code> 会话</li>
</ul>
<h2 id="在-screen-会话中操作">在 <code>screen</code> 会话中操作</h2>
<p>在 <code>screen</code> 会话中，所有命令都以 <code>Ctrl + a</code>
开头（表示“前缀键”），然后跟随其他按键来执行操作。</p>
<ul>
<li><strong><code>Ctrl + a, X</code></strong>：关闭当前的分割窗口。</li>
<li><strong><code>Ctrl + a, c</code></strong>：创建一个新的窗口。</li>
<li><strong><code>Ctrl + a, n</code></strong>：切换到下一个窗口。</li>
<li><strong><code>Ctrl + a, p</code></strong>：切换到上一个窗口。</li>
<li><strong><code>Ctrl + a, "</code></strong>：列出当前所有窗口，允许选择切换。</li>
<li><strong><code>Ctrl + a, 0-9</code></strong>：切换到指定编号的窗口（例如
<code>Ctrl + a, 1</code> 切换到窗口 1）。</li>
<li><strong><code>Ctrl + a, d</code></strong>：将当前
<code>screen</code> 会话断开（detach），会话继续在后台运行。</li>
<li><strong><code>Ctrl + a, A</code></strong>：重命名当前窗口。</li>
<li><strong><code>Ctrl + a, K</code></strong>：关闭当前窗口。</li>
<li><strong><code>Ctrl + a, S</code></strong>：水平分割当前窗口。</li>
<li><strong><code>Ctrl + a, |</code></strong>：垂直分割当前窗口。</li>
<li><strong><code>Ctrl + a, tab</code></strong>：在分割的窗口间切换焦点。</li>
<li><strong><code>Ctrl + a, Q</code></strong>：关闭除了当前窗口外的所有分割窗口。</li>
<li><strong><code>Ctrl + a, X</code></strong>：关闭当前的分割窗口。</li>
</ul>
<h2 id="会话管理">会话管理</h2>
<ul>
<li><strong><code>Ctrl + a, :quit</code></strong> 或
<strong><code>Ctrl + a, d</code></strong>：结束当前 <code>screen</code>
会话。</li>
<li><strong><code>Ctrl + a, :kill</code></strong>：强制关闭当前窗口。</li>
<li><strong><code>exit</code></strong>：退出当前 <code>screen</code>
会话。</li>
</ul>
]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>Linux工具</tag>
      </tags>
  </entry>
</search>
